1
00:00:00,000 --> 00:00:01,966
内容/录制:Z0MI酱，视频后期/字幕:梁嘉铭

2
00:00:02,600 --> 00:00:04,100
hello大家早上好

3
00:00:04,133 --> 00:00:06,300
有没有发现上班就像喝粥

4
00:00:06,300 --> 00:00:07,800
你吃不饱也饿不着

5
00:00:07,800 --> 00:00:09,666
但是你还得慢慢熬

6
00:00:12,433 --> 00:00:13,066
今天

7
00:00:13,100 --> 00:00:15,533
来到了整个AI计算集群里面

8
00:00:15,533 --> 00:00:16,366
的挑战

9
00:00:16,366 --> 00:00:17,366
我是ZOMI

10
00:00:17,833 --> 00:00:19,900
那看一下整个计算集群里面

11
00:00:19,933 --> 00:00:20,166
其实

12
00:00:20,166 --> 00:00:23,600
现在已经讲到了这一节内容

13
00:00:23,600 --> 00:00:24,166
那一开始

14
00:00:24,166 --> 00:00:26,833
会讲HPC的一个定义

15
00:00:26,833 --> 00:00:29,900
接着去看看HPC的一个发展趋势

16
00:00:30,000 --> 00:00:31,900
因为有了HPC之后

17
00:00:31,900 --> 00:00:32,966
整体的HPC

18
00:00:32,966 --> 00:00:36,700
是经过初代二代三代到现在融合HPC

19
00:00:36,733 --> 00:00:38,033
融合AI的内容

20
00:00:38,033 --> 00:00:38,433
因此

21
00:00:38,433 --> 00:00:41,666
现在正式来到AI计算集群

22
00:00:41,700 --> 00:00:43,166
那有了AI计算集群之后

23
00:00:43,166 --> 00:00:43,900
现在

24
00:00:43,933 --> 00:00:46,900
正式看一下AI计算集群的整体的挑战

25
00:00:46,900 --> 00:00:48,900
将会在下一节内容里面展开

26
00:00:48,900 --> 00:00:52,100
AI计算集群的整体的架构

27
00:00:52,700 --> 00:00:53,733
那不管怎么样

28
00:00:53,733 --> 00:00:55,333
今天在开始之前

29
00:00:55,333 --> 00:00:57,000
还是想提个小问题

30
00:00:57,000 --> 00:00:59,800
带着这个小问题一起往下去看

31
00:01:00,100 --> 00:01:01,300
首先第一个问题就是

32
00:01:01,300 --> 00:01:03,566
现在要建一个AI计算集群

33
00:01:03,566 --> 00:01:04,800
但是软硬件栈

34
00:01:04,800 --> 00:01:06,700
应该分哪几层

35
00:01:06,966 --> 00:01:09,400
整个AIInfra就是AI集群

36
00:01:09,400 --> 00:01:10,700
到底是怎么分层

37
00:01:10,833 --> 00:01:12,800
听说硅基流动

38
00:01:12,800 --> 00:01:14,900
就说自己是做AIInfra

39
00:01:14,933 --> 00:01:16,333
那那些芯片公司

40
00:01:16,333 --> 00:01:19,500
华为寒武纪也说自己是做AIInfra

41
00:01:19,633 --> 00:01:22,200
到底AIInfra是指哪几层

42
00:01:22,200 --> 00:01:24,033
对吧大家都有软硬件

43
00:01:24,033 --> 00:01:25,566
到底是怎么分层

44
00:01:25,700 --> 00:01:27,500
将会在这一个视频里面

45
00:01:27,500 --> 00:01:28,966
跟大家介绍清楚

46
00:01:29,100 --> 00:01:31,033
第二个第二个问题

47
00:01:31,033 --> 00:01:34,700
你觉得AIInfra里面这么多家公司会有

48
00:01:34,733 --> 00:01:37,300
哪几家公司能够脱颖而出

49
00:01:37,300 --> 00:01:38,800
是寒武纪还是华为

50
00:01:38,800 --> 00:01:39,966
还是天数

51
00:01:39,966 --> 00:01:42,566
最近阿里平头哥的PPU

52
00:01:42,566 --> 00:01:44,166
都已经出来

53
00:01:44,166 --> 00:01:47,633
后面还有谁能够在AIInfra里面

54
00:01:47,633 --> 00:01:49,100
脱颖而出

55
00:01:49,166 --> 00:01:50,566
那带着这两个问题

56
00:01:50,566 --> 00:01:53,800
将会深入以后的内容

57
00:01:54,866 --> 00:01:56,066
那在这期视频里面

58
00:01:56,100 --> 00:01:59,100
可能会简单分开3个小节

59
00:01:59,100 --> 00:02:01,000
跟大家一起去分享一下

60
00:02:01,000 --> 00:02:02,633
就整个AI计算集群里面

61
00:02:02,633 --> 00:02:05,466
首先看一下所谓的一些挑战

62
00:02:05,500 --> 00:02:07,100
真正迎来了挑战

63
00:02:07,100 --> 00:02:08,000
有了挑战之后

64
00:02:08,000 --> 00:02:11,400
就开始构建整个AI集群的架构

65
00:02:11,400 --> 00:02:12,700
那构建完架构之后

66
00:02:12,733 --> 00:02:13,300
看一下

67
00:02:13,300 --> 00:02:16,533
这个AI集群的建设的目标到底是什么

68
00:02:16,533 --> 00:02:19,100
怎么去提升有效的算力

69
00:02:19,200 --> 00:02:20,566
那接下来

70
00:02:20,566 --> 00:02:23,400
就深入的展开后面的内容

71
00:02:24,733 --> 00:02:26,166
第一个先来看看

72
00:02:26,166 --> 00:02:27,900
整个AI计算集群的挑战

73
00:02:27,933 --> 00:02:31,833
看一下到底建一个AI计算集群中心

74
00:02:31,833 --> 00:02:34,033
到底会遇到哪些阻碍

75
00:02:34,033 --> 00:02:35,300
特别难的东西

76
00:02:35,333 --> 00:02:36,100
首先

77
00:02:36,100 --> 00:02:39,133
来看一下通算业务的整体负载

78
00:02:39,133 --> 00:02:40,000
所谓的通算

79
00:02:40,000 --> 00:02:41,433
以云数据中心

80
00:02:41,433 --> 00:02:43,366
DC中心作为一个例子

81
00:02:43,366 --> 00:02:44,866
那这里面比较明确

82
00:02:44,900 --> 00:02:46,400
因为整个云数据中心

83
00:02:46,400 --> 00:02:48,200
承载非常多的业务

84
00:02:48,200 --> 00:02:50,366
那这些业务之间非常的分散

85
00:02:50,366 --> 00:02:51,900
任务之间的关联很少

86
00:02:51,933 --> 00:02:53,600
例如可能云数据中心

87
00:02:53,600 --> 00:02:55,100
会有一些很多的看板

88
00:02:55,133 --> 00:02:56,800
去统计相关的数据

89
00:02:56,800 --> 00:02:58,866
因此有大数据的一些服务

90
00:02:58,900 --> 00:02:59,533
那另外的话

91
00:02:59,533 --> 00:03:00,700
可能云数据中心

92
00:03:00,700 --> 00:03:02,700
会存很多用户的一些信息

93
00:03:02,700 --> 00:03:04,633
于是提供了云盘

94
00:03:04,666 --> 00:03:06,400
各种各样的云存储的功能

95
00:03:06,400 --> 00:03:06,766
当然

96
00:03:06,766 --> 00:03:08,600
可能还会有一些Linux的系统

97
00:03:08,600 --> 00:03:11,200
方便客户做一些建站的问题

98
00:03:11,200 --> 00:03:13,633
于是又有了虚拟化的功能

99
00:03:13,633 --> 00:03:14,633
所以你可以看到

100
00:03:14,633 --> 00:03:16,066
很多不同的业务

101
00:03:16,100 --> 00:03:18,166
负载在不同的一个服务器内

102
00:03:18,166 --> 00:03:19,266
或者同一个任务

103
00:03:19,300 --> 00:03:21,000
分布在不同的服务器

104
00:03:21,033 --> 00:03:22,633
或者多个负载

105
00:03:22,633 --> 00:03:24,033
在单个服务器内

106
00:03:24,033 --> 00:03:25,866
所以整体的通算的业务

107
00:03:25,900 --> 00:03:27,400
整体服务器之间

108
00:03:27,400 --> 00:03:29,366
是一个松耦合的关系

109
00:03:29,366 --> 00:03:30,700
而且负载的类型

110
00:03:30,733 --> 00:03:32,633
可以看到已经非常多样化

111
00:03:32,633 --> 00:03:33,833
不同的负载

112
00:03:33,833 --> 00:03:35,700
可能会在单服务器内闭环

113
00:03:35,733 --> 00:03:37,800
也可以在集群里面闭环

114
00:03:37,800 --> 00:03:38,166
所以说

115
00:03:38,166 --> 00:03:41,600
很难做到集群的整体的性能的优化

116
00:03:41,633 --> 00:03:44,500
反正我提供非常多的廉价的算力

117
00:03:44,533 --> 00:03:46,800
给到大家去租赁或去存储

118
00:03:46,800 --> 00:03:48,366
和各种各样的功能都有

119
00:03:49,600 --> 00:03:51,766
那现在看一下整体的负载

120
00:03:51,766 --> 00:03:52,466
那可以看到

121
00:03:52,500 --> 00:03:53,000
基本上

122
00:03:53,000 --> 00:03:54,900
就是从另外一个角度来看

123
00:03:54,933 --> 00:03:56,100
传统的数据中心

124
00:03:56,100 --> 00:03:57,933
它的应用非常的分散

125
00:03:58,066 --> 00:03:58,966
往右边看一看

126
00:03:58,966 --> 00:04:00,700
整体的TOP50的热点应用

127
00:04:00,733 --> 00:04:02,633
实际上只占了60%的cycle

128
00:04:02,633 --> 00:04:04,466
而且还在持续的下降

129
00:04:04,500 --> 00:04:05,233
那这个时候

130
00:04:05,233 --> 00:04:06,900
只能针对特定的应用

131
00:04:06,933 --> 00:04:08,766
进行一些软件的优化

132
00:04:08,833 --> 00:04:09,900
很难的说实话

133
00:04:09,933 --> 00:04:11,800
真的很难去针对某一类应用

134
00:04:11,800 --> 00:04:13,266
专门集群的优化

135
00:04:13,300 --> 00:04:14,500
那我存云盘的时候

136
00:04:14,500 --> 00:04:16,966
你看看客户的量是非常的多

137
00:04:16,966 --> 00:04:18,766
而且客户是非常的分散

138
00:04:18,766 --> 00:04:20,700
我不可能针对某个客户的数据

139
00:04:20,733 --> 00:04:22,400
进行一个快速的回传

140
00:04:22,400 --> 00:04:23,833
对吧那这个时候

141
00:04:23,833 --> 00:04:26,300
你就只能做通算的任务

142
00:04:26,333 --> 00:04:28,766
那接着看一下整个AI的一个集群

143
00:04:28,766 --> 00:04:30,233
或者AI的计算

144
00:04:30,233 --> 00:04:32,866
AI的智算的一个负载的特点

145
00:04:32,900 --> 00:04:33,833
那AI的智算

146
00:04:33,833 --> 00:04:34,600
可以看到

147
00:04:34,600 --> 00:04:36,000
真的是很特别

148
00:04:36,000 --> 00:04:38,466
首先模型的参数规模

149
00:04:38,466 --> 00:04:39,833
是越来越大

150
00:04:39,833 --> 00:04:41,033
一开始的时候

151
00:04:41,033 --> 00:04:43,266
可能在2015年到2018年的时候

152
00:04:43,300 --> 00:04:45,700
大部分的都受各种各样的ResNet50

153
00:04:45,700 --> 00:04:46,600
还有MobileNet

154
00:04:46,600 --> 00:04:47,566
efficientNet

155
00:04:47,566 --> 00:04:49,633
VGGNet各种各样的网络模型

156
00:04:49,700 --> 00:04:51,833
但是那些网络模型的参数量

157
00:04:51,833 --> 00:04:53,100
大家都没有提

158
00:04:53,133 --> 00:04:53,566
基本上

159
00:04:53,566 --> 00:04:55,466
都在一个千万级的规模

160
00:04:55,500 --> 00:04:58,366
但是来到了2018年到2022年的时候

161
00:04:58,366 --> 00:05:00,033
迎来了GPT时代

162
00:05:00,033 --> 00:05:01,400
或者BERT的时代

163
00:05:01,400 --> 00:05:02,433
那GPT跟BERT

164
00:05:02,433 --> 00:05:03,166
其实你会发现

165
00:05:03,166 --> 00:05:05,033
它的网络模型的参数量

166
00:05:05,166 --> 00:05:07,833
基本上是在一个亿左右徘徊

167
00:05:07,833 --> 00:05:08,466
小于一个亿

168
00:05:08,500 --> 00:05:09,366
大于一个亿

169
00:05:09,366 --> 00:05:11,200
那这个就是亿级的参数量

170
00:05:11,200 --> 00:05:12,066
的一个时代

171
00:05:12,733 --> 00:05:14,166
到了2023年的时候

172
00:05:14,166 --> 00:05:15,800
特别是去年

173
00:05:15,800 --> 00:05:18,033
去年2024年的12月份的时候

174
00:05:18,033 --> 00:05:19,300
deepseek幻方

175
00:05:19,333 --> 00:05:21,733
就发布了自己的674B

176
00:05:21,733 --> 00:05:25,633
也就是6,740亿的一个大规模的参数

177
00:05:25,633 --> 00:05:26,700
那这里面可以看到

178
00:05:26,733 --> 00:05:28,733
网络模型的规模越来越大

179
00:05:28,733 --> 00:05:31,400
对算力的需求肯定会越来越多

180
00:05:31,400 --> 00:05:33,366
于是可能训练一个resnet50

181
00:05:33,366 --> 00:05:35,866
可能在单张的GPU里面就能训了 

182
00:05:35,900 --> 00:05:36,933
训练一个GPT 1

183
00:05:36,933 --> 00:05:38,633
可能需要一个节点

184
00:05:38,633 --> 00:05:39,466
如果训练一个

185
00:05:39,500 --> 00:05:41,500
超大规模参数的模型

186
00:05:41,500 --> 00:05:44,533
可能要在一个E级算力AI集群

187
00:05:44,533 --> 00:05:45,600
所以总体可以看到

188
00:05:45,600 --> 00:05:48,366
从单张卡到一个节点

189
00:05:48,400 --> 00:05:50,400
到一个SuperPod超节点

190
00:05:50,400 --> 00:05:52,433
甚至一个千卡到万卡的集群

191
00:05:52,433 --> 00:05:54,000
整体的AI的业务

192
00:05:54,000 --> 00:05:56,833
就变成高并行网络化的特征

193
00:05:56,833 --> 00:05:58,800
整体需要一个集群

194
00:05:58,800 --> 00:06:00,266
真的需要一个集群

195
00:06:00,300 --> 00:06:03,033
最后就演变成为整个集群

196
00:06:03,033 --> 00:06:04,033
就右边的这个

197
00:06:04,033 --> 00:06:04,566
一个集群

198
00:06:04,566 --> 00:06:05,766
成为大模型训练

199
00:06:05,766 --> 00:06:07,966
最佳的实践平台

200
00:06:07,966 --> 00:06:08,400
当然

201
00:06:08,400 --> 00:06:11,200
除了刚才讲到的c算力以外

202
00:06:11,200 --> 00:06:12,566
现在还需要互联

203
00:06:12,566 --> 00:06:13,966
网络模型越来越大

204
00:06:13,966 --> 00:06:16,100
从单个卡到单个节点

205
00:06:16,133 --> 00:06:17,100
那节点内

206
00:06:17,100 --> 00:06:18,366
就需要进行互联

207
00:06:18,366 --> 00:06:20,300
从节点到超节点

208
00:06:20,333 --> 00:06:22,300
或者到一个千卡到万卡

209
00:06:22,300 --> 00:06:22,966
那这个时候

210
00:06:22,966 --> 00:06:25,500
就需要节点间进行互联

211
00:06:25,533 --> 00:06:27,166
要上NV switch

212
00:06:27,166 --> 00:06:28,366
要上网络

213
00:06:28,366 --> 00:06:30,500
需要上各种各样的超节点

214
00:06:30,533 --> 00:06:32,400
灵渠各种各样的东西

215
00:06:32,500 --> 00:06:33,033
那另外的话

216
00:06:33,033 --> 00:06:34,666
看一下存储

217
00:06:34,700 --> 00:06:35,933
存储也是很简单

218
00:06:35,933 --> 00:06:36,233
一开始

219
00:06:36,233 --> 00:06:38,433
可能在一个消费级的服务器里面

220
00:06:38,433 --> 00:06:39,266
就可以存

221
00:06:39,300 --> 00:06:39,833
那接着

222
00:06:39,833 --> 00:06:41,700
数据量越来越大

223
00:06:41,733 --> 00:06:43,400
因为有了无监督学习

224
00:06:43,400 --> 00:06:44,666
全世界的数据

225
00:06:44,700 --> 00:06:46,766
都可以给进行一个存储

226
00:06:46,766 --> 00:06:48,466
所以从TB级别的存储

227
00:06:48,500 --> 00:06:49,933
到PB级别的存储

228
00:06:49,933 --> 00:06:51,966
到单独的出现存储PoD

229
00:06:51,966 --> 00:06:53,666
存储资源池

230
00:06:53,800 --> 00:06:55,000
这可以看到

231
00:06:55,000 --> 00:06:56,366
集群现在已经成为

232
00:06:56,366 --> 00:06:58,900
大模型训练的最佳平台

233
00:06:58,933 --> 00:07:00,800
既然分析完负载之后

234
00:07:00,800 --> 00:07:03,600
看一下要建一个AI集群

235
00:07:03,600 --> 00:07:05,000
到底怎么建

236
00:07:05,366 --> 00:07:06,300
肯定首先

237
00:07:06,300 --> 00:07:08,033
需要分析它的一个挑战

238
00:07:08,166 --> 00:07:08,466
第一个

239
00:07:08,500 --> 00:07:11,133
先看一下基础设施的先进性

240
00:07:11,133 --> 00:07:12,766
整个AI计算中心

241
00:07:12,766 --> 00:07:13,400
可以看到

242
00:07:13,400 --> 00:07:16,000
基础设施是跟传统的机房不一样

243
00:07:16,100 --> 00:07:18,100
更多的走向以密集型的NPU

244
00:07:18,100 --> 00:07:19,966
或者GPU的算力为中心

245
00:07:19,966 --> 00:07:21,466
进行整体的规划

246
00:07:21,500 --> 00:07:22,100
那这里面

247
00:07:22,100 --> 00:07:23,600
密集很重要

248
00:07:23,600 --> 00:07:25,000
是算力密集型

249
00:07:25,000 --> 00:07:25,766
可以看到

250
00:07:25,766 --> 00:07:27,100
既然算力密集型

251
00:07:27,133 --> 00:07:30,766
所以说整体的芯片的功耗会越来越

252
00:07:30,766 --> 00:07:34,000
高但芯片的H100已经去到了700瓦

253
00:07:34,000 --> 00:07:35,233
非常的夸张 

254
00:07:35,733 --> 00:07:38,366
芯片的TDP功耗在上升

255
00:07:38,366 --> 00:07:39,066
所以这个时候

256
00:07:39,100 --> 00:07:40,900
使得液冷相关的技术

257
00:07:40,900 --> 00:07:42,500
就非常的有必要

258
00:07:42,500 --> 00:07:43,566
而且可以看到

259
00:07:43,566 --> 00:07:44,233
计算中心

260
00:07:44,233 --> 00:07:46,233
还会进一步拉高整体的功耗

261
00:07:46,233 --> 00:07:47,433
密度的需求

262
00:07:47,433 --> 00:07:50,200
现在单柜已经超过50千瓦

263
00:07:50,200 --> 00:07:51,033
是单柜

264
00:07:51,033 --> 00:07:53,566
一个机柜非常夸张

265
00:07:53,566 --> 00:07:53,966
你想想

266
00:07:53,966 --> 00:07:58,300
你家用一天什么时候能超过50千瓦

267
00:08:03,100 --> 00:08:06,000
所以随着单位的功耗的密度

268
00:08:06,000 --> 00:08:07,066
越来越高 

269
00:08:07,100 --> 00:08:11,233
规模的液冷的应用就越来越重要

270
00:08:12,033 --> 00:08:13,800
接着看一下第二个因素

271
00:08:13,800 --> 00:08:14,900
第二个比较大的挑战

272
00:08:14,933 --> 00:08:17,900
就是超大规模的集群互联呐

273
00:08:17,900 --> 00:08:19,100
说实话集群互联

274
00:08:19,100 --> 00:08:21,333
其实经过了好长的发展

275
00:08:21,333 --> 00:08:23,000
第一个肯定是PCIE

276
00:08:23,000 --> 00:08:24,066
通过CPU

277
00:08:24,100 --> 00:08:26,100
做外围的外设的一个拓展

278
00:08:26,100 --> 00:08:26,533
接着

279
00:08:26,533 --> 00:08:28,900
因为GPU跟GPU之间要进行互联

280
00:08:28,900 --> 00:08:31,600
所以就英伟达推出自己的NV link

281
00:08:31,600 --> 00:08:32,100
现在

282
00:08:32,133 --> 00:08:35,133
都已经推出了NV switch和NV fusion 

283
00:08:35,566 --> 00:08:37,000
让更多的芯片厂商

284
00:08:37,000 --> 00:08:39,066
用NV link进行组网

285
00:08:39,100 --> 00:08:41,133
那另外的话还基于RDMA

286
00:08:41,133 --> 00:08:42,033
就开发了IB 

287
00:08:42,033 --> 00:08:44,433
跟RoCE相关的一些网络的协议

288
00:08:44,433 --> 00:08:46,700
可以看到网络模型

289
00:08:46,733 --> 00:08:47,933
越来越大

290
00:08:47,933 --> 00:08:50,500
从百亿的稠密到千亿的稀疏

291
00:08:50,500 --> 00:08:51,033
那现在

292
00:08:51,033 --> 00:08:52,500
其实是降了一个等级

293
00:08:52,533 --> 00:08:54,233
并没有那么的夸张

294
00:08:54,233 --> 00:08:56,466
那单模态走向多模态

295
00:08:56,500 --> 00:08:58,100
整体的模型规模

296
00:08:58,100 --> 00:08:59,900
虽然没有说不断的拓展

297
00:08:59,900 --> 00:09:01,600
但是可以看到模态越来越多

298
00:09:01,600 --> 00:09:03,400
各种各样的模型越来越多

299
00:09:03,400 --> 00:09:04,400
那这个时候

300
00:09:04,400 --> 00:09:07,000
对于大规模的组网负载的均衡

301
00:09:07,000 --> 00:09:08,566
还有多级的缓存 

302
00:09:08,800 --> 00:09:09,900
未来的超节点

303
00:09:09,900 --> 00:09:11,100
迎来了新的技术

304
00:09:11,100 --> 00:09:12,333
所以说现在

305
00:09:12,333 --> 00:09:13,533
大模型的发展

306
00:09:13,533 --> 00:09:15,033
建一个计算中心

307
00:09:15,033 --> 00:09:16,433
主要是训大模型嘛

308
00:09:16,433 --> 00:09:17,433
大模型很明显

309
00:09:17,433 --> 00:09:19,766
已经成为一个不可逆的技术点

310
00:09:19,766 --> 00:09:22,033
所以说超大规模的集群互联

311
00:09:22,033 --> 00:09:23,866
就变得很重要

312
00:09:25,066 --> 00:09:25,833
第三个点

313
00:09:25,833 --> 00:09:27,666
可能也是ZOMI最关心

314
00:09:27,700 --> 00:09:29,633
刚才建完集群

315
00:09:29,633 --> 00:09:31,833
建完风火水电液冷之后

316
00:09:31,833 --> 00:09:32,866
现在最重要

317
00:09:32,900 --> 00:09:35,533
就是提升集群的算力的利用率

318
00:09:35,533 --> 00:09:36,500
那这里面的提升

319
00:09:36,500 --> 00:09:38,600
就有很多个点可以讲究

320
00:09:38,600 --> 00:09:39,900
整个AI计算集群

321
00:09:39,933 --> 00:09:41,333
通过软硬件协同

322
00:09:41,333 --> 00:09:42,533
算存网

323
00:09:42,533 --> 00:09:45,633
整体的全栈来提升整体的算力

324
00:09:45,633 --> 00:09:47,366
也就是从上下分层

325
00:09:47,366 --> 00:09:47,900
软硬

326
00:09:47,933 --> 00:09:51,933
到整体的全栈去一个储备力量

327
00:09:51,933 --> 00:09:52,533
那这里面

328
00:09:52,533 --> 00:09:54,800
其实统计的一个指标也比较明确

329
00:09:54,800 --> 00:09:56,500
首先就是单机执行的效率

330
00:09:56,533 --> 00:09:58,133
这里面叫有效算力

331
00:09:58,133 --> 00:10:00,600
将会在第3小节后面的内容

332
00:10:00,600 --> 00:10:01,700
跟大家详细详细

333
00:10:01,733 --> 00:10:03,966
详细的展开这里面的内容

334
00:10:04,400 --> 00:10:04,966
这里面

335
00:10:04,966 --> 00:10:06,300
只是简单的跟大家看一下

336
00:10:06,333 --> 00:10:07,400
有哪些挑战

337
00:10:07,400 --> 00:10:09,466
那随着网络模型的规模的增大

338
00:10:09,500 --> 00:10:10,200
可以看到

339
00:10:10,200 --> 00:10:12,200
单机的执行的内存和IO的瓶颈

340
00:10:12,200 --> 00:10:14,600
还有计算之间的一个数据的传输

341
00:10:14,600 --> 00:10:16,233
还有资源的调度

342
00:10:16,233 --> 00:10:19,666
很多问题会影响整体的模型的利用率

343
00:10:19,700 --> 00:10:21,400
MFU那第二个

344
00:10:21,400 --> 00:10:23,000
就是线性度

345
00:10:23,000 --> 00:10:24,633
集群的并行效率

346
00:10:24,633 --> 00:10:26,600
随着集群规模增大

347
00:10:26,600 --> 00:10:27,200
说实话

348
00:10:27,200 --> 00:10:29,900
各种各样的并行的方式就出来

349
00:10:29,933 --> 00:10:30,900
就3维并行

350
00:10:30,900 --> 00:10:32,200
其实现在已经不够

351
00:10:32,200 --> 00:10:33,966
已经用到七八维并行

352
00:10:33,966 --> 00:10:35,500
整体的并行的效率

353
00:10:35,533 --> 00:10:37,533
会影响算力的利用率

354
00:10:37,533 --> 00:10:38,600
所以并行

355
00:10:38,600 --> 00:10:40,966
会约束有效的算力

356
00:10:41,000 --> 00:10:41,766
那这个时候

357
00:10:41,766 --> 00:10:43,500
集群的线性度

358
00:10:43,533 --> 00:10:44,933
就变得非常的重要

359
00:10:44,933 --> 00:10:47,833
随着我的集群规模的拓展变大

360
00:10:47,833 --> 00:10:50,566
集群的线性度不应该急着下降

361
00:10:50,566 --> 00:10:51,000
那这个

362
00:10:51,000 --> 00:10:52,066
将会在后面

363
00:10:52,100 --> 00:10:53,800
要跟大家详细的展开

364
00:10:53,800 --> 00:10:55,700
第三个就是比较直接

365
00:10:55,733 --> 00:10:58,933
看一下单step的一个训练的时长

366
00:10:58,966 --> 00:11:01,100
说实话整个大模型的训练时长

367
00:11:01,133 --> 00:11:02,500
大家非常的关心

368
00:11:02,500 --> 00:11:03,900
整体的统计指标

369
00:11:03,900 --> 00:11:05,933
就变成多少tokens每秒

370
00:11:05,933 --> 00:11:07,800
因为现在大模型进去的是TOKEN

371
00:11:07,800 --> 00:11:09,366
出来的也是TOKEN

372
00:11:09,633 --> 00:11:10,800
单step的训练时长

373
00:11:10,800 --> 00:11:12,200
实际上是有条公式

374
00:11:12,200 --> 00:11:14,666
就等于计算的时间加上通讯的时间

375
00:11:14,700 --> 00:11:16,433
再加上存储的IO的时间

376
00:11:16,433 --> 00:11:17,966
减去可以隐藏

377
00:11:17,966 --> 00:11:19,400
就是通算的时间

378
00:11:19,400 --> 00:11:22,433
再减去可隐藏的IO的时间

379
00:11:22,433 --> 00:11:23,000
那这条

380
00:11:23,000 --> 00:11:25,500
就是整个单step的训练的时长

381
00:11:25,533 --> 00:11:27,833
将会在后面也是一样的展开

382
00:11:27,833 --> 00:11:29,966
这里面到底是怎么去算

383
00:11:30,033 --> 00:11:31,700
为什么这条公式是怎么来

384
00:11:31,733 --> 00:11:32,333
那这里面

385
00:11:32,333 --> 00:11:34,633
就有几个可以做的一些东西

386
00:11:34,633 --> 00:11:36,766
第一个就是提升一个带宽嘛

387
00:11:36,766 --> 00:11:39,000
提升带宽就可以降低通讯的时长

388
00:11:39,233 --> 00:11:40,066
但它提升带宽

389
00:11:40,100 --> 00:11:41,833
不是说你提升带宽的硬件

390
00:11:41,833 --> 00:11:42,500
我的软件

391
00:11:42,533 --> 00:11:44,333
就能充分的把带宽的利用率

392
00:11:44,333 --> 00:11:45,333
全都用满

393
00:11:45,333 --> 00:11:47,500
传输的时候还可以结合算法

394
0:11:47,500 --> 00:11:48,533
做all2all通讯

395
00:11:48,533 --> 00:11:50,033
还是all reduce通讯

396
00:11:50,033 --> 00:11:51,233
到底是大包的通讯

397
00:11:51,233 --> 00:11:52,400
还是小包的通讯

398
00:11:52,400 --> 00:11:53,966
传统的模型的参数量

399
00:11:53,966 --> 00:11:55,466
是多的还是少

400
00:11:55,500 --> 00:11:57,200
所以这里面有很多的讲究

401
00:11:57,200 --> 00:11:59,233
跟算法是强相关

402
00:11:59,333 --> 00:11:59,766
第二个

403
00:11:59,766 --> 00:12:01,433
就是提升存储性能

404
00:12:01,433 --> 00:12:03,066
降低IO时间

405
00:12:03,100 --> 00:12:05,033
这个就是针对CKPT

406
00:12:05,033 --> 00:12:07,366
网络模型的参数去设置

407
00:12:07,633 --> 00:12:08,866
第三个就是计算

408
00:12:08,900 --> 00:12:10,166
跟网络的协同

409
00:12:10,200 --> 00:12:12,033
说白了计算跟网络协同

410
00:12:12,033 --> 00:12:15,766
就是做一些并行和计算和通讯的隐藏

411
00:12:15,766 --> 00:12:17,033
我一边在通讯

412
00:12:17,033 --> 00:12:18,633
一边我在计算

413
00:12:18,633 --> 00:12:21,000
是更可能的就并行起来

414
00:12:21,000 --> 00:12:23,000
而不是说计算完之后再通讯

415
00:12:23,000 --> 00:12:24,100
计算完之后再通讯

416
00:12:24,133 --> 00:12:25,633
通讯完之后再计算

417
00:12:26,333 --> 00:12:28,800
那最后一个就是计算跟存储协同

418
00:12:28,800 --> 00:12:30,500
隐藏IO时间

419
00:12:30,566 --> 00:12:31,800
反正一边在计算

420
00:12:31,800 --> 00:12:32,966
一边我的CPU

421
00:12:32,966 --> 00:12:35,000
就在各种各样的存数据

422
00:12:35,000 --> 00:12:35,766
把数据

423
00:12:35,766 --> 00:12:38,966
从hbm又拿回来做多级的缓存

424
00:12:38,966 --> 00:12:40,200
多级的存储

425
00:12:40,200 --> 00:12:40,866
那可以看到

426
00:12:40,900 --> 00:12:42,366
整体整体

427
00:12:42,366 --> 00:12:44,366
zomi觉得整个AI计算集群中心

428
00:12:44,366 --> 00:12:46,200
建完所有的硬件之后

429
00:12:46,200 --> 00:12:47,066
就后面

430
00:12:47,100 --> 00:12:48,800
就像建房一样

431
00:12:48,800 --> 00:12:52,100
做完硬装要做全栈的软装

432
00:12:52,700 --> 00:12:53,800
那最后一个

433
00:12:53,800 --> 00:12:56,200
就可能大家也会简单的忽略掉

434
00:12:56,200 --> 00:12:58,433
就是整个计算集群的一个高可用

435
00:12:58,433 --> 00:12:59,800
和高运维

436
00:12:59,900 --> 00:13:02,333
说白了现在的整个集群

437
00:13:02,333 --> 00:13:03,333
非常的大

438
00:13:03,333 --> 00:13:05,366
动辄就拉起一个万卡集群

439
00:13:05,466 --> 00:13:07,466
很多的万卡集训的器件

440
00:13:07,500 --> 00:13:08,500
都非常的多

441
00:13:08,500 --> 00:13:10,133
而且故障率非常的高

442
00:13:10,133 --> 00:13:11,600
管理起来非常的难

443
00:13:11,600 --> 00:13:14,000
硬件会有大家能够想象到

444
00:13:14,000 --> 00:13:15,233
和不能够想象到

445
00:13:15,233 --> 00:13:16,033
各种问题

446
00:13:16,033 --> 00:13:16,433
zomi

447
00:13:16,433 --> 00:13:18,400
就曾经在AI的计算中心里面

448
00:13:18,400 --> 00:13:20,600
就遇到光模块被污染

449
00:13:20,600 --> 00:13:22,000
但是查了很久才发现

450
00:13:22,000 --> 00:13:23,500
某个光模块被污染

451
00:13:23,533 --> 00:13:24,000
就为什么

452
00:13:24,000 --> 00:13:26,266
他某个光模块或者某个链路

453
00:13:26,366 --> 00:13:27,566
他的通讯量

454
00:13:27,633 --> 00:13:30,100
还有他的一个通讯效率突然降

455
00:13:30,133 --> 00:13:31,533
这大家各种各样的排查

456
00:13:31,533 --> 00:13:32,633
最后去到机房

457
00:13:32,633 --> 00:13:34,000
发现隔壁机房

458
00:13:34,000 --> 00:13:35,500
原来还在同步的建

459
00:13:35,533 --> 00:13:36,000
所以说

460
00:13:36,000 --> 00:13:37,233
有很多灰尘

461
00:13:37,733 --> 00:13:38,500
那这个讲完

462
00:13:38,500 --> 00:13:39,100
元器件中

463
00:13:39,100 --> 00:13:40,366
看一下单任务

464
00:13:40,366 --> 00:13:42,466
可以上现在都是一个任务跑

465
00:13:42,500 --> 00:13:45,233
在万卡上面的任何一个任务

466
00:13:45,233 --> 00:13:46,466
节点出现故障

467
00:13:46,500 --> 00:13:48,933
就会导致整个训练是中断

468
00:13:48,933 --> 00:13:49,700
中断之后

469
00:13:49,700 --> 00:13:50,100
说实话

470
00:13:50,100 --> 00:13:52,900
整体的定界定位是非常的复杂

471
00:13:52,900 --> 00:13:54,833
就整个模型hand on

472
00:13:54,833 --> 00:13:56,100
down掉了说实话

473
00:13:56,133 --> 00:13:57,566
有时候你可能真的不知道

474
00:13:57,566 --> 00:13:58,666
为什么会down掉

475
00:13:58,700 --> 00:14:00,300
所以你需要对所有东西

476
00:14:00,300 --> 00:14:01,633
进行一个排查

477
00:14:01,733 --> 00:14:02,366
那第三个

478
00:14:02,366 --> 00:14:03,666
就是整体

479
00:14:03,700 --> 00:14:06,133
看到整个系统级的可靠性

480
00:14:06,166 --> 00:14:07,200
非常的重要

481
00:14:07,200 --> 00:14:09,366
希望能够实现实时的感知

482
00:14:09,366 --> 00:14:10,266
硬件

483
00:14:10,300 --> 00:14:12,166
能够做到智能的快速的定位

484
00:14:12,166 --> 00:14:13,833
和快速的恢复

485
00:14:13,833 --> 00:14:15,633
所以说平均的恢复时长

486
00:14:15,633 --> 00:14:18,433
还有整个万卡集群的稳定性

487
00:14:18,733 --> 00:14:20,566
就变得尤为的重要

488
00:14:20,566 --> 00:14:22,466
反正我按下回车那个按钮

489
00:14:22,500 --> 00:14:24,633
调参侠就在开始看loss

490
00:14:24,633 --> 00:14:26,633
看后面的看板

491
00:14:26,633 --> 00:14:27,500
高可用度

492
00:14:27,533 --> 00:14:29,900
可运维性非常的核心

493
00:14:30,333 --> 00:14:32,566
那刚才讲到三个挑战之后

494
00:14:32,566 --> 00:14:34,366
现在简单的展开一下

495
00:14:34,366 --> 00:14:36,033
整个计算的一个负载任务

496
00:14:36,033 --> 00:14:38,800
特别是刚才讲到的多维的并行

497
00:14:38,833 --> 00:14:39,833
因为多维的并行

498
00:14:39,833 --> 00:14:41,666
会很大程度的去决定

499
00:14:41,700 --> 00:14:44,100
极致性能的使用的效率

500
00:14:44,100 --> 00:14:46,333
特别是单机的一个执行的效率

501
00:14:46,333 --> 00:14:49,200
和集群并行的一个执行的效率

502
00:14:49,200 --> 00:14:49,666
那么现在

503
00:14:49,700 --> 00:14:51,333
整体看一下多维并行里面

504
00:14:51,333 --> 00:14:52,566
因为为什么会ZOMI

505
00:14:52,566 --> 00:14:54,300
会重点的强调这一个点

506
00:14:54,333 --> 00:14:56,033
是因为现在的一个

507
00:14:56,033 --> 00:14:57,633
大模型的训练的业务

508
00:14:57,633 --> 00:15:00,966
直接在所有的服务器上面去训

509
00:15:00,966 --> 00:15:03,366
也就是一个大模型训练的任务

510
00:15:03,366 --> 00:15:04,700
在一个万卡里面

511
00:15:04,766 --> 00:15:07,000
所以说计算通讯跟存储

512
00:15:07,000 --> 00:15:08,066
算存网

513
00:15:08,100 --> 00:15:09,800
整体是紧耦合

514
00:15:09,800 --> 00:15:11,500
那看一下左边的这个图

515
00:15:11,533 --> 00:15:12,300
可以看到

516
00:15:12,300 --> 00:15:14,233
上面的这个是计算节点

517
00:15:14,233 --> 00:15:15,166
也就这一坨

518
00:15:15,166 --> 00:15:17,366
下面这个也是计算节点

519
00:15:17,366 --> 00:15:18,900
所有的计算节点

520
00:15:18,933 --> 00:15:19,700
就中间

521
00:15:19,700 --> 00:15:22,033
有可能放一个服务器

522
00:15:22,033 --> 00:15:23,200
或者交换机

523
00:15:23,200 --> 00:15:24,766
中间主要是放交换机

524
00:15:24,766 --> 00:15:25,900
但是有一些交换机

525
00:15:25,933 --> 00:15:26,800
会放在上面

526
00:15:26,800 --> 00:15:28,400
或者超节点的交换机

527
00:15:28,400 --> 00:15:30,000
会放在中间这一排

528
00:15:30,000 --> 00:15:31,066
不管怎么样

529
00:15:31,100 --> 00:15:32,166
计算

530
00:15:32,166 --> 00:15:34,366
都都是包裹着整体的参数面

531
00:15:34,366 --> 00:15:36,433
网络的那所谓的参数面网络

532
00:15:36,433 --> 00:15:38,266
就主要是存一个网络

533
00:15:38,300 --> 00:15:41,000
模型的训练的参数

534
00:15:41,000 --> 00:15:42,666
但那左边的这个存储服务器哈

535
00:15:42,700 --> 00:15:43,800
数据面就算

536
00:15:43,800 --> 00:15:45,166
可以简单的提

537
00:15:45,166 --> 00:15:46,366
可以看到数据

538
00:15:46,366 --> 00:15:47,766
这些连线非常多

539
00:15:47,766 --> 00:15:50,800
每一条连线就是对应右边

540
00:15:50,933 --> 00:15:53,200
各种各样的通讯

541
00:15:53,200 --> 00:15:54,100
那可以看到

542
00:15:54,133 --> 00:15:55,433
就是不同的节点

543
00:15:55,433 --> 00:15:56,433
现在大模型很多

544
00:15:56,433 --> 00:15:58,466
需要进行模型的并行

545
00:15:58,500 --> 00:15:59,233
比如TP

546
00:15:59,233 --> 00:16:00,400
Tensor parallel

547
00:16:00,466 --> 00:16:03,466
然后从节点之间进行一个互存

548
00:16:03,500 --> 00:16:04,166
那另外的话

549
00:16:04,166 --> 00:16:06,000
还有一些PP并行

550
00:16:06,000 --> 00:16:07,066
还有DP并行

551
00:16:07,100 --> 00:16:07,933
跨域的并行

552
00:16:07,933 --> 00:16:09,366
从存储服务器

553
00:16:09,366 --> 00:16:10,566
拿到很多的数据

554
00:16:10,566 --> 00:16:13,266
给到不同的节点下面也是一样

555
00:16:13,300 --> 00:16:14,533
还有训练的过程当中

556
00:16:14,533 --> 00:16:16,233
CKPT

557
00:16:16,233 --> 00:16:17,633
也就是训练的模型

558
00:16:17,633 --> 00:16:19,200
会把它周期性

559
00:16:19,200 --> 00:16:20,833
写回存储服务器

560
00:16:20,833 --> 00:16:21,466
所以这里面

561
00:16:21,500 --> 00:16:22,633
大家可以停下来自

562
00:16:22,633 --> 00:16:23,466
己去看一下

563
00:16:23,500 --> 00:16:26,300
每一条连线到底代表的是什么

564
00:16:26,800 --> 00:16:27,433
从这个图

565
00:16:27,433 --> 00:16:28,266
可以看到

566
00:16:28,300 --> 00:16:31,533
整体的整个智算中心的业务

567
00:16:31,833 --> 00:16:34,400
除了整个单机单卡的性能提升以外

568
00:16:34,400 --> 00:16:37,033
很重要的是处理并行的效率

569
00:16:37,033 --> 00:16:38,633
通讯的效率

570
00:16:38,633 --> 00:16:39,566
各种各样

571
00:16:39,600 --> 00:16:40,766
还有处理周期性

572
00:16:40,766 --> 00:16:42,866
跟故障时的一个快速恢复

573
00:16:42,900 --> 00:16:43,800
所以这里面说

574
00:16:43,800 --> 00:16:44,800
计算通讯

575
00:16:44,800 --> 00:16:47,800
存储是一个非常紧耦合的关系

576
00:16:47,800 --> 00:16:50,566
需要进行全栈的优化

577
00:16:50,566 --> 00:16:51,666
包括机房

578
00:16:51,700 --> 00:16:53,600
一个交换机怎么摆

579
00:16:53,600 --> 00:16:55,000
都成为了一个问题

580
00:16:55,000 --> 00:16:56,700
因为你的网络连接线

581
00:16:56,733 --> 00:16:58,233
拓扑越多

582
00:16:58,233 --> 00:17:00,800
到了3级到4级的拓扑

583
00:17:00,933 --> 00:17:01,366
这个时候

584
00:17:01,366 --> 00:17:03,400
会非常影响通讯的带宽

585
00:17:03,400 --> 00:17:04,166
讲到拓扑

586
00:17:04,166 --> 00:17:07,033
就跟智算中心的一个设计

587
00:17:07,033 --> 00:17:08,233
非常强相关

588
00:17:08,233 --> 00:17:11,800
所以说软硬件全栈的协同问题

589
00:17:11,800 --> 00:17:13,366
挑战很多

590
00:17:14,366 --> 00:17:14,833
了解完

591
00:17:14,833 --> 00:17:17,500
整个AI计算集群的整体挑战之后

592
00:17:17,533 --> 00:17:18,533
现在来看到

593
00:17:18,533 --> 00:17:20,966
ZOMI觉得是所有的视频里面

594
00:17:20,966 --> 00:17:22,266
最核心的一个内容

595
00:17:22,300 --> 00:17:25,700
就是整个AIInfra的一个整体的架构

596
00:17:25,700 --> 00:17:26,166
看一下

597
00:17:26,166 --> 00:17:28,200
整个AIInfra应该长什么样子

598
00:17:28,200 --> 00:17:28,800
那这里面

599
00:17:28,800 --> 00:17:31,066
ZOMI分开两层来去看

600
00:17:31,100 --> 00:17:32,366
第一层就是硬件

601
00:17:32,366 --> 00:17:33,033
硬装

602
00:17:33,033 --> 00:17:34,600
第二层就是软件

603
00:17:34,600 --> 00:17:35,800
对应的软装

604
00:17:36,066 --> 00:17:36,800
那不管怎么样

605
00:17:36,800 --> 00:17:38,700
还是总体的去看一下

606
00:17:38,733 --> 00:17:40,700
首先说所谓的硬装

607
00:17:40,700 --> 00:17:42,700
就ZOMI称为AI集群

608
00:17:42,733 --> 00:17:44,800
硬件的解决方案了 

609
00:17:44,800 --> 00:17:47,766
ZOMI更多的应该称为硬件解决方案

610
00:17:47,766 --> 00:17:48,633
那硬件解决方案

611
00:17:48,633 --> 00:17:50,666
这里面就划分成为两层

612
00:17:50,700 --> 00:17:53,533
第一层就是楼宇的建设

613
00:17:53,533 --> 00:17:54,400
就是机房

614
00:17:54,400 --> 00:17:55,800
楼宇配配套的一个

615
00:17:55,800 --> 00:17:58,466
风火水电相关的内容

616
00:17:58,500 --> 00:18:00,600
需要有对应的一个核心

617
00:18:00,600 --> 00:18:02,600
放计算的集群

618
00:18:02,600 --> 00:18:04,033
一些核心的机房

619
00:18:04,033 --> 00:18:04,666
机房旁边

620
00:18:04,700 --> 00:18:06,433
就会配备一些中低高压

621
00:18:06,433 --> 00:18:07,900
不同的一个配备的电室

622
00:18:07,933 --> 00:18:09,133
还有UPS的电室

623
00:18:09,133 --> 00:18:10,933
当然还会有一些电池室

624
00:18:10,933 --> 00:18:12,166
方便断电的时候

625
00:18:12,166 --> 00:18:12,966
做一个蓄电

626
00:18:12,966 --> 00:18:14,833
当然还会有一些冷却塔

627
00:18:14,833 --> 00:18:16,033
因为整体的夜冷

628
00:18:16,033 --> 00:18:17,000
风冷的时候

629
00:18:17,000 --> 00:18:18,466
还是有非常多的冷却塔

630
00:18:18,500 --> 00:18:20,533
如果大家去过一些计算中心之后

631
00:18:20,533 --> 00:18:22,600
你会发现旁边有非常多的大罐罐

632
00:18:22,600 --> 00:18:23,466
那这些大罐罐

633
00:18:23,500 --> 00:18:25,966
主要就是冷却塔或者冷冻站

634
00:18:25,966 --> 00:18:28,300
另外的话可能还会有一些主控室

635
00:18:28,333 --> 00:18:29,633
备件室还有会议室

636
00:18:29,633 --> 00:18:30,900
各种各样的实验室

637
00:18:30,933 --> 00:18:32,333
或者大家在这里面研讨

638
00:18:32,333 --> 00:18:34,533
或者这里面开个玻璃可以看

639
00:18:34,533 --> 00:18:35,366
那不管怎么样

640
00:18:35,366 --> 00:18:37,066
整体楼宇的建设

641
00:18:37,100 --> 00:18:38,500
就变成了L0

642
00:18:38,533 --> 00:18:40,433
主要是负责刚才讲到的机房

643
00:18:40,433 --> 00:18:41,466
楼宇的配套

644
00:18:41,500 --> 00:18:43,133
还有风火水电

645
00:18:43,333 --> 00:18:45,800
那讲完了解L0层之后

646
00:18:45,800 --> 00:18:47,466
就到了L1层

647
00:18:47,500 --> 00:18:50,300
L1层就是物理的基础设施

648
00:18:50,300 --> 00:18:51,366
所谓的基础设施

649
00:18:51,366 --> 00:18:54,800
就是刚才讲到的供电，制冷，布线

650
00:18:54,800 --> 00:18:57,800
机柜，安防等相关的内容了 

651
00:18:57,800 --> 00:19:00,400
就相对有点以硬装的意味

652
00:19:00,400 --> 00:19:02,100
液冷塔怎么走线

653
00:19:02,133 --> 00:19:03,966
给到核心的机房

654
00:19:03,966 --> 00:19:06,033
然后整体的核心的机房之间

655
00:19:06,033 --> 00:19:09,066
上层的机架到底是怎么去布线

656
00:19:09,100 --> 00:19:11,733
因为有可能不仅仅只有一个核心机房

657
00:19:11,733 --> 00:19:13,600
可能有四五个核心机房

658
00:19:13,600 --> 00:19:15,200
在一个平面层

659
00:19:15,200 --> 00:19:17,633
那四五个核心机房之间的布线

660
00:19:17,733 --> 00:19:19,366
是怎么进行互传

661
00:19:19,366 --> 00:19:20,566
因为这里面有可能

662
00:19:20,566 --> 00:19:21,466
这大家看到这有

663
00:19:21,500 --> 00:19:23,033
个箱子这些箱子

664
00:19:23,033 --> 00:19:25,500
有可能就是传到另外一个机房

665
00:19:25,533 --> 00:19:26,733
一个路由器

666
00:19:27,000 --> 00:19:27,866
那这个时候

667
00:19:27,900 --> 00:19:30,433
就非常讲究整体的布线

668
00:19:30,433 --> 00:19:32,700
智能安防等相关的内容

669
00:19:32,733 --> 00:19:34,200
包括供电

670
00:19:34,200 --> 00:19:36,100
电源也可能在这个位置

671
00:19:36,166 --> 00:19:37,200
那第三个

672
00:19:37,200 --> 00:19:38,766
就是最核心

673
00:19:38,766 --> 00:19:41,400
最关心的整体的算力的底座

674
00:19:41,866 --> 00:19:42,766
整体算力底座

675
00:19:42,766 --> 00:19:44,900
更多就围绕着这一块内容

676
00:19:44,933 --> 00:19:47,600
核心机房来去进行建设

677
00:19:47,600 --> 00:19:49,033
所以算算力的底座

678
00:19:49,033 --> 00:19:51,366
就是刚才讲到的算存网

679
00:19:51,433 --> 00:19:52,866
计算中心

680
00:19:52,900 --> 00:19:54,400
计算的服务器

681
00:19:54,400 --> 00:19:56,033
还有存储的系统

682
00:19:56,033 --> 00:19:57,466
还有网络的系统

683
00:19:57,500 --> 00:19:58,400
然后存储系统

684
00:19:58,400 --> 00:19:59,666
就包括存储的PoD

685
00:19:59,733 --> 00:20:00,300
有可能

686
00:20:00,300 --> 00:20:01,400
这里面几台机器

687
00:20:01,400 --> 00:20:04,100
就单独划分回存储的PoD

688
00:20:04,166 --> 00:20:05,133
存储的系统

689
00:20:05,133 --> 00:20:07,033
然后每个计算中心里面

690
00:20:07,033 --> 00:20:08,433
或者每个计算集群里面

691
00:20:08,433 --> 00:20:09,500
可能左右两边

692
00:20:09,533 --> 00:20:11,100
就是放计算的服务器

693
00:20:11,100 --> 00:20:11,966
中间的这一块

694
00:20:11,966 --> 00:20:14,466
就是放对应的交换机

695
00:20:14,566 --> 00:20:16,100
那交换机跟交换机之间

696
00:20:16,133 --> 00:20:17,300
柜跟柜之间

697
00:20:17,300 --> 00:20:19,333
每一排跟每一排计算节点之间

698
00:20:19,333 --> 00:20:20,733
就进行一个互传

699
00:20:20,733 --> 00:20:23,200
这个就对应的算力底座

700
00:20:23,200 --> 00:20:24,966
了解完硬件之后

701
00:20:24,966 --> 00:20:26,666
现在来看看软件

702
00:20:26,700 --> 00:20:28,600
软件你还是从这里面进行切分

703
00:20:28,600 --> 00:20:31,800
看一下L3跟L4软件有什么区别

704
00:20:31,800 --> 00:20:33,500
那软件的L3层

705
00:20:33,533 --> 00:20:34,966
就算力底座之上

706
00:20:34,966 --> 00:20:37,233
叫做算力使能平台

707
00:20:37,233 --> 00:20:38,400
那算力使能平台

708
00:20:38,400 --> 00:20:40,033
就更多的往上看一下

709
00:20:40,033 --> 00:20:41,300
有哪些内容

710
00:20:41,800 --> 00:20:44,100
看一下软件上面其实是有多算力

711
00:20:44,133 --> 00:20:44,600
有昇腾

712
00:20:44,600 --> 00:20:45,033
有鲲鹏

713
00:20:45,033 --> 00:20:46,300
有GPU 有X86

714
00:20:46,333 --> 00:20:48,000
还有存储的子系统

715
00:20:48,000 --> 00:20:50,033
NFS各种各样的一些

716
00:20:50,033 --> 00:20:51,700
内容那不管怎么样

717
00:20:51,733 --> 00:20:52,600
算力

718
00:20:52,600 --> 00:20:54,066
就是往上

719
00:20:54,100 --> 00:20:55,500
需要有算力服务的底座

720
00:20:55,500 --> 00:20:57,600
还有ZOMI之前分享的AI system

721
00:20:57,600 --> 00:20:58,666
相关的内容

722
00:20:58,800 --> 00:21:00,833
从对应的计算的架构

723
00:21:00,833 --> 00:21:02,300
例如英伟达有自己的CUDA啦

724
00:21:02,333 --> 00:21:03,366
华为有自己的CANN

725
00:21:03,366 --> 00:21:04,466
各种各样的东西

726
00:21:04,533 --> 00:21:05,933
然后华为有多租户

727
00:21:05,933 --> 00:21:06,600
虚拟化

728
00:21:06,600 --> 00:21:07,866
分布式计算

729
00:21:07,900 --> 00:21:09,433
Pytorch AI框架

730
00:21:09,433 --> 00:21:12,100
还有各种各样的推理引擎相关

731
00:21:12,133 --> 00:21:13,833
算力的使能层

732
00:21:13,833 --> 00:21:15,366
那算力所有的使能层 

733
00:21:15,366 --> 00:21:17,033
都在算力服务底座

734
00:21:17,033 --> 00:21:17,700
那最后

735
00:21:17,733 --> 00:21:19,500
就还有一些应用与服务

736
00:21:19,500 --> 00:21:20,833
现在的MaaS

737
00:21:20,833 --> 00:21:23,800
model as service

738
00:21:23,800 --> 00:21:25,900
一些云服务

739
00:21:25,933 --> 00:21:28,200
或者AI的模型的服务

740
00:21:28,200 --> 00:21:28,866
那这些

741
00:21:28,900 --> 00:21:30,733
都在应用与服务层

742
00:21:30,733 --> 00:21:32,300
里面可以提供一些大数据

743
00:21:32,300 --> 00:21:33,166
互联网HPC

744
00:21:33,166 --> 00:21:34,300
AI相关的功能

745
00:21:34,333 --> 00:21:36,433
当然在AI集群计算中心里面

746
00:21:36,433 --> 00:21:38,366
主要是对外的提供了AI的功能

747
00:21:38,366 --> 00:21:41,300
所以叫做MaaS的model as service

748
00:21:41,333 --> 00:21:41,733
对外

749
00:21:41,733 --> 00:21:43,633
提供各种各样的模型的API服务

750
00:21:43,633 --> 00:21:45,433
模型的接口也好

751
00:21:45,433 --> 00:21:45,800
然后

752
00:21:45,800 --> 00:21:48,966
再封装成为整体的AI计算中心

753
00:21:48,966 --> 00:21:51,000
就是用刚才讲到的L0 L1 L2

754
00:21:51,000 --> 00:21:53,833
L3 L4 这几层去组成

755
00:21:53,833 --> 00:21:56,266
那可以看到整个AI集群的架构

756
00:21:56,300 --> 00:21:58,700
是非常的重要

757
00:21:58,700 --> 00:21:59,333
那这里面

758
00:21:59,333 --> 00:22:00,833
刚才讲到了分开好几层

759
00:22:00,833 --> 00:22:01,266
第一层

760
00:22:01,300 --> 00:22:02,366
可能L0跟L1

761
00:22:02,366 --> 00:22:03,966
就更多的就放在一起

762
00:22:03,966 --> 00:22:07,100
zomi更愿意称它为一个物理的配套

763
00:22:07,133 --> 00:22:10,300
就刚才讲到的整体的L0 L1 L2

764
00:22:10,300 --> 00:22:12,033
是硬件的底座

765
00:22:12,033 --> 00:22:13,366
或者整体的硬件

766
00:22:13,366 --> 00:22:14,166
的解决方案

767
00:22:14,166 --> 00:22:15,066
那L0 L1

768
00:22:15,100 --> 00:22:16,500
更多的是物理的配套

769
00:22:16,500 --> 00:22:17,166
提供能耗

770
00:22:17,166 --> 00:22:18,300
供电承重

771
00:22:18,333 --> 00:22:19,100
整体的内容

772
00:22:19,100 --> 00:22:22,100
尽可能的降低整体的PUE

773
00:22:22,100 --> 00:22:24,566
集群中心的一个能耗的指标

774
00:22:24,566 --> 00:22:26,266
那L2就是算力底座

775
00:22:26,300 --> 00:22:27,133
就比较明确

776
00:22:27,133 --> 00:22:29,133
现在大家都搞的算存网

777
00:22:29,133 --> 00:22:30,766
那长稳的算力底座

778
00:22:30,766 --> 00:22:32,266
达成极致的MFU

779
00:22:32,300 --> 00:22:34,200
就是努力追求的目标

780
00:22:34,200 --> 00:22:35,066
当然L3层

781
00:22:35,100 --> 00:22:37,133
就是算力的调度和使能层

782
00:22:37,133 --> 00:22:39,133
算力使能层主要提供一些paas

783
00:22:39,133 --> 00:22:40,233
saas maas

784
00:22:40,233 --> 00:22:41,466
各种各样的内容

785
00:22:41,500 --> 00:22:44,333
提升AI训练的一个能效

786
00:22:44,333 --> 00:22:46,900
有支撑集群的高可用

787
00:22:46,900 --> 00:22:48,533
通过这么几层

788
00:22:48,533 --> 00:22:51,766
真正的实现整个AI集群的解决方案

789
00:22:51,766 --> 00:22:53,066
那AIInfra

790
00:22:53,100 --> 00:22:54,833
就刚才第一个问题

791
00:22:54,833 --> 00:22:55,766
第二个问题

792
00:22:55,766 --> 00:22:58,166
一开始的时候就整个AIInfra

793
00:22:58,166 --> 00:23:00,033
实际上就包括了L4 L2

794
00:23:00,033 --> 00:23:02,400
L3 L1 L0 各种各样

795
00:23:02,400 --> 00:23:03,300
每一层

796
00:23:03,333 --> 00:23:06,300
其实都属于整个AIInfra的解决方案

797
00:23:06,300 --> 00:23:08,100
里面的其中一环 

798
00:23:10,033 --> 00:23:12,800
那现在要举一个非常具体的例子

799
00:23:12,800 --> 00:23:14,700
就是某某某客户

800
00:23:14,733 --> 00:23:16,033
那某某某客户

801
00:23:16,033 --> 00:23:18,433
现在要买一个AI集群中心

802
00:23:18,433 --> 00:23:21,500
例如腾讯现在要买一个AI集群中心

803
00:23:21,533 --> 00:23:22,966
解决方案

804
00:23:22,966 --> 00:23:24,700
厂商就例如天数

805
00:23:24,733 --> 00:23:25,766
华为寒武纪

806
00:23:26,300 --> 00:23:26,933
英伟达

807
00:23:26,933 --> 00:23:29,166
就对应的解决方案的厂商

808
00:23:29,166 --> 00:23:30,233
开源的组件

809
00:23:30,233 --> 00:23:32,166
主要是指在Git

810
00:23:32,166 --> 00:23:35,000
git github啦

811
00:23:35,100 --> 00:23:37,833
git hub上面托管的一些开源的组件

812
00:23:37,833 --> 00:23:40,066
那同样的还是分开l0

813
00:23:40,100 --> 00:23:42,633
l12l3l4

814
00:23:42,633 --> 00:23:44,500
每一层去看一下L0

815
00:23:44,533 --> 00:23:45,733
层物理配套层

816
00:23:45,833 --> 00:23:47,166
L2算力底座

817
00:23:47,366 --> 00:23:49,033
L智算使能层

818
00:23:49,033 --> 00:23:50,300
L4大模型的应用层

819
00:23:50,333 --> 00:23:52,366
逐层的去打开看一下

820
00:23:52,366 --> 00:23:55,033
到底客户需要提供什么

821
00:23:55,033 --> 00:23:56,233
解决方案厂商

822
00:23:56,233 --> 00:23:57,366
需要提供什么

823
00:23:57,366 --> 00:23:59,200
要使用哪些开源组件

824
00:23:59,233 --> 00:23:59,966
那首先

825
00:23:59,966 --> 00:24:03,200
比较明确的就是L0跟L1物理的配套

826
00:24:03,200 --> 00:24:04,233
说白了物理的配套

827
00:24:04,233 --> 00:24:05,300
可以看到这里面

828
00:24:05,333 --> 00:24:06,600
绿色的比较多

829
00:24:06,600 --> 00:24:08,300
就整体的机房的建设啦

830
00:24:08,300 --> 00:24:09,800
液冷方舱的建设啦

831
00:24:09,800 --> 00:24:11,066
还有一些小母线

832
00:24:11,100 --> 00:24:12,400
还有末端的空调

833
00:24:12,400 --> 00:24:14,466
其实大部分都是由客户自己去承担

834
00:24:14,500 --> 00:24:16,166
但客户也可以放出去

835
00:24:16,166 --> 00:24:18,500
让更多的第三方拿去承载

836
00:24:18,533 --> 00:24:19,200
但不管怎么样

837
00:24:19,200 --> 00:24:20,633
这些是基建的问题

838
00:24:20,633 --> 00:24:22,466
那可能解决方案的厂商

839
00:24:22,500 --> 00:24:23,300
就比较明确

840
00:24:23,300 --> 00:24:25,566
就需要用到液冷的机柜

841
00:24:25,566 --> 00:24:27,200
那这个SuperPoD超节点这些

842
00:24:27,200 --> 00:24:29,366
或者NVL 72 

843
00:24:29,366 --> 00:24:31,766
华为的那个cloudmatrix 都是这样

844
00:24:31,766 --> 00:24:34,266
就基本上都会配套有对应的内容

845
00:24:34,300 --> 00:24:36,233
所以这个内容或者这一块

846
00:24:36,233 --> 00:24:38,400
就有解决方案厂商来提供

847
00:24:38,600 --> 00:24:41,433
接着看一下整体的算力的底座

848
00:24:41,433 --> 00:24:43,000
算力底座就算存网

849
00:24:43,000 --> 00:24:45,100
这里面可以看到整体的算力底座

850
00:24:45,133 --> 00:24:46,300
红色的框框

851
00:24:46,300 --> 00:24:48,000
还是黄色

852
00:24:48,000 --> 00:24:51,433
黄色对对黄色的黄框黄色

853
00:24:56,166 --> 00:24:58,033
黄色的框框会比较多

854
00:24:58,033 --> 00:25:00,066
那最重要的三大件

855
00:25:00,100 --> 00:25:02,833
计算网络还有存储

856
00:25:02,833 --> 00:25:03,600
都是大部分

857
00:25:03,600 --> 00:25:05,600
都是解决方案厂商来提供

858
00:25:05,600 --> 00:25:06,300
但存储

859
00:25:06,333 --> 00:25:09,166
有些可能会原始的厂商已经有

860
00:25:09,166 --> 00:25:10,033
就不需要

861
00:25:10,033 --> 00:25:13,433
但是计算能网络是非常的强绑定

862
00:25:13,433 --> 00:25:15,800
那为什么业务面会单独由绿色

863
00:25:15,800 --> 00:25:18,600
是因为业务面主要是由云管控服务

864
00:25:18,666 --> 00:25:19,700
很多客户自己

865
00:25:19,733 --> 00:25:21,566
会提供一些计算管控的服务

866
00:25:21,566 --> 00:25:22,466
网络管控的服务

867
00:25:22,500 --> 00:25:23,800
存储管控的服务

868
00:25:23,800 --> 00:25:25,300
这些管控的服务面

869
00:25:25,333 --> 00:25:26,833
会通过整个业务面

870
00:25:26,833 --> 00:25:29,400
进行控制到算存网三个内容

871
00:25:29,400 --> 00:25:30,566
所以说在网络里面

872
00:25:30,566 --> 00:25:31,566
可能业务面

873
00:25:31,566 --> 00:25:33,466
更多的是厂商

874
00:25:33,500 --> 00:25:35,100
客户自己提供

875
00:25:35,466 --> 00:25:37,100
那另外刚才讲到

876
00:25:37,133 --> 00:25:39,033
有了算存网之后

877
00:25:39,033 --> 00:25:40,700
整体的异构计算架构

878
00:25:40,733 --> 00:25:42,400
那英伟达有自己的CUDA

879
00:25:42,400 --> 00:25:44,500
昇腾华为有自己的CANN

880
00:25:44,533 --> 00:25:45,833
来去控制计算

881
00:25:45,833 --> 00:25:47,566
充分的发挥算力

882
00:25:47,566 --> 00:25:49,000
提供相关的算子

883
00:25:49,000 --> 00:25:51,433
当然了还会有一些运维的服务

884
00:25:51,433 --> 00:25:52,166
那运维的服务

885
00:25:52,166 --> 00:25:54,200
你其实发现很多运维的系统

886
00:25:54,200 --> 00:25:57,266
实际上是由计算跟网络的厂商

887
00:25:57,366 --> 00:25:58,600
去单独提供

888
00:25:58,600 --> 00:25:59,833
然后客户

889
00:25:59,833 --> 00:26:02,466
更多的是做一个运维服务的集成

890
00:26:02,500 --> 00:26:03,566
相关的内容

891
00:26:03,566 --> 00:26:04,233
那可以看到

892
00:26:04,233 --> 00:26:05,566
客户自己其实比较惨

893
00:26:05,566 --> 00:26:06,566
主要是卖算力

894
00:26:06,566 --> 00:26:07,633
但是买的算力

895
00:26:07,633 --> 00:26:09,100
都是买英伟达或者华为

896
00:26:09,133 --> 00:26:09,900
是最赚钱

897
00:26:09,900 --> 00:26:10,900
就是这一层

898
00:26:10,900 --> 00:26:13,166
大家都铆足劲在这一层赚钱

899
00:26:13,166 --> 00:26:13,866
而这一层

900
00:26:13,900 --> 00:26:15,600
也称为AIInfra层

901
00:26:15,600 --> 00:26:17,633
其中的硬件最核心的内容

902
00:26:17,700 --> 00:26:19,766
那接着看一下iAIInfra层

903
00:26:19,766 --> 00:26:21,700
上面还有AIInfra层的算力

904
00:26:21,733 --> 00:26:23,033
智算的使能层

905
00:26:23,033 --> 00:26:25,100
说白了刚才有了cann之后

906
00:26:25,133 --> 00:26:27,166
会有了一个算力使能

907
00:26:27,166 --> 00:26:29,066
之后往上就是容器

908
00:26:29,100 --> 00:26:30,100
现在的容器

909
00:26:30,100 --> 00:26:32,033
可能用的更多的是K8S

910
00:26:32,033 --> 00:26:33,066
那有了K8S之后

911
00:26:33,100 --> 00:26:34,800
会基于容器去部署

912
00:26:34,800 --> 00:26:35,466
在训练的时候

913
00:26:35,500 --> 00:26:37,300
可能会部署Pytorch

914
00:26:37,300 --> 00:26:38,500
各种各样的框架

915
00:26:38,500 --> 00:26:38,733
当然

916
00:26:38,733 --> 00:26:41,200
这些框架基本上业界已经不维护

917
00:26:41,200 --> 00:26:41,866
那在Huggingface 

918
00:26:41,900 --> 00:26:42,233
上面

919
00:26:42,233 --> 00:26:45,000
其实也没有这些框架的一些模型

920
00:26:45,000 --> 00:26:45,800
所以说现在

921
00:26:45,800 --> 00:26:47,233
Pytorch一枝独大

922
00:26:47,233 --> 00:26:48,000
另外的话

923
00:26:48,000 --> 00:26:50,500
可能还会用到一些推理引擎

924
00:26:50,566 --> 00:26:51,366
有vllm啦

925
00:26:51,366 --> 00:26:52,466
sglang的啦这些

926
00:26:52,500 --> 00:26:54,200
但可能服务器厂商

927
00:26:54,200 --> 00:26:56,366
像华为昇腾推出自己的MindIE

928
00:26:56,366 --> 00:26:58,500
然后英伟达推出自己的TensorRT

929
00:26:58,566 --> 00:26:59,500
各种各样的内容

930
00:26:59,533 --> 00:27:00,800
但是其实已经不多

931
00:27:00,800 --> 00:27:03,166
业界已经全面的转向开源

932
00:27:03,166 --> 00:27:04,900
所以说智算使能层

933
00:27:04,933 --> 00:27:08,100
说实话如果你做AIInfra或你做什么

934
00:27:08,166 --> 00:27:10,766
说实话这一层是比较难赚钱

935
00:27:10,766 --> 00:27:13,166
那可能更多的是你对外

936
00:27:13,166 --> 00:27:14,833
提供一个智算的平台

937
00:27:14,833 --> 00:27:16,666
卖服务卖算力

938
00:27:16,700 --> 00:27:17,600
那这种情况

939
00:27:17,600 --> 00:27:19,166
AIInfra软件厂商

940
00:27:19,166 --> 00:27:20,633
才能够喝点汤

941
00:27:21,200 --> 00:27:22,766
那了解完智算使能层之后

942
00:27:22,766 --> 00:27:24,833
看一下大模型的应用层

943
00:27:24,833 --> 00:27:27,000
那现在商汤为什么扶摇直下

944
00:27:27,000 --> 00:27:28,900
身为整个大模型的应用层

945
00:27:28,933 --> 00:27:31,133
现在开源已经非常的多

946
00:27:31,133 --> 00:27:32,433
所以说大家这一层

947
00:27:32,433 --> 00:27:33,966
也是很难赚钱

948
00:27:33,966 --> 00:27:34,833
随着开源

949
00:27:34,833 --> 00:27:35,966
所以说你会发现

950
00:27:35,966 --> 00:27:37,500
绿色的很少

951
00:27:37,533 --> 00:27:38,700
就自建的很少

952
00:27:38,700 --> 00:27:41,133
更多的是利用灰色的开源的组件

953
00:27:41,333 --> 00:27:43,000
包括在分布式加速的时候

954
00:27:43,000 --> 00:27:44,000
英伟达有自己的megatron

955
00:27:44,000 --> 00:27:45,166
微软有DeepSpeed

956
00:27:45,166 --> 00:27:47,266
还有华为有自己的MindSpeed

957
00:27:47,300 --> 00:27:49,400
当然MindSpeed是套壳megatron

958
00:27:49,400 --> 00:27:50,166
那不管怎么样

959
00:27:50,166 --> 00:27:51,100
可以看到

960
00:27:51,133 --> 00:27:52,800
有了大模型的加速之后

961
00:27:52,800 --> 00:27:55,466
就可以对各种各样的模型

962
00:27:55,500 --> 00:27:56,433
不管是语言

963
00:27:56,433 --> 00:27:58,000
图文多模态的模型

964
00:27:58,000 --> 00:27:59,266
进行加速

965
00:27:59,333 --> 00:27:59,900
那当然

966
00:27:59,900 --> 00:28:02,966
厂商也会提供一个各种各样

967
00:28:02,966 --> 00:28:04,466
英伟达有nvinsight

968
00:28:04,500 --> 00:28:06,933
华为昇腾有自己的mind studio相关

969
00:28:06,933 --> 00:28:09,800
内容去提供到给客户

970
00:28:09,800 --> 00:28:10,766
提供了有什么用

971
00:28:10,766 --> 00:28:11,833
为什么放在这一层

972
00:28:11,833 --> 00:28:13,500
是因为训练模型的时候

973
00:28:13,533 --> 00:28:15,000
这跟模型强相关

974
00:28:15,000 --> 00:28:16,666
模型跑飞了怎么办

975
00:28:16,766 --> 00:28:19,366
然后怎么去看Timeline的热图

976
00:28:19,400 --> 00:28:21,300
怎么去做性能的调优

977
00:28:21,333 --> 00:28:23,766
都通过厂商提供的那些内容

978
00:28:23,766 --> 00:28:25,100
就不管是英伟达还是华为

979
00:28:25,133 --> 00:28:26,533
昇腾了还是寒武纪

980
00:28:26,533 --> 00:28:28,733
天数相关都会提供这么一个内容

981
00:28:28,733 --> 00:28:30,300
所以说整体全栈

982
00:28:30,700 --> 00:28:32,433
基本上就是这么去划分

983
00:28:32,433 --> 00:28:34,433
那简单的概括一下

984
00:28:34,433 --> 00:28:36,966
首先如果客户要买一个万卡集群

985
00:28:36,966 --> 00:28:38,800
或者客户要要建一个万卡集群

986
00:28:38,800 --> 00:28:40,500
很重要的就是大部分这些

987
00:28:40,533 --> 00:28:42,000
都是自己建

988
00:28:42,000 --> 00:28:44,633
然后服务器厂商或者计算的厂商

989
00:28:44,633 --> 00:28:45,700
芯片的厂商

990
00:28:45,766 --> 00:28:48,266
提供更多的是这一层的内容

991
00:28:48,266 --> 00:28:49,233
那当然

992
00:28:49,233 --> 00:28:50,900
这一层会跟一些客户

993
00:28:50,933 --> 00:28:52,566
进行联合去基建

994
00:28:52,566 --> 00:28:53,966
就所谓的磨合的阶段

995
00:28:53,966 --> 00:28:55,033
那再往上层

996
00:28:55,033 --> 00:28:56,000
就越来越开放

997
00:28:56,000 --> 00:28:56,766
大家看到没有

998
00:28:56,766 --> 00:28:58,033
灰色的越来越多

999
00:28:58,033 --> 00:28:58,500
那这个

1000
00:28:58,533 --> 00:29:00,333
就是开源组件

1001
00:29:00,366 --> 00:29:01,300
开源的发展

1002
00:29:01,333 --> 00:29:04,133
对整个AI业界一个具体的影响

1003
00:29:04,133 --> 00:29:06,133
就算力的使能层越来越多

1004
00:29:06,133 --> 00:29:07,300
然后模型

1005
00:29:07,300 --> 00:29:08,166
开源的模型

1006
00:29:08,166 --> 00:29:09,033
也越来越多

1007
00:29:09,033 --> 00:29:10,200
而算力的使能

1008
00:29:10,200 --> 00:29:13,766
大家说那个袁进辉老师做硅基流动

1009
00:29:13,766 --> 00:29:14,766
还有尤阳老师

1010
00:29:14,766 --> 00:29:15,633
做一个cursor ai

1011
00:29:16,433 --> 00:29:18,866
说实话也在算力使能层

1012
00:29:18,900 --> 00:29:19,800
但算力使能层

1013
00:29:19,800 --> 00:29:20,800
大部分都是开源

1014
00:29:20,800 --> 00:29:21,400
所以这层

1015
00:29:21,400 --> 00:29:22,866
其实不怎么赚钱

1016
00:29:22,900 --> 00:29:25,400
那做的更多的就是智算的平台

1017
00:29:25,400 --> 00:29:26,766
他们搭一个平台出来

1018
00:29:26,766 --> 00:29:28,833
然后说白了就是卖服务

1019
00:29:28,833 --> 00:29:30,000
卖算力嘛

1020
00:29:30,000 --> 00:29:31,033
那所谓的卖服务

1021
00:29:31,033 --> 00:29:31,566
有可能

1022
00:29:31,566 --> 00:29:32,600
把模型的服务都

1023
00:29:32,600 --> 00:29:33,800
包进来所以

1024
00:29:33,800 --> 00:29:35,700
他们有可能做这一层

1025
00:29:35,733 --> 00:29:36,966
也可能做这一层

1026
00:29:36,966 --> 00:29:37,833
那不管怎么样

1027
00:29:37,833 --> 00:29:38,700
现在

1028
00:29:38,733 --> 00:29:39,766
就基本上了解

1029
00:29:39,766 --> 00:29:43,166
整个AIInfra的全栈的架构啦

1030
00:29:44,100 --> 00:29:45,966
哎一口气讲了好多口水

1031
00:29:46,166 --> 00:29:46,866
那不管怎么样

1032
00:29:46,900 --> 00:29:47,400
这一期视频

1033
00:29:47,400 --> 00:29:49,033
还是zomi觉得非常的核心

1034
00:29:49,033 --> 00:29:50,166
也非常的重要

1035
00:29:50,166 --> 00:29:51,866
之前的视频大家都可以不看

1036
00:29:52,200 --> 00:29:53,100
但是这一期视频

1037
00:29:53,133 --> 00:29:54,400
zomi要重点的推

1038
00:29:54,400 --> 00:29:55,566
非常的核心

1039
00:29:56,100 --> 00:29:57,533
那看到第三个内容

1040
00:29:57,533 --> 00:30:00,000
就整个AI计算集群的建设的目标

1041
00:30:00,000 --> 00:30:00,800
所谓的建设目标

1042
00:30:00,800 --> 00:30:02,700
后面也会有一节

1043
00:30:02,766 --> 00:30:03,800
有一节十几个视频

1044
00:30:03,800 --> 00:30:05,433
或者几三四个视频

1045
00:30:05,433 --> 00:30:07,433
重点的去讲讲计算

1046
00:30:07,433 --> 00:30:08,400
怎么提升效率

1047
00:30:08,400 --> 00:30:09,066
怎么去算

1048
00:30:09,100 --> 00:30:10,433
有可以用哪些算法

1049
00:30:10,433 --> 00:30:13,066
因为这是最核心的课嘛

1050
00:30:13,100 --> 00:30:15,033
所以还是做一个整体的综述

1051
00:30:15,200 --> 00:30:15,900
那不管怎么样

1052
00:30:15,933 --> 00:30:17,833
整个AI集群计算的目标

1053
00:30:17,833 --> 00:30:19,066
就是围绕

1054
00:30:19,100 --> 00:30:21,400
集群的规模越大越好

1055
00:30:21,400 --> 00:30:22,800
就卡数越来越多

1056
00:30:22,800 --> 00:30:23,500
那第二个

1057
00:30:23,533 --> 00:30:25,333
乘以计算的效率

1058
00:30:25,333 --> 00:30:26,300
那集群的规模

1059
00:30:26,300 --> 00:30:28,166
实际上是由单卡算力

1060
00:30:28,166 --> 00:30:30,066
乘以NPU卡数

1061
00:30:30,100 --> 00:30:32,033
所以说单卡数越多

1062
00:30:32,033 --> 00:30:34,400
有效算力不就提升了吗

1063
00:30:34,400 --> 00:30:36,233
那这个就是整体

1064
00:30:36,233 --> 00:30:38,966
AI的有效算力的一个计算方式

1065
00:30:38,966 --> 00:30:40,600
当然了单卡的算力

1066
00:30:40,600 --> 00:30:42,233
有可能更多的是依赖于

1067
00:30:42,233 --> 00:30:44,466
一个纳米的工艺的制成

1068
00:30:44,533 --> 00:30:46,433
NPU的卡数依赖于什么

1069
00:30:46,600 --> 00:30:48,433
越来越荷包

1070
00:30:48,433 --> 00:30:50,200
越来越口袋的钱

1071
00:30:50,200 --> 00:30:51,066
你钱多

1072
00:30:51,100 --> 00:30:52,433
你不就可以建更多的卡吗

1073
00:30:52,433 --> 00:30:53,633
买更多的卡吗

1074
00:30:53,733 --> 00:30:54,733
那另外的话

1075
00:30:54,733 --> 00:30:55,966
卡的厂商

1076
00:30:55,966 --> 00:30:56,766
就昇腾

1077
00:30:56,766 --> 00:30:58,366
英伟达提供的是什么

1078
00:30:58,366 --> 00:30:59,800
算力的效率

1079
00:30:59,800 --> 00:31:01,033
那所谓的计算效率

1080
00:31:01,033 --> 00:31:02,900
就是下面这条公式啦

1081
00:31:03,300 --> 00:31:05,766
计算效率就是单机执行最优

1082
00:31:05,766 --> 00:31:08,000
还有一个集群并行最优

1083
00:31:08,000 --> 00:31:10,300
加上中断时间最短

1084
00:31:10,400 --> 00:31:13,366
来实现计算的效率最大化

1085
00:31:13,366 --> 00:31:15,000
计算效率最大化

1086
00:31:15,000 --> 00:31:17,800
我的单卡算力加NPU的卡数的性能

1087
00:31:17,800 --> 00:31:19,900
就能充分的发挥出来

1088
00:31:19,933 --> 00:31:21,133
那接下来就看一下

1089
00:31:21,133 --> 00:31:23,033
整体的一个计算的效率

1090
00:31:23,033 --> 00:31:24,200
到底是怎么算

1091
00:31:24,233 --> 00:31:26,500
首先单机的执行是最高

1092
00:31:26,533 --> 00:31:27,100
那这个时候

1093
00:31:27,100 --> 00:31:28,200
就很重要

1094
00:31:28,200 --> 00:31:30,266
就讲到模型的算力的利用率

1095
00:31:30,300 --> 00:31:32,833
反正算的更快更省更稳

1096
00:31:32,833 --> 00:31:34,166
那更稳是下面

1097
00:31:34,166 --> 00:31:35,500
是更高效哈

1098
00:31:35,533 --> 00:31:36,633
更稳是下面

1099
00:31:36,633 --> 00:31:38,100
中断时间更短

1100
00:31:38,766 --> 00:31:40,466
更快更省更高效

1101
00:31:40,500 --> 00:31:41,700
那快怎么快

1102
00:31:41,700 --> 00:31:43,500
就单机的算子的效率

1103
00:31:43,500 --> 00:31:44,566
算法的效率

1104
00:31:44,566 --> 00:31:45,766
执行的效率

1105
00:31:45,766 --> 00:31:46,900
i o的效率

1106
00:31:46,933 --> 00:31:49,033
还有GPU跟CPU之间

1107
00:31:49,033 --> 00:31:50,233
下发的效率

1108
00:31:50,233 --> 00:31:50,833
那这些

1109
00:31:50,833 --> 00:31:53,433
都是提升效率的一个过程

1110
00:31:53,633 --> 00:31:54,000
第二个

1111
00:31:54,000 --> 00:31:55,633
就集群并行最优

1112
00:31:55,633 --> 00:31:56,566
集群并行最优

1113
00:31:56,566 --> 00:31:58,000
就讲到的一个假设

1114
00:31:58,000 --> 00:31:58,666
在万卡集群

1115
00:31:58,700 --> 00:31:59,933
甚至10万卡集群

1116
00:31:59,933 --> 00:32:00,500
不管怎么样

1117
00:32:00,500 --> 00:32:01,500
还是百卡

1118
00:32:01,500 --> 00:32:02,700
那个千卡集群

1119
00:32:02,700 --> 00:32:03,166
很重要

1120
00:32:03,166 --> 00:32:04,366
讲究线性度

1121
00:32:04,366 --> 00:32:05,700
尽可能的高效的并行

1122
00:32:05,733 --> 00:32:07,366
高效的通讯

1123
00:32:07,366 --> 00:32:08,000
那并行

1124
00:32:08,000 --> 00:32:10,666
有可能去取决于切分策略

1125
00:32:10,700 --> 00:32:11,400
这里没写

1126
00:32:11,400 --> 00:32:12,300
其实还有一个

1127
00:32:12,333 --> 00:32:15,300
就是取决于一个组网的拓扑

1128
00:32:15,366 --> 00:32:18,833
怎么组网通讯的算法到底是怎么样

1129
00:32:18,833 --> 00:32:20,466
这个也是很核心

1130
00:32:20,500 --> 00:32:21,033
那第三个

1131
00:32:21,033 --> 00:32:22,300
就是增大数据面

1132
00:32:22,333 --> 00:32:23,333
一个传输的带宽

1133
00:32:23,333 --> 00:32:25,033
说白了就是带宽

1134
00:32:25,033 --> 00:32:26,300
带宽要大

1135
00:32:26,333 --> 00:32:28,833
那降低冲突那

1136
00:32:28,833 --> 00:32:29,633
就是哈希冲突

1137
00:32:29,633 --> 00:32:31,000
各种各样的网络的冲突

1138
00:32:31,000 --> 00:32:32,966
提升网络的整体的利用率

1139
00:32:33,066 --> 00:32:35,466
所以说除了并行策略以外

1140
00:32:35,500 --> 00:32:36,833
还有带宽的问题

1141
00:32:36,833 --> 00:32:37,466
硬件的问题

1142
00:32:37,500 --> 00:32:39,000
还有网络算法的问题

1143
00:32:39,000 --> 00:32:41,233
所以大家不要觉得我训练一个大模型

1144
00:32:41,233 --> 00:32:42,800
我懂并行切分就行

1145
00:32:42,800 --> 00:32:44,500
你懂并行切分不够

1146
00:32:44,533 --> 00:32:45,133
那这个时候

1147
00:32:45,133 --> 00:32:46,000
看了zomi的视频

1148
00:32:46,000 --> 00:32:48,000
你就可以去面试很多的公司

1149
00:32:48,000 --> 00:32:49,266
你可以了解很多的东西

1150
00:32:49,300 --> 00:32:50,000
那这个也是

1151
00:32:50,000 --> 00:32:52,166
zomi在建一个万卡集群当中

1152
00:32:52,166 --> 00:32:54,033
就随着公司建很多万卡集群

1153
00:32:54,033 --> 00:32:55,100
到10万卡集群

1154
00:32:55,133 --> 00:32:57,433
积累的很多的经验的总结

1155
00:32:57,433 --> 00:32:58,066
那第三个

1156
00:32:58,100 --> 00:33:00,400
就中断时间最短

1157
00:33:00,400 --> 00:33:01,500
那所以中断时间最短

1158
00:33:01,533 --> 00:33:03,833
就是尽可能的反过来看

1159
00:33:03,833 --> 00:33:06,866
故障恢复时间越快越好

1160
00:33:07,000 --> 00:33:08,900
那中断是必然

1161
00:33:08,933 --> 00:33:10,500
但是要快速恢复

1162
00:33:10,500 --> 00:33:12,700
所以说降低中断时间

1163
00:33:12,700 --> 00:33:13,333
那这个时候

1164
00:33:13,333 --> 00:33:15,533
目标就是少出故障

1165
00:33:15,533 --> 00:33:17,566
尽可能的提升硬件

1166
00:33:17,566 --> 00:33:19,433
单板的可靠性

1167
00:33:19,466 --> 00:33:21,566
要提前做压测各软件的内容

1168
00:33:21,566 --> 00:33:21,866
第二个

1169
00:33:21,900 --> 00:33:23,566
中断是必然会出现

1170
00:33:23,566 --> 00:33:24,666
所以说中断的时候

1171
00:33:24,700 --> 00:33:26,600
要做快速的断点续训

1172
00:33:26,600 --> 00:33:27,900
快速的定界定位

1173
00:33:27,933 --> 00:33:29,000
快速的恢复

1174
00:33:29,000 --> 00:33:30,633
不管是软件带来的故障

1175
00:33:30,633 --> 00:33:33,366
还是硬件带来的故障相关的内容

1176
00:33:34,300 --> 00:33:35,100
好

1177
00:33:35,233 --> 00:33:37,366
其实zomi在生活上还没有这么乐观

1178
00:33:37,366 --> 00:33:39,566
现在来到了一个总结跟思考

1179
00:33:39,566 --> 00:33:42,366
其实刚才讲到了很多的挑战

1180
00:33:42,366 --> 00:33:44,066
整个AIInfra它是怎么建

1181
00:33:44,100 --> 00:33:44,700
那看一下

1182
00:33:44,700 --> 00:33:45,633
面向未来

1183
00:33:45,633 --> 00:33:47,000
说实话这个图

1184
00:33:47,000 --> 00:33:49,800
是因为英伟达预计的一个4年后的一个

1185
00:33:49,800 --> 00:33:51,466
哎这个图太老

1186
00:33:51,500 --> 00:33:52,033
没关系啦

1187
00:33:52,033 --> 00:33:52,766
不管怎么样

1188
00:33:52,766 --> 00:33:53,466
其实确实

1189
00:33:53,500 --> 00:33:55,300
整个面向未来

1190
00:33:55,300 --> 00:33:56,233
计算的集群

1191
00:33:56,233 --> 00:33:58,500
你会发现整体的GPU的算力

1192
00:33:58,533 --> 00:34:00,433
是百倍式的增长

1193
00:34:00,433 --> 00:34:01,566
非常的夸张

1194
00:34:01,566 --> 00:34:02,633
全球的AI的算力

1195
00:34:02,633 --> 00:34:04,433
你可以发现现在越建越多

1196
00:34:04,433 --> 00:34:06,366
没有停歇的一个趋势

1197
00:34:06,366 --> 00:34:08,600
反正宏大的叙事还是要讲

1198
00:34:08,600 --> 00:34:11,166
然后openAI的一个引领

1199
00:34:11,166 --> 00:34:12,800
也在往前

1200
00:34:12,800 --> 00:34:14,000
那未来的计算集群

1201
00:34:14,000 --> 00:34:16,700
你会发现有比较重要的六大的特征

1202
00:34:16,733 --> 00:34:19,000
这是zomi做了一个简单的总结

1203
00:34:19,000 --> 00:34:21,833
面向AI计算的一个2023

1204
00:34:21,833 --> 00:34:22,100
第一个

1205
00:34:22,133 --> 00:34:24,233
就是柔性的资源

1206
00:34:24,300 --> 00:34:26,400
各种各样的资源可能会做一个融合

1207
00:34:26,433 --> 00:34:26,700
第二个

1208
00:34:26,733 --> 00:34:28,033
就是多样性的泛载

1209
00:34:28,033 --> 00:34:29,066
会建一个大集群

1210
00:34:29,100 --> 00:34:30,033
拥有新的形态

1211
00:34:30,033 --> 00:34:31,800
融合更多的算力

1212
00:34:31,800 --> 00:34:32,400
那第三个

1213
00:34:32,400 --> 00:34:35,066
就是负载更加亲和芯片

1214
00:34:35,100 --> 00:34:36,700
会出现更多的新的算力

1215
00:34:36,700 --> 00:34:38,200
新的存储的内容

1216
00:34:38,233 --> 00:34:38,866
那第四个

1217
00:34:38,900 --> 00:34:40,166
就是安全智慧

1218
00:34:40,166 --> 00:34:42,066
各种各样的高安全高可靠性

1219
00:34:42,100 --> 00:34:43,500
还有高智能性

1220
00:34:43,600 --> 00:34:45,433
那另外还会有一些对等的互联

1221
00:34:45,433 --> 00:34:46,266
让超融合

1222
00:34:46,300 --> 00:34:47,133
超高性能

1223
00:34:47,133 --> 00:34:48,566
还有光内生

1224
00:34:48,566 --> 00:34:50,800
在一个光互联

1225
00:34:50,833 --> 00:34:52,233
还有0碳节能

1226
00:34:52,233 --> 00:34:53,233
液冷

1227
00:34:53,233 --> 00:34:56,266
供电的新的能源相关的内容来

1228
00:34:56,300 --> 00:34:58,100
面向整个AI计算集群

1229
00:34:58,100 --> 00:35:00,766
还是有很多事情要做

1230
00:35:00,900 --> 00:35:01,766
那今天

1231
00:35:01,766 --> 00:35:03,700
做一个简单的总结

1232
00:35:03,833 --> 00:35:05,666
首先大模型训练的负载

1233
00:35:05,700 --> 00:35:08,033
你会发现高并行跟网络化

1234
00:35:08,033 --> 00:35:09,100
越来越重要

1235
00:35:09,133 --> 00:35:10,300
整体的集群

1236
00:35:10,300 --> 00:35:12,533
已经成为整个算力的最佳平台

1237
00:35:12,533 --> 00:35:14,900
所以现在大家都要建AI集群

1238
00:35:14,900 --> 00:35:16,833
建AI智算中心

1239
00:35:16,966 --> 00:35:18,166
那建AI智算中心

1240
00:35:18,166 --> 00:35:20,766
就有很多关键的要素

1241
00:35:20,766 --> 00:35:21,100
第一个

1242
00:35:21,133 --> 00:35:22,800
就是基础设施的先进性

1243
00:35:22,800 --> 00:35:23,400
第二就是超

1244
00:35:23,400 --> 00:35:24,366
大规模互联

1245
00:35:24,366 --> 00:35:26,100
第三个就是极致的算力效率

1246
00:35:26,133 --> 00:35:27,900
还有集群的运维可靠

1247
00:35:27,900 --> 00:35:29,833
对应四大挑战

1248
00:35:29,833 --> 00:35:31,266
四大金刚

1249
00:35:31,300 --> 00:35:32,900
那可能面对四大金刚

1250
00:35:32,900 --> 00:35:37,533
就提出来了刚才讲到的L0 L1 L2 L3 L4

1251
00:35:37,533 --> 00:35:39,166
每一层不同的内容

1252
00:35:39,166 --> 00:35:40,800
从L2到L3层

1253
00:35:40,800 --> 00:35:42,200
是硬件的解决方案

1254
00:35:42,200 --> 00:35:43,900
从L4到L3

1255
00:35:43,933 --> 00:35:46,533
就是软件平台跟应用

1256
00:35:47,666 --> 00:35:49,433
最后建设的目标

1257
00:35:49,433 --> 00:35:51,400
就是围绕着集群的规模的增长

1258
00:35:51,400 --> 00:35:53,033
要提升计算的效率

1259
00:35:53,033 --> 00:35:55,266
还有长稳的运行

1260
00:35:55,300 --> 00:35:59,400
打造亲和的亲和算法的AI集群的架构

1261
00:35:59,400 --> 00:36:02,100
最大化的使能算力

1262
00:36:02,133 --> 00:36:03,033
那今天的内容

1263
00:36:03,033 --> 00:36:03,833
就到这里为止

1264
00:36:03,833 --> 00:36:04,266
谢谢各位

1265
00:36:04,300 --> 00:36:05,366
拜了个拜

