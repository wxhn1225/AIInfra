<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# HPC 硬件发展趋势

Author by: 陈悦孜

承接上文我们介绍了高性能计算和集群的定义，这章我们介绍高性能计算发展的趋势。我们将从核心硬件、基础软件和应用软件三个维度来分析高性能计算发展的趋势。

高性能计算硬件发展历程和未来趋势从高性能网络、处理器、服务器、存储器四个核心维度展开分析。其演进逻辑始终围绕性能突破、能效优化与场景适配展开：

- 高性能处理器：从通用多核到异构计算。
- 高性能网络：从低延迟到高带宽互联。
- 高性能存储器：从容量扩展到存算协同。
- 高性能服务器：从单机性能到绿色化集群。



## 高性能处理器

### CPU 主导时代

早期高性能计算依赖于并行 CPU 集群，比如 Intel Xeon, AMD Opteron 系列，通过提升主频、增加核心数量（多核/众核）和优化指令集实现性能增长。

Intel Xeon系列自 1998 年诞生起专注服务器市场，早期通过 Pentium Pro 微架构奠定基础（超纯量、超管线设计），2006 年后以 Merom 微架构统一服务器与桌面产品线，提升能耗比。而 2003 年推出的 AMD Opteron 系列（K8 架构）首次集成内存控制器，采用 HyperTransport 总线，支持低成本 64 位计算并兼容 32 位软件，颠覆了 RISC 服务器的垄断地位。

AMD Opteron系列第五代至强（Emerald Rapids）支持单 CPU 64 核心，三级缓存容量扩大近 3 倍，通过 DDR5 内存和 UPI 2.0 互连提升并行吞吐量，AI 推理性能提升 42%。2010 年代 Opteron 6100 系列推出 12 核型号（如 6180 SE），在 2.5GHz 主频下实现高密度计算（如戴尔 PowerEdge C6145 服务器支持单机柜 96 核）。

![Intel Xeon和AMD Opteron](images/02TrendHard01.png)



#### 主频提升达瓶颈

2000 年代初，Intel Prescott 核心尝试通过 90nm 工艺提升主频，但功耗突破 100W，导致“高频低效”问题，最终转向多核架构随着工艺微缩接近物理极限，单核主频停滞在 3-4GHz 范围，性能增长转而依赖多核并行和指令级并行（ILP）优化。

早期代表性 HPC CPU 参数对比

|系列|代表型号|	发布时间|核心数|主频|关键技术特点 |
|-|-|-|-|-|-|
AMD Opteron|	Opteron 6180 SE	|2010|	12|	2.5GHz|	集成内存控制器，HyperTransport 总线|
Intel Xeon|	Xeon E5-2600 v4	|2016|	22|	2.2GHz|	超线程，AVX2 指令集|
AMD Opteron|	Opteron 6166 HE|	2010|	12	|1.8GHz|	低功耗设计（65W）|
Intel Xeon	|Xeon Platinum 8380	|2021|	40|	2.3GHz|	支持 PCIe 4.0，8 通道 DDR4|

![Intel Xeon和AMD Opteron](images/02TrendHard02.png)

#### 协处理器兴起

协处理器是一种辅助处理器，设计用于与主 CPU 协同工作，专门处理特定类型的任务以提高系统整体性能。协处理器是计算机系统中与主处理器（CPU）配合工作的专用处理单元，它能够分担 CPU 的特定计算任务、提高特定类型运算的效率，并且优化系统整体性能。

协处理器可用于专用加速，比如 Intel Xeon Phi（MIC 架构，2012-2020）尝试众核路线。以下为 Phi 初期型号及其参数。

型号|	核心数|	浮点性能|	内存容量|	内存带宽|	TDP|	售价|
|-|-|-|-|-|-|-|
Phi 3100|	57|	1 TFlops|	6 GB|	240 GB/s|	300W	|<=$2,000|
Phi 5110P|	60|	1.01 TFlops|	8 GB|	320 GB/s|	225W|	$2,649|
Phi 7120|	61|	1.2 TFlops|	16 GB|	352 GB/s	|300W|	$4,129|

![PHI](images/02TrendHard03.png)


​​GPU 加速​​也属于协处理器范畴，NV CUDA 革命性地将 GPU 用于通用计算， Tesla 成为通用并行计算标杆。首代 Tesla 架构（如 G80）引入统一着色器设计，将矢量计算单元拆分为标量核心（CUDA Core），支持 C 语言编程，实现 SIMT（单指令多线程）执行模型，奠定通用计算基础，将 GPU 从图形协处理器升级为通用计算引擎，定义“CPU+GPU”异构标准。

![Tesla](images/02TrendHard04.png)


#### 国产 CPU 突破

中国自主研发的申威 SW26010 处理器，凭借其创新的异构众核架构（包含 256 个高性能计算核心和 1 个管理核心），实现了单芯片高达每秒 3 万亿次（3 TFlops）的峰值浮点计算能力。这一突破性设计，成为支撑中国首台、也是世界首批 E 级（Exascale， 百亿亿次）超级计算机——“神威·太湖之光”的核心动力源泉。

尤为关键的是，申威 SW26010 采用了完全独立自主的申威指令集架构（SW ISA），彻底摆脱了对国外主流指令集的依赖，确保了核心技术自主可控的安全性与战略意义。

搭载了超过 40,000 颗申威 SW26010 处理器的“神威·太湖之光”超级计算机，于 2016 年 6 月在全球超级计算机 TOP500 排行榜上震撼登顶，终结了美国超算长达 23 年的榜首垄断地位。它不仅以 93 PFlops（每秒 9.3 亿亿次）的 Linpack 实测持续性能创造了当时的世界纪录，更是全球首台突破 10 亿亿次/秒（100 PFlops）大关的超级计算机，标志着人类正式迈入百亿亿次计算时代（E 级超算时代） 的门槛。

![申威 SW26010](images/02TrendHard05.png)

#### Arm 高性能计算突破

NVIDIA Grace CPU 实现内存子系统革新。​​LPDDR5X 和纠错码​​的设计使得能效提升 2 倍；CPU-GPU 一致性缓存的机制和​NVLink-C2C 直连使​​延迟降至 1/10。

![NVIDIA Grace CPU](images/02TrendHard06.png)

鲲鹏 920ARM-based 处理器采用 7nm 工艺，ARM 架构授权，华为自主设计。通过优化分支预测算法、提升运算单元数量、改进内存子系统架构等一系列微架构设计，提高处性能。

![鲲鹏 920ARM-based 处理器](images/02TrendHard07.png)

以下表格对比传统 Arm（移动端）和高性能计算优化版（服务器级）的架构对比。

|特性 | 传统Arm（移动端）|HPC优化版（服务器级）|
|-|-|-|
|指令集|精简指令集（RISC）|拓展SIMD之路（SVE/SVE2）|
|核心规模|多核低频（能效优先）|512核以上众核架构（Fujitsu A64FX）|
|内存系统|低带宽LPDDR|HBM2e（>1TB/s带宽）|
|功耗管理|动态调频（DVFS）|精细功耗门控（Per-core PowerGating）|



### 处理器发展趋势

GPU 主导加速市场，国产力量野蛮生长​。NVIDIA H100/AMD MI300X 成为 AI/HPC 核心算力，国产替代（e.g. 寒武纪、华为昇腾）加速发展。

​​Chiplet 技术​​和存算一体技术加速异构多样化。

#### Chiplet技术

AI技术蓬勃发展，数据中心对高算力芯片需求技术增长，算力芯片与传统消费级芯片相比，算力芯片面积更大，存储容量更大，对互连速度要求更高，Chiplet技术可以很好的满足这些大规模芯片的性能和成本需求，因而得到广泛运用。

Chiplet是一种先进的芯片设计和制造方法。Chiplet 即小芯粒，它将一类满足特定功能的die（裸片），通过die-to-die
内部互联技术将多个模块芯片与底层基础芯片封装在一起，形成一个系统芯片。 

它不再像传统方式那样把包含处理器核心、内存控制器、I/O 接口等所有功能都集成在一个巨大的单片硅芯片（Monolithic Die）上，而是将复杂的大芯片拆分成多个更小、功能更单一、工艺更优化的独立小芯片（Chiplet）。这些小芯片（例如：CPU 核心、GPU 核心、高速缓存、I/O 模块）各自可以采用最适合其功能和成本的半导体工艺（如逻辑用先进工艺，I/O 用成熟工艺）独立制造。然后，它们通过高速、高密度的先进封装技术（如硅中介层、EMIB、CoWoS 等）像“搭乐高”一样集成封装在一个基板上，形成一个功能完整的系统级芯片（SoC）。

![Chiplet](images/02TrendHard08.png)

这种技术提高良率、降低制造成本（尤其是先进工艺成本）、设计更灵活（可复用成熟 Chiplet）、加速产品上市、实现异构集成（不同工艺、不同厂商的 Chiplet 组合）。AMD、Intel 通过多芯片封装提升集成度和良率（比如 Intel Ponte Vecchio 含 47 颗 Chiplet）。

#### 存算一体

算力的需求急速发展使得业界也通过变革当前计算架构来实现算力突破。主流芯片的冯诺依曼结构设计将季赛和存储分离，二者配合完成数据存取与运算。但是由于处理器的设计以提升计算速度为主，存储则更注重容量提升和成本优化，“存”“算”之间性能失配，从而导致了访存带宽低、时延长、功耗高等问题，即通常所说的“存储墙”和“功耗墙”访存愈密集，“墙”的问题愈严重算力提升愈困难。

由此，存算一体的技术应运而生。它的技术核心是将存储与计算完全融合，有效克服冯·诺依曼架构瓶颈，并结合后摩尔时代先进封装、新型存储器件等技术，实现计算能效的数量级提升。

存算技术（Computing-in-Memory / Near-Memory Computing）是一种旨在突破传统计算机“冯·诺依曼瓶颈”的革命性架构。目前学术界和工业界均在开展存算一体技术研究，学术界主要关注狭义的存算一体，即利用存储介质进行计算;工业界关注商用化进程，着重宣传广义存算一体概念，但分类方法尚未完全统一。本章节将对广义存算一体技术进行分类，望达成广泛共识。

根据存储与计算的距离远近，我们将广义存算一体的技术方案分为三大类，分别是近存计算(ProcessingNear Memory,PNM)、存内处理(Processing In Memory, PlM)和存内计算(Computing in Memory, ClM)。存内计算即狭义的存算一体。

- 近存计算通过芯片封装和板卡组装等方式，将存储单元和计算单元集成，增加访存带宽、减少数据搬移，提升整体计算效率。近存计算仍是存算分离架构，本质上计算操作由位于存储外部、独立的计算单元完成其技术成熟度较高，主要包括存储上移、计算下移两种方式。

- 存内处理是在芯片制造的过程中，将存和算集成在同一个晶粒(Die)中，使存储器本身具备了一定算的能力。存内处理本质上仍是存算分离相比于近存计算，“存”与“算”距离更近。当前存内处理方案大多在内存(DRAM)芯片中实现部分数据处理，在DRAM Die中内置处理单元，提供大吞吐低延迟片上处理能力，可应用于语音识别、数据库索引搜索、基因匹配等场景。

- 存内计算即狭义的存算一体在芯片设计过程中，不再区分存储单元和计算单元，真正实现存算融合。存内计算是计算新范式的研究热点，其本质是利用不同存储介质的物理特性，对存储电路进行重新设计使其同时具备计算和存储能力，直接消除“存”"算个界限，使计算能效达到数量级提升的目标。


存算技术这种技术大幅减少数据搬运，从而显著提升能效比（数十倍甚至百倍） 和计算速度，特别适用于数据密集型的 AI 推理/训练、大数据分析等场景。台积电 3D Fabric 技术将计算单元堆叠至存储层（比如 CXL 协议设备），突破冯·诺依曼瓶颈。


## 高性能网络

### 网络发展历程：早期阶段 (1990s-2000s)

一方面以太网主导网络应用领域。千兆以太网（GigE）成本低被广泛采用，但延迟高（>100μs）、带宽瓶颈明显。千兆以太网是 IEEE 802.3ab/z 标准定义的以太网技术，传输速率达 1 Gbps（1000 Mbps），是传统百兆以太网（100 Mbps）的 10 倍。它延续了以太网的帧结构（如 MAC 地址、CSMA/CD 机制）和基础设施（双绞线、光纤）。它具有低成本与高兼容性的优点，同时有广泛应用场景。使用广泛普及的 Cat 5e/6 双绞线（百米内无需中继）或光纤，布线成本低。兼容现有网络设备（交换机、路由器），支持平滑升级。它是企业局域网（LAN）、家庭宽带、工业控制、IP 监控摄像头等领域的主流选择，同时因成熟稳定，成为中低速设备的默认网络接口（如打印机、IoT 设备）。

另一方面大量的专有网络兴起。Myrinet、Quadrics 等私有协议网络出现，延迟~10μs，但生态封闭制约普及。专有网络，如 Myrinet 和 Quadrics，是高性能计算领域为突破传统以太网性能瓶颈而兴起的私有协议互连技术。其核心优势在于实现了~10 微秒级的极低通信延迟和高吞吐量，这得益于其精简的协议栈（绕过操作系统内核直接在网卡硬件处理通信）和定制的交换架构。然而，这些技术的生态高度封闭成为其致命短板：它们依赖专属的硬件（特定网卡、交换机）和私有软件栈（如专用通信库），导致成本高昂（远超商用以太网）、兼容性差、用户被厂商锁定且不同系统间互操作性困难。最终，这种封闭性严重制约了其普及和发展，在 2009 年前后，它们被更具开放性和成本效益的技术（如 InfiniBand 和基于以太网的 RDMA）所取代而退出主流市场。

![Ethernet和Myrinet](images/02TrendHard09.png)

### 网络发展历程：主流技术成型（2010s 至今）

这里介绍主流技术有 InfiniBand、RoCE 和 NVLink。

- InfiniBand 由 Mellanox 主导，采用 RDMA 实现微秒级延迟和百 GB/s 带宽，成为 HPC 主流。InfiniBand是由 Mellanox（现属 NVIDIA）主导推动的开放标准高性能网络，与封闭的专有网络（如 Myrinet/Quadrics）形成鲜明对比。它通过 RDMA（远程直接内存访问） 技术彻底绕开操作系统协议栈，实现 微秒级超低延迟（可低于 1μs）和 超高带宽（当前达 400Gbps，约 50GB/s），完美匹配 HPC 与 AI 算力需求。其核心优势在于开放生态，由国际组织 IBTA 统一标准，兼容多厂商设备（网卡、交换机），同时支持与以太网融合（如 RoCE 协议）。凭借性能与通用性的平衡，InfiniBand 自 2010 年代起取代旧式专有网络，成为超算中心（如 Summit、Sierra）和云数据中心的主流互连方案。

- RoCE 是基于以太网 RDMA 的技术，兼顾低成本与高性能，是华为、阿里云等国产厂商加速布局的重要方向。RoCE 是一种在标准以太网上实现 RDMA（远程直接内存访问）的高性能网络技术，由 InfiniBand 贸易协会（IBTA）制定开放标准。它通过绕过操作系统内核协议栈，使数据直接从应用内存访问网卡，实现微秒级延迟（典型值 10-20μs） 和高吞吐量（可达 400Gbps），逼近 InfiniBand 性能。其核心价值在于兼顾高性能与低成本：复用现有以太网交换机和布线设施（需支持无损特性如 PFC/ECN），大幅降低部署门槛。正因这一优势，华为（含其 CE 系列交换机）、阿里云、腾讯云等中国厂商积极布局 RoCEv2 协议，推动其在云数据中心、AI 训练集群及存储网络（如 NVMe-oF）中的规模化应用，成为突破 InfiniBand 生态垄断的关键国产化路径。

- NVLink 是 NV 专为 GPU 互联设计高速总线，用于节点内多卡互联，演进 NV Fusion。NVLink 是 NVIDIA 专为 GPU 高性能互联设计的私有高速总线协议，核心目标是解决节点内多 GPU 卡间的通信瓶颈。它通过点对点直连架构（替代传统 PCIe 总线），实现远超 PCIe 的带宽（第四代达 900GB/s） 和纳秒级延迟，显著加速 GPU 间数据交换（如 AI 训练中的梯度同步）。

### 高性能网络发展趋势：低延迟与融合​

在人工智能（AI）、高性能计算（HPC）和超大规模数据中心蓬勃发展的浪潮下，网络性能，尤其是低延迟和高带宽，已成为制约整体系统效能的关键瓶颈。同时，不同技术路径的融合趋势日益明显，共同推动网络基础设施向更高性能、更智能、更灵活的方向演进。核心发展趋势聚焦于以下几个方面。

- InfiniBand持续领先超低延迟领域​​。InfiniBand (IB) 凭借其原生设计的超低延迟、高吞吐量、无损传输和强大的远程直接内存访问 (RDMA) 能力，长期以来是HPC和AI训练集群的首选互联技术。代表当前IB发展巅峰的NVIDIA Quantum-2 支持 400Gb/s 单端口带宽，显著提升了节点间通信效率，NDR 1.6Tb/s超高速率将满足未来千卡、万卡级AI训练集群对极致带宽的需求。

- RoCE v2与智能网卡 (DPU/IPU) 重塑高性能以太网​​。借助 DPU/IPU 降低 CPU 负载，提升以太网竞争力。基于融合以太网的 RDMA (RoCE v2) 技术通过在标准以太网上实现类似IB的RDMA能力，大大提升了以太网的性能和效率，使其在高性能网络领域具备了强大的竞争力。智能网卡比如数据处理器 (DPU) 或基础设施处理器 (IPU)这些专用硬件将网络、存储和安全功能从主机CPU卸载下来，通过处理网络协议栈（尤其是RoCE的拥塞控制、流量管理）、虚拟化、存储加速（NVMe-oF）、安全加密等技术，释放宝贵的CPU核心用于运行应用。智能网卡提供硬件级RDMA加速，显著降低延迟，提升带宽利用率，使以太网接近IB的性能水平。

- ​光互连技术突破铜缆限制，奠定下一代网络基石​​。随着速率演进，传统铜缆在距离、功耗、密度和信号完整性方面面临严峻挑战。光互连是突破这些瓶颈的可行方案。硅光集成、CPO（共封装光学）、 OCS（光学链路开关）成为下一代网络关键。硅光集成 (Silicon Photonics)利用成熟的硅基半导体工艺制造光学器件（调制器、探测器、波导等），实现高集成度、低成本、低功耗的光模块/芯片，是规模化应用的关键。共封装光学 (CPO - Co-Packaged Optics)将光引擎与交换机芯片/ASIC共同封装在同一基板上（而非传统可插拔模块）。这大大缩短了电信号传输距离，显著降低功耗（~30-50%），提升带宽密度和信号质量，是800G/1.6T及更高速率的主流解决方案。光学链路开关 (OCS - Optical Circuit Switch)利用光开关技术直接在光层动态配置连接路径（无需经过电交换芯片）。特别适用于超大规模数据中心中计算、存储资源池间的极低延迟、高带宽、可重构的互连（如用于分解式存储/内存、大规模AI训练集群的All-to-All通信），能效比极高。

- 新互联协议面向异构计算与AI的专用优化。华为灵渠总线互联，实现 CPU2NPU、NPU2NPU 新一代的集群互联。传统的CPU间互联协议（如PCIe）在连接CPU与加速器（如GPU、NPU）或加速器之间的高效通信时，存在延迟、带宽和扩展性瓶颈，对于CPU与加速器或者加速器提速通信的需求，新互联协议成为新一代的集群互联技术。华为灵渠总线互联是一个典型的代表，是专门为 CPU-to-NPU 和 NPU-to-NPU 通信设计的新一代集群互联技术。它提供超高带宽、超低延迟的直接连接，显著优化大规模AI训练和推理集群中，异构计算单元（特别是海量NPU间）的数据交换效率。这类协议代表了网络技术与特定计算架构（如AI集群）的深度融合，通过硬件和协议层的协同设计，最大化整体计算效能。随着异构计算（CPU+GPU+NPU+其他加速器）成为主流，针对特定场景（尤其是AI）优化的专用高速互联协议将成为高性能网络发展的重要分支，与IB、以太网（RoCE）形成互补或竞争关系。

图为谷歌推出的OCS（光学链路开关），光互连技术。

![高性能网络发展趋势](images/02TrendHard11.png)


## 高性能存储

### 发展历程1：硬盘时代

SSD 取代 HDD 提升 I/O 速度引发的硬盘革命。

HDD（普通硬盘）通过涂有磁性材料的旋转磁盘（盘片）来存储数据。它们的机械臂上具有读/写头，通过在这些旋转磁盘上来回移动来访问数据。由于机械构造上的局限性，传统HDD的运行速度比较慢。它们的数据传输速率较低，且延迟较高。由于HDD包含活动部件（旋转磁盘和机械臂），因此更容易出现机械故障。与SSD相比，HDD通常可提供更大的存储容量，而且每GB成本更低。希捷、西数推出 HDD，受限于输入输出延迟（ms 级），难满足高性能计算的需求。

SSD（固态硬盘）是硬盘技术后起之秀，相比HDD具有明显的性能优势，因而广受欢迎。SSD使用基于半导体的非易失性闪存，利用硅芯片的物理和化学特性来提供更大的存储容量，能够加快数据访问和传输速度。在读取速度，耐用性和能效上和HDD比都有明显的优势。速度上SSD 比 HDD 快得多，启动速度和文件传输速度都更快，并且可缩短应用程序的加载时间。耐用性更高，由于 SSD 没有活动部件，因此它们更加耐用，不容易因为遭到冲击或跌落，而造成物理损坏和数据丢失。SSD 的功耗比 HDD 低，因此适合在笔记本电脑和移动设备中使用，这些设备的电池续航时间至关重要。三星 PM1733（30TB）顺序读写达 7GB/s，延迟降至 50μs。

![硬盘革命](images/02TrendHard12.png)

### 发展历程2：分布式存储和新存储文件系统


#### 分布式文件系统

分布式文件系统（Distributed File System，DFS）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点（可简单的理解为一台计算机）相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。DFS为分布在网络上任意位置的资源提供一个逻辑上的树形文件系统结构，从而使用户访问分布在网络上的共享文件更加简便。Lustre、Ceph 等文件系统支撑EB级数据吞吐，比如Lustre带宽突破 1TB/s。

![Lustre](images/02TrendHard18.png)

#### AI 驱动存储优化 ：WekaIO、VAST Data通过NVMe-oF + RDMA实现全闪存存储集群。

随着LLM的爆火，在训练和推理层面都对Infra提出新挑战，传统深度学习时代模型规模较小，猝存储上面临的都是小文件问题，对性能没有太多需求，而LLM动辄100B的参数量，在数据管道的各个阶段需要花费大量时间在不同系统之间复制数据，对GPU利用率较低的现象有很大影响，所以AI存储技术需要有新的进展。

全闪文件系统（All-Flash File System）是专门为全闪存（All-Flash）存储阵列设计的文件系统，这种存储阵列完全由固态驱动器（SSD）组成，而不是传统的机械硬盘（HDD）。与传统的文件系统相比，全闪文件系统针对固态驱动器的特性进行了优化，以提高性能和效率，延长固态驱动器的使用寿命。

全闪方案通过NVMe-OF和RDMA实现存储集群。NVMe规范是从零开始专为SSD而设计的规范，而NVMe-oF支持创建超高性能存储网络，其时延能够比肩直接连接的存储器。因而可在服务器之间按需共享快速存储设备。NVMe-oF可视为基于光纤通道的SCSI或iSCSI的替代品，其优势在于时延更低、I/O 速率更高，且生产力更优。NVMe-OF+RDMA采用不中断远程机器系统 CPU 处理的情况下直接存取内存的技术实现AI高性能存储。

需要注意的是，全闪方案基本没有开源方案，全部都是商业公司的支持，有些还需要购买专门的硬件，或者需要在公有云依赖特定的硬件支持。目前比较火的是weka.io和VAST，国内的云厂商都有提供对应的解决方案，比如阿里云的CPFS以及百度云的PFS等。开源的目前发现有 Intel 的 DAOS。

![WekaIO](images/02TrendHard19.png)

如图展示wekaIO全闪存储集群，它是一个8节点的WEKApod，具有PCIe Gen 5连接，通过Nvidia的Quantum-2 QM9700 64端口400 Gbps InfiniBand交换机连接到Nvidia DGX H100服务器。

![wekaIO全闪存储集群](images_src/02TrendHard19.png)



### 高性能存储发展历程3： 新存储技术革命
#### SCM（存储级内存）：Intel Optane 持久内存试图弥合内存与存储间的性能鸿沟。

SCM的全称为Storage Class Memory，即“存储级内存”。有时它也被人们称作Persisent Memory（持久内存）或Non-Volatile Memory（非易失性内存）。它是一种拥有近似于硬盘的持久性（Sotrage-Class），又如内存般高速（Memory）的存储介质。

在数据中心的结构中，DRAM内存与SSD硬盘之间存在巨大的性能鸿沟。SRAM和DRAM在数据存取延迟较小，性能佳，但与此同时需要更高的成本，也有着更快的损耗速度。数据在内存与硬盘之间的往返，成为了制约整体性能的瓶颈所在。为了填补DRAM同SSD之间的巨大性能落差，SCM以分层存储结构中“DRAM缓冲区”的身份被研制出来。

Intel Optance傲腾是英特尔率先投入商用的SCM产品，P4800X固态硬盘于2017年3月发布。傲腾不同于传统硬盘的地方在于它采用3DXpoint的相变存储（PCM）技术。

图为傲腾P4800X。

![傲腾P4800X](images/02TrendHard20.png)

PCM通过改变温度的方式进行数据写入。在低温下，介质处于低电阻的结晶状态；而在高温加热后，介质的电阻升高，转变为不定型状态。物质的不同状态，就对应了二进制数据的0与1。3DXpoint借此完成对数据的存储与记录。

这项技术给傲腾带来哪些技术优势呢？
1. 减少电子泄露，增强断电后的数据保持能力，改善“冷数据”问题。
固态硬盘存在“冷数据”问题，传统固态硬盘使用的是NAND“与非门”，以电子来存储记录数据。但在不通电时，NAND中的电子会随着时间而泄露，在下一次读取时，这便会引发错误，导致读取速度缓慢，甚至数据丢失。而3DXpoint以电阻记录数据的独特原理，从根本上断绝了电子泄露的可能。PCM材料的相变需要上百摄氏度的极苛刻条件，这让傲腾在断电后的数据保持能力大大增强，“冷数据”问题大幅改善。

2. 大幅降低数据的读取延迟，有卓越的随机读写性能。
现今，动辄突破7GB/s读写速度的NAND固态硬盘，多半仰仗于SLC Cache技术。SLC Cache为NAND带来“鸡血”的同时，也造就了更大的发热与写入放大问题。日常生活也有相当部分软件不支持SLC Cache的速度进行读取器。3DXPoint比SLC更快，因此无需SLC Cache的介入。规避了SLC Cache存在的种种问题，傲腾不存在“出缓掉速”的可能，更保持了使用中的性能一致性。

同时，由于不需要DRAM缓存的优化，无需断电保护电容，傲腾自身就能达成停电保护的能力。

英特尔宣称“傲腾性能接近内存”，但是也有缺点，就是成本高昂。SCM制造企业Dapustor（大普微）根据镁光和英特尔的财报推测，在2020年第二季度，英特尔第一代3D Xpoint的晶圆（Wafer）成本约是96层TLC NAND的10倍。由于容量上有着先天不足，成本与价格也始终居高不下。傲腾没能像英特尔设想的那样，代替硬盘与内存中的任何一方。

#### HBM（高带宽内存）：NPU/GPU 高度集成与依赖， HBM3 每引脚数据速率提高到 6.4Gb/s，单设备带宽 819GB/s 。


存储与运算之间的“内存墙”于计算速度的影响由于数据、算力需求的增长愈发显现，提升内存带宽是存储芯片关注的重要问题。HBM（High Bandwidth Memory）高带宽存储器，被视作是新一代DRAM技术，它通过使用硅通孔（TSV）垂直堆叠多个 DRAM，和NPU/GPU封装一起，实现大容量、高位宽的DDR组合阵列。


![HBM技术示意图](images/02TrendHard14.png)

HBM在2013年由SK海力士将TSV技术应用于DRAM而首次研发出来，HBM1的带宽高于DDR4和GDDR5产品，同时以较小的外形尺寸消耗较低的功率，更能满足GPU等带宽需求较高的处理器。2016年，三星和SK海力士开始量产HBM2 DRAM。2018年末，JEDEC推出HBM2E规范，以支持增加的带宽和容量。当传输速率上升到每管脚3.6Gbps时，HBM2E可以实现每堆栈461GB/s的内存带宽。此外，HBM2E支持最多12个DRAM的堆栈，内存容量高达每堆栈24GB。与HBM2相比，HBM2E具有技术更先进、应用范围更广泛、速度更快、容量更大等特点。2019年和2020年，SK海力士和三星分别研发量产出HBM2E产品。2020年，另一家存储巨头美光宣布加入到这一赛场中来。

2022年，新一代高内宽内存HBM3的标准规范发布，在存储密度、带宽、通道、可靠性、能效等各个层面进行扩充升级，具体包括：

- 主接口使用0.4V低摆幅调制，运行电压降低至1.1V，进一步提升能效表现。
- 传输数据率在HBM2基础上再次翻番，每个引脚的传输率为6.4Gbps，配合1024-bit位宽，单颗最高带宽可达819GB/s。
- 独立通道数从8个翻番到16个，再加上虚拟通道，单颗支持32通道。
- 支持4层、8层和12层TSV堆栈，并为未来扩展至16层TSV堆栈做好准备。
- 每个存储层容量8/16/32Gb，单颗容量起步4GB(8Gb 4-high)、最大容量64GB(32Gb 16-high)。
- 支持平台级RAS可靠性，集成ECC校验纠错，支持实时错误报告与透明度。

这是HBM1到HBM3的标准变化参数，SK海力士和三星一直是HBM的领先者。

| 产品世代 | 数据传输速率（Gb/s）| 每个设备带宽（GB/s）|堆栈厚度|最大DRAM容量（Gb）|最大设备容量（GB）|
|-|-|-|-|-|-|
|HBM|1.0|128|8|16|16|
|HBM2|2.0|256|8|16|16|
|HBM2E|3.6|461|12|24|36|
|HBM3|6.4|819|16|32|64|
|HBM3E|9.6|1229|16|32|64|

其中HBM3E由SK海力士、美光和三星共同发布。



在AI芯片中，高带宽存储主要用于加速模型执行。对于大型神经网络，模型权重和中间激活会占据数GB到数十GB内存，HBM提供的高带宽使得这些数据可以在算力单元和内存之间高速交换，减少等待时间。例如，NVIDIA A100 GPU配备了40GB HBM2e显存，带宽达1.5TB/s，能够高效训练上百亿参数的模型；在推理时，HBM也使得批处理、并发推理的吞吐大幅提升。AI芯片通常会将HBM带宽与片上SRAM缓存相配合：频繁访问的数据尽量留存在片上缓存，不常用的大数据则从HBM取用。高带宽存储的出现甚至影响了算法设计，当内存带宽不再是主要限制，研究人员可以设计更宽的网络或更大的并行度。


### 高性能存储：近计算与层级优化

这里总结高性能存储未来发展趋势。
1. 存储级内存 SCM：CXL 3.0 协议推动存储池化，实现 CPU/GPU 共享内存资源。
存储级内存（SCM）在CXL 3.0协议的推动下，将实现跨CPU/GPU/FPGA的内存资源池化，打破传统内存墙限制。通过高速互联技术，不同处理器可动态共享SCM资源，显著提升异构计算的效率，同时降低延迟和系统复杂度。
2. HBM3e 与 HBM4​​：2025 年 HBM3 带宽突破 1TB/s，成 AI 芯片标配。
HBM技术持续迭代，2025年HBM3e将实现1TB/s以上的带宽，HBM4更可能采用3D堆叠与硅通孔（TSV）优化，成为AI训练芯片的标配。其高带宽特性可满足大模型参数实时加载的需求，推动千亿级参数模型的部署。
3. 存算一体架构：计算单元嵌入存储层，减少数据搬运。
存算一体架构通过将计算单元嵌入存储层，彻底重构传统冯·诺依曼架构。近内存计算（如三星HBM-PIM）和内存内计算（如存内逻辑）可减少数据搬运能耗，特别适合矩阵运算密集型任务。
4. 存算一体器件：忆阻器 Memristor、相变存储器 PCM 支持内存计算，降低搬运功耗。
新型存算一体器件如忆阻器（Memristor）、相变存储器（PCM）和铁电存储器（FeRAM）利用其物理特性直接进行模拟计算，在边缘AI场景中展现出超低功耗优势。例如，基于忆阻器的神经形态芯片可实现能效比提升100倍，为端侧智能设备带来突破。

![高性能发展趋势](images/02TrendHard15.png)


## 高性能服务器
### 发展历程 1：性能导向阶段（2000s）
#### 刀片服务器普及：HP BladeSystem、IBM BladeCenter 通过高密度设计提升机架利用率。

刀片服务器其实是一种采用模块化设计的“精简版”服务器，为了尽量少占用机柜空间而进行了优化。整个刀片系统由刀片机箱和刀片设备组成。刀片机箱可容纳多个刀片服务器，它同时提供了共享的电源、冷却、网络及各种互连和管理组件。当然，不同刀片厂商在刀片和机箱里面添加哪些组件，有着不一样的设计思想。

![刀片式服务器和机架式服务器](images/02TrendHard16.png)

高密度是刀片最主要的优势之一。在标准的高42U、宽19英寸的服务器机柜中，1U（高1.75英寸）规定了任何设备的最小尺寸，对于机架式设备的安装台数有着严格的限制，比如最多放24台1U的机架服务器。而刀片计算的主要优点就在于消除了这种限制，截至2009年，通过刀片系统，一个机柜可实现容纳128个独立服务器的超高密度。“高计算密度、资源整合、易管理、低功耗”等是刀片服务器的天然优势。

从2000年开始，刀片服务器以“超高的计算密集性”为核心，如RLX Technologies公司的System 324刀片服务器，可以在标准42U机箱中集成324个CPU。但是为了支持如此高的计算密集度，设计者不得不在单个CPU的功耗和散热问题上做出很大的让步。所以早期采用的CPU都是性能相对较弱的低功耗的处理器，如PIII和Transmeta Crusoe。对于简单的任务实现，这是个合适的实现方法。

发展到2003年，高性能计算需求增长，这些性能较弱的高密度刀片服务器跟不上时代需求。IBM推出BladeCenter刀片式服务器。刀片服务器不再过分强调计算密集性，而是通过把计算密集性和单个刀片性能进行了一次权衡，在两者之中选择了一个合适的平衡点。2006年，IBM推出了革命性的BladeCenter H，增加了紧凑型刀片服务器的带宽，可将其网络传输数据容量提高10倍。同时，IBM还推出了基于Cell 宽带引擎（Cell BE）的刀片计算系统，专为那些需要密集计算能力和独特性能来应对大量图形处理和数字应用的公司而设计。同年，HP发布其历时三年研发的第三代产品——BladeSystem c-class刀片服务器，主要针对企业下一代数据中心而设计，具备虚拟连接（Virtual Connect）、能量智控（Thermal Logic）和洞察管理（Insight Control）等三项创新技术。

刀片服务器发展里程碑：IBM的BladeCenter H。

![IBM BladeCenter](images/02TrendHard21.jpg)

HP潜心三年的力作——BladeSystem c-class刀片服务器。

![HP BladeSystem](images/02TrendHard22.jpg)

#### 风冷散热瓶颈：单机柜功耗超 20kW 后，传统风冷效率骤降（PUE > 1.5）。

高密度服务器如刀片服务器机柜能够达到的最大功率达20 kW以上。传统风冷冷却介质是空气适用于小于15kW功率的机柜，PUE大于1.5。这种负载大大超出了传统数据中心的供电和散热设计能力。

P.S. PUE（Power Usage Effectiveness，电能使用效率） 是衡量数据中心能源效率的核心指标，计算公式为 $PUE= \frac{IT设备耗电量}{数据中心总耗电量}$ 。理想PUE = 1.0，所有电力均用于IT设备，无损耗。实际上大多数数据中心PUE在1.2~2.0之间，超算/AI数据中心通过液冷等技术可降至 1.1 以下。

同时因为服务器工作负载量大，且在对安全性和可靠性要求较高的场合，如政府机关、银行、证券、通信、企业等领域，特殊的工作性质，要求服务7X24不间断工作，服务器的处理器始终处于高负载状态，使之发热量也随之加大。如今，散热设计与散热能力已经成为服务器设计者的首要考虑问题，它不仅关系到服务器的稳定性和安全性，而且还要考虑到散热系统所需的能耗。

### 发展历程 2：绿色化阶段（2010s ~ 至今）
#### 散热技术突破，从传统风冷机架到高密度液冷机架演进：
##### 冷板液冷：直接冷却CPU/GPU（比如曙光浸没式液冷，PUE ≈ 1.04）

冷板式液冷作为非接触式液冷，通过液冷板将发热器件的热量间接传递给封闭在循环管路中的冷却液体带走热量。冷板式液冷对于服务器芯片组件及附属部件改动较小，主要途径为加装液冷模块，采用集中式或分布式CDU供液、Manifold分液，对芯片、内存等部件进行精准制冷。主要可分为部分冷板式、全冷板式以及冷板背门式。

曙光数创作为液冷数据中心的先行者，旗下的SLiquid液冷技术品牌旗舰产品——相变浸没液冷数据中心基础设施解决方案C8000系列，可实现PUE低至1.04，不仅符合国家的各项政策要求，更是把节能做到了极致。能达到这样的高效散热效果，又绿色低碳。曙光数创通过六大核心技术实现高效液冷：

- 冷媒新材料：联合中科院研发自主可控冷媒，具备高绝缘、低沸点、大潜热特性，环保且成本优势显著，迭代三代实现技术自主。

- 材料兼容性：建立万小时级测试平台，形成材料“白/黑名单”数据库，确保电子元器件长期稳定运行。

- 高速信号优化：针对浸没环境信号衰减，制定主板Layout规范与测试标准，保障1Gbps+高速信号完整性。

- 密封与转接技术：开发气液密封连接器，实现电气信号与冷媒循环的可靠隔离。

- 相变换热创新：通过微纳结构强化沸腾/冷凝，提升换热效率，沸腾启动温度降低30%，冷凝液膜脱离速度提高50%。

- 智能控制系统：采用神经网络建模与串级预测控制，动态调节流量/压力，响应速度提升60%，避免干烧风险。

图为曙光浸没式液冷服务器。

<img src="images/02TrendHard23.jpeg" style="width: 30%; height: auto;" alt="曙光浸没式液冷服务器">

图为曙光浸没式液冷服务器内部结构。

<img src="images/02TrendHard24.jpeg" style="width: 50%; height: auto;" alt="曙光浸没式液冷解决方案">

##### 浸没式液冷：整机浸入非导电冷却液（比如阿里云“麒麟”数据中心 PUE ≈ 1.09）

浸没式液冷是将发热电子元器件直接浸没在非导电冷却工质中,通过冷却工质循环流动来进行散热的接触式冷却技术,可实现100%液体冷却,散热节能优势明显，且几乎没有噪音。浸没式液冷技术根据冷却液是否发生相变,分为单相和相变两种类型。

阿里云的“麒麟”采用了浸没式液冷散热的服务器部署方案，利用绝缘冷却液取代传统风冷。由于无需风扇、空调等制冷设备，可在任何地方部署，因此也极大地降低了冷却能耗和空间消耗。这种浸没液冷服务器集群技术，可将数据中心能源使用效率（PUE）大幅降低到接近于理论极限值1.0（能源使用效率），达到世界领先水平。“麒麟”节能，可大幅降低数据中心的冷却能耗；绿色环保，冷却液吸收元器件的热量后，再沸腾气化，气体上升遇到冷凝板后液化落回主机柜中循环利用，热量在几乎零功耗的情况下被冷凝管传递出去，与此同时高密封性也使得元器件远离湿度、灰尘、振动的影响，受干扰情况几乎为0。“麒麟”所使用的绝缘冷却液，完全绝缘且无腐蚀性，即使浸没元器件20年以上，成分不会发生任何变化，也不会对电子元器件产生任何影响。而得益于浸没式液冷高效的散热效率，“麒麟”无需空调等大型制冷设备，可以在任何地方部署，节省空间75%以上。“麒麟”在单位体积内的计算能力比过去提升了10倍。

![阿里云“麒麟”浸没式液冷解决方案](images/02TrendHard25.png)

#### 模块化设计 ：超算模块化机柜（比如Frontier）实现计算、存储、网络的灵活扩展。

高性能服务器走向模块化，超算系统的模块化设计通过标准化机柜单元集成计算、存储、网络等核心组件，实现快速部署与弹性扩展。

以Frontier超级计算机为例，其采用模块化机柜架构，每个机柜单元包含计算节点（AMD CPU+GPU）、液冷散热系统、分布式存储模块及高速互联网络（Slingshot-11互连），单个模块即可独立运行，74个机柜通过高速网络互联可以连接一起运行。

Frontier超级计算机部署于美国能源部橡树岭国家实验室，由74个标准19英寸（48厘米）机柜组成，每个机柜重达8,000磅。系统采用高度集成的模块化设计，每个机柜配备64个刀片服务器，每个刀片包含2个计算节点，共计9,472个节点（部分资料显示为9,408个，差异源于统计口径）。每个计算节点配置：

- CPU：1颗64核AMD EPYC 7453s（代号"Trento"）处理器，搭配512GB DDR4内存

- GPU：4颗AMD Radeon Instinct MI250X加速卡

总计算资源如下：

- 606,208个CPU核心（或602,112个）

- 8,335,360个GPU核心（或8,138,240个）

- 内存系统：4.6 PB DDR4 + 4.6 PB HBM（每个GPU含128GB HBM）

系统通过HPE Cray Slingshot-11高速网络互联，采用基于以太网的定制拓扑结构，实现CPU-GPU间高效协同。该设计体现了超算系统在模块化架构、异构计算与高密度集成方面的前沿水平。

![Frontier超级计算机](images/02TrendHard27.jpeg)

### 未来趋势

未来高性能服务器将呈现两大核心发展趋势：在散热技术方面，液冷技术正从实验性方案快速演进为标准配置；在系统架构方面，模块化整机柜设计变得流行，如曙光硅立方率先实现机柜级全浸没液冷方案，华为昇腾也推出Cloud Matrix超节点架构。这些创新技术共同推动高性能服务器向高效节能、智能集成的方向发展。

#### 液冷普及：欧盟要求 2025 年后数据中心 PUE ≤ 1.3，液冷从实验走向标准方案。

全球范围内，数据中心行业面临日益严格的能效和碳排要求。欧盟的《能效指令》要求到2030年数据中心PUE不超过1.3，中国"东数西算"工程对枢纽节点的PUE要求更为严格(≤1.2)。液冷凭借物理性散热优势（比风冷效率高1000倍）成为唯一达标路径，进而从边缘实验技术跃升为全球数据中心的新一代基础设施标准。Intel/AMD的部分商用服务器采用冷板式液冷；阿里云“麒麟”浸没液冷集群，谷歌AI数据中心采用浸没式液冷技术。

#### 整机柜设计：曙光硅立方实现机柜级液冷，昇腾推出 Cloud Matrix 超节点。

整机柜设计是数据中心技术演进的核心方向，它通过系统级整合大幅提升算力密度与能效。

曙光硅立方单机柜整合计算、冷却、供电单元，支持300kW制冷能力，采用浸没相变液冷技术，将计算部件整体浸入液态冷媒中，实现PUE值低至1.04，单机柜功率密度达160kW，较传统风冷提升4-5倍。曙光硅立方呈现独特的“硅立方”形态，采用层次化、模块化、立体化的创新设计，大幅节省物理空间。

![曙光硅立方](images/02TrendHard26.jpeg)

华为昇腾384超节点，首创将384颗昇腾NPU和192颗鲲鹏CPU通过全新高速网络MatrixLink全对等互联，形成一台超级“AI服务器”，单卡推理吞吐量跃升到2300 Tokens/s。在高速互联总线联接下，昇腾384超节点共由12个计算柜和4个总线柜构成，是目前业界最大规模的超节点，算力总规模达300Pflops，网络互联总带宽达269TB/s，内存总带宽达1229TB/s，通过最佳负载均衡组网等方案，未来可以支撑更大规模的模型演进。

![华为昇腾384超节点](images/02TrendHard18.jpg)



## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114767700172746&bvid=BV16A3wzEELU&cid=30765482532&p=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>


## reference
https://www.witintech.com/upfiles/202312051029031006456460.pdf

https://pdf.dfcfw.com/pdf/H3_AP202307091592122354_1.pdf 

https://www.dell.com/support/contents/zh-cn/article/product-support/self-support-knowledgebase/data-storage-backup-and-recovery/support-for-hard-disk-drive

https://aws.amazon.com/cn/compare/the-difference-between-ssd-hard-drive/

https://post.smzdm.com/p/a90el8pp/

https://zhuanlan.zhihu.com/p/23269431888

https://zhuanlan.zhihu.com/p/597784274

https://server.51cto.com/article/143125.html

https://dataenergy.sugon.com/c48283.jsp

https://dataenergy.sugon.com/c81871.jsp

https://mp.weixin.qq.com/s/ZVklrCPV7oNHvfcJpnPdJA

https://developer.aliyun.com/article/74471

https://www.eet-china.com/mp/a135747.html

https://zh.wikipedia.org/wiki/%E5%89%8D%E6%B2%BF_(%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6)

https://www.baogaobox.com/insights/250602000011116.html

https://server.zol.com.cn/731/7319835.html

https://baike.baidu.com/item/%E7%A1%85%E7%AB%8B%E6%96%B9/24229490?anchor=1&fragment=1#1

https://www.sohu.com/a/915158063_115479?scm=10001.325_13-325_13.0.0.5_32

https://www.jiuyangongshe.com/a/2g6hckcy8nz

