<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 大模型推理性能指标

Author by：杨小珑

### 推理性能指标

接下来将介绍现如今 LLM 在推理过程中的一些重要的性能指标的定义和含义。对于这些指标而言，大致可以分为三类：Latency 延迟、Throughput 吞吐、Utilization 利用率。当然还有一些其他的比如经济成本效益等指标，就不在本文中进行过多介绍了。


- Latency延迟

在大语言模型领域延迟Latency指标衡量从请求提交到获得响应所需的时间，对于实时交互式应用至关重要。关于Latency延迟相关的指标，又可以细化分为：首Token生成时间/延迟（Time to First Token, TTFT）、逐个Token生成时间/延迟（Time Per Output Token, TPOT）、端到端请求时间/延迟（End-to-End Request Latency）。接下来逐个介绍他们的定义。

**首Token生成时间/延迟（Time to First Token, TTFT）：** TTFT衡量的是从用户提交查询到接收到第一个输出Token所需的时间。这包括了整个初始处理流程，如请求排队时间、预填充Prefill阶段以及任何网络延迟 。TTFT对于对话式AI等实时交互应用至关重要。低的TTFT能确保响应迅速的用户体验，因为它决定了用户感知模型开始生成输出的速度。

**逐个Token生成时间/延迟（Time Per Output Token, TPOT）：** TPOT定义为连续输出（也就是decode阶段）Token生成之间的平均时间延迟。从实际基准测试的角度来看，TPOT通常计算为:TPOT = (总延迟 - 首次生成时间) / 总输出Token数。

**端到端请求时间/延迟（End-to-End Request Latency）：** 该指标表示从提交查询到接收到完整响应（包括所有生成的Token）所经过的总时间。它提供了全面的衡量标准，考虑了整体推理服务系统的底层排队和批处理等机制的性能以及网络延迟。对于单个请求来说，它就是请求发送与最终Token接收之间的时间差。端到端延迟 = TTFT + TPOT * N（N为decode阶段生成的Token数量）

- Throughput吞吐量

吞吐量指标衡量推理服务系统在给定时间内处理的请求或令牌数量。可以细化分为：每秒Token数（Tokens Per Second, TPS）、每秒请求数量（Requests Per Second，RPS）。

在这里每秒Token数吞吐量TPS又可以被继续细分为Prefill预填充阶段的TPS和Decode解码阶段的TPS。并且对于这两个阶段的TPS可以直接通过延迟指标进行反推得出。即Prefill预填充阶段的**TPS = N / TTFT**（其中N为提示词prompt的数量），Decode解码阶段的**TPS = 1 / TPOT** 。

而对于RPS每秒请求数量这个指标而言，其是从一个服务系统整体上对吞吐的衡量。其中关于Request请求的定义是：当用户（或测试客户端）通过调用 API向 LLM 推理服务发出 一次调用，服务端接收到这个调用并将其放入处理队列（或开始处理）时，这个调用就被计为 一个请求。无论该请求是一个简单的“你好”还是一个复杂的问题，无论该请求会生成多少Token，都统一算作一次Reuqest。之所以要设置这样的一个性能指标的原因是，对于一个LLM推理服务系统来说，尤其是在一些高并发场景下，系统的瓶颈不一定总是在模型的GPU计算上，而是可能被例如：CPU对请求队列的处理、内存带宽限制、网络I/O、系统调度等等多种因素所影响。所以该指标是对一个整体服务系统吞吐量的性能衡量指标。

- 资源利用率

资源利用率指标衡量硬件资源被LLM推理任务有效利用的程度。其中可以大致分为计算资源利用率、内存资源利用率。 
对于计算资源利用率来说，主要是指GPU、CPU利用率。此指标衡量GPU或CPU在LLM推理相关计算任务中活跃处理时间的百分比 。高利用率表明硬件资源得到了高效利用。

内存利用率指标量化了LLM推理过程中，LLM的静态权重、动态激活以及键值（KV）缓存所消耗的GPU显存（VRAM）或系统内存（RAM）量。内存，特别是GPU显存，一直被认为是LLM推理的关键瓶颈，对于超大型模型和涉及长输入上下文的场景尤为如此 。高效的内存管理直接影响模型在可用硬件上的部署能力、可实现的吞吐量以及整体响应延迟。

## 推理性能对比

（实验）TBD
