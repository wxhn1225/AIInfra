本节将介绍几种主流开源模型在数据处理流程与方法上的实践。需要特别说明的是，数据是各大厂商的核心资源，虽然它们通常会开放模型权重，但很少会公开自己的训练数据。因此，大多数技术报告中并不会披露训练数据的具体细节。不过，通过厂商有限的信息披露，我们仍然能够学习和借鉴许多业界先进的数据处理方法。接下来将分别介绍 Qwen2.5、Qwen3、Llama3.1 以及 Gemini 等模型的数据处理流程。


## Llama3.1
- 技术报告：https://arxiv.org/pdf/2407.21783

Llama3.1在发布的时候被视作为开源之光，达到了当时开源模型的上届。且技术报告中详细的介绍了自己训练数据的相关信息，因此把它作为第一个学习的内容。



###  一、预训练数据处理

预训练是模型学习语言基础规律和世界知识的阶段，其数据处理的目标是构建大规模、高质量、多样化的文本语料库。Llama 3.1的预训练数据处理可分为**数据来源与筛选**、**清洗与去重**、**数据混合与优化**三个关键步骤。


#### 1.1 数据来源与筛选

Llama 3.1的预训练数据覆盖多领域、多语言，总规模达**15T tokens**（是Llama 2的8倍以上），主要来源包括：

- **网页数据**：从互联网收集的文本，涵盖知识、新闻、论坛等内容，是预训练数据的主要来源。
- **代码数据**：包括开源代码仓库（如GitHub）中的各类编程语言（Python、Java、C++等）代码及注释。
- **推理与数学数据**：包含数学题、逻辑推理题、科学论文等需要复杂推理的文本。
- **多语言数据**：覆盖176种语言，重点优化了英语、德语、法语、 Hindi等8种核心语言。

为确保数据质量，Llama 3.1首先通过**筛选机制**排除低质量来源：
- 移除包含大量个人可识别信息（PII）的网站（如含有身份证号、手机号的域名）；
- 过滤已知包含成人内容、有害信息的域名（基于Meta内部安全标准）；
- 剔除低质量文本（如内容重复、逻辑混乱的网页）。


#### 1.2 数据清洗与去重

原始数据往往存在噪声（如重复内容、格式混乱、错误信息），需通过清洗和去重提升质量。Llama 3.1的处理流程如下：

##### 1.2.1 文本提取与基础清洗
- **网页文本提取**：使用自定义HTML解析器从网页中提取核心内容，去除广告、导航栏等“ boilerplate（冗余格式文本）”，同时保留数学公式、代码块的结构（如保留`alt`标签中的数学公式文本）。
- **格式标准化**：移除markdown标记（实验发现markdown会降低模型在网页数据上的性能），统一文本编码和标点格式。
- **低质量过滤**：通过启发式规则剔除噪声，例如：
  - 移除包含大量重复n-gram的文本（如日志、错误信息中的重复字段）；
  - 过滤“脏词”比例过高的文本（如成人网站特有的词汇）；
  - 通过token分布的KL散度筛选“异常文档”（如包含大量罕见token的文本）。


##### 1.2.2 去重处理
重复数据会导致模型过度拟合，降低泛化能力。Llama 3.1采用**三级去重策略**：
- **URL级去重**：对同一URL的网页，仅保留最新版本；
- **文档级去重**：使用MinHash算法计算文档相似度，移除近重复文档（相似度超过阈值的文档）；
- **行级去重**：对文本中的单行内容，若在3000万文档中出现超过6次，则视为冗余内容并移除（如网站通用的“隐私政策”片段）。

*注：MinHash是一种快速计算文档相似度的算法，通过将文档映射为短哈希值，可高效比对大规模文档的重复程度。*


##### 1.2.3 领域专属数据处理
针对代码、推理等特殊领域数据，Llama 3.1设计了**领域专属 pipeline**：
- **代码数据**：通过DistilRoberta模型识别包含代码的网页，提取代码块并保留语法结构（如缩进、注释），过滤无法解析的错误代码；
- **推理数据**：重点筛选包含数学推导、科学推理的文本（如物理题解题步骤），通过模型标注确保内容的逻辑连贯性；
- **多语言数据**：使用fasttext语言识别模型将文本分类为176种语言，然后在单语言内部进行文档级和行级去重，避免跨语言重复。


#### 1.3 数据混合与优化

数据混合比例直接影响模型在不同任务上的性能。Llama 3.1通过**知识分类**和**缩放定律实验**确定最优混合比例：

- **知识分类**：使用分类器将数据分为“通用知识”“数学与推理”“代码”“多语言”等类别，下调网页中过度代表的类别（如娱乐内容）；
- **缩放定律实验**：训练多个小模型测试不同数据混合比例的效果，预测大模型性能，最终确定混合比例为：
  - 通用知识：50%
  - 数学与推理：25%
  - 代码：17%
  - 多语言：8%

此外，为进一步提升关键能力，Llama 3.1还采用**退火数据**策略：在预训练后期，使用高比例的高质量代码和数学数据进行微调，显著提升模型在GSM8K（数学题）等 benchmark上的性能（8B模型提升24%）。


### 二、微调数据处理

预训练模型仅具备基础语言能力，需通过微调（Post-training）对齐人类偏好（如遵循指令、安全无害）。Llama 3.1的微调数据处理聚焦于**偏好数据**和**SFT数据**的构建与优化。


#### 2.1 偏好数据处理

偏好数据用于训练模型理解“人类更喜欢什么样的回答”，是对齐模型行为的核心。Llama 3.1的偏好数据处理流程如下：

##### 2.1.1 数据收集
- **人类标注**：让标注者对比两个模型对同一prompt的回答，按“显著更好”“更好”“稍好”“几乎相同”四级评分；
- **编辑优化**：标注者对“更好”的回答进一步编辑（如修正错误、补充细节），形成“编辑后回答 > 原优选回答 > 被拒绝回答”的三级排序。

##### 2.1.2 数据筛选
- 仅保留“显著更好”和“更好”的样本（过滤差异不明显的样本）；
- 确保数据覆盖多领域：通用英语（81.99%）、代码（6.93%）、多语言（5.19%）、推理与工具使用（5.89%）。


#### 2.2 SFT数据处理

SFT（Supervised Fine-tuning，有监督微调）数据用于教会模型“如何生成符合指令的回答”，Llama 3.1的SFT数据来源与处理如下：

##### 2.2.1 数据来源
- **人类标注对话**：标注者与模型的多轮对话（如问答、代码调试）；
- **拒绝采样（Rejection Sampling）数据**：对同一prompt生成10-30个回答，用奖励模型（RM）筛选最优结果；
- **合成数据**：由模型自动生成的针对性数据（如数学题解题步骤、代码翻译案例）。

##### 2.2.2 质量控制
- **主题分类**：用Llama 3 8B模型将数据分类为“数学推理”“代码生成”等细粒度类别，确保覆盖全面；
- **质量评分**：结合奖励模型（RM）和Llama模型的评分（如“准确性”“指令遵循度”），仅保留高分样本；
- **难度筛选**：通过“意图标签（Instag）”和人工标注区分“简单”“中等”“困难”样本，优先保留高难度样本；
- **语义去重**：用RoBERTa模型聚类相似对话，在每个聚类中按“质量×难度”排序，仅保留不重复的优质样本。


#### 2.3 特殊能力数据优化

为增强模型在长文本、工具使用等场景的能力，Llama 3.1针对性处理了特殊数据：

- **长文本数据**：生成16K-128K tokens的超长文档问答、摘要数据（如书籍章节总结、代码仓库分析），仅占SFT数据的0.11%但显著提升长上下文理解；
- **工具使用数据**：包含搜索、代码解释器、Wolfram Alpha等工具调用的对话，标注者需评估“工具调用是否合理”“结果是否正确”；
- **多语言数据**：翻译高质量推理数据（如数学题）到非英语语言，避免“翻译腔”（如德语的语法错误）。


### 三、总结与思考


Llama 3.1的数据处理流程体现了三个核心原则：
1. **质量优先**：从源头筛选高价值数据，通过多级清洗、去重和评分确保数据纯净度；
2. **多样性平衡**：通过数据混合策略平衡通用知识与专项能力（代码、推理等），避免模型“偏科”；
3. **针对性优化**：对长文本、多语言等特殊场景设计专属数据处理流程，精准提升模型短板。


### 四、思考问题
下面是一些思考问题，大家在学习后可以尝试作答

1. 为什么Llama 3.1在预训练中要“逐步增加序列长度”（从8K到128K），而不是直接训练128K长文本？（提示：考虑自注意力的计算复杂度与模型适应性）
2. 多语言数据仅占预训练数据的8%，但Llama 3.1仍能支持8种语言的高质量输出，这说明数据处理中的哪些步骤起到了关键作用？
3. 合成数据（如模型生成的代码题）在SFT中占比很高，你认为合成数据的优势和潜在风险是什么？
4. 对比预训练数据和微调数据的处理目标，为什么预训练更注重“规模与多样性”，而微调更注重“质量与偏好对齐”？


## Qwen2.5
- 技术报告：https://arxiv.org/pdf/2412.15115

## Qwen3
- 技术报告：https://arxiv.org/pdf/2505.09388

## Gemini2.5
- 技术报告；https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf