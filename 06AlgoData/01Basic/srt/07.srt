1
00:00:00,000 --> 00:00:02,300
内容/录制:Z0MI酱，视频后期/字幕:梁嘉铭

2
00:00:02,400 --> 00:00:03,500
hello大家好

3
00:00:03,533 --> 00:00:05,733
人生不一定要往高处走

4
00:00:05,733 --> 00:00:08,300
我们也可以往四处走嘛

5
00:00:08,533 --> 00:00:09,400
到处玩一玩

6
00:00:09,400 --> 00:00:10,566
到处耍一耍哈

7
00:00:10,566 --> 00:00:12,766
到处看看有什么好玩的东西

8
00:00:16,933 --> 00:00:17,600
那今天

9
00:00:17,600 --> 00:00:20,633
我们来到了一个模型的参数的定义

10
00:00:20,633 --> 00:00:22,233
看一下整个Transformer

11
00:00:22,233 --> 00:00:24,966
变形金刚的参数是怎么定义

12
00:00:25,000 --> 00:00:26,466
同样的我们现在已经来到了

13
00:00:26,500 --> 00:00:28,033
整个Transformer架构

14
00:00:28,033 --> 00:00:31,233
整体相关的内容的最后一小节

15
00:00:31,233 --> 00:00:31,966
那这一小节

16
00:00:31,966 --> 00:00:33,666
是ZOMI特别特别的关心

17
00:00:33,700 --> 00:00:36,166
也是有很多客户会问ZOMI

18
00:00:36,166 --> 00:00:37,366
到底整个Transformer架构

19
00:00:37,366 --> 00:00:39,566
模型参数应该怎么去配比

20
00:00:39,566 --> 00:00:41,200
QKV应该怎么去设置

21
00:00:41,200 --> 00:00:42,900
然后一个sliding Windows

22
00:00:42,933 --> 00:00:45,833
sliding Windows的w怎么去设置

23
00:00:45,833 --> 00:00:46,566
所以我们今天

24
00:00:46,566 --> 00:00:48,700
就重点来去打开看看

25
00:00:48,733 --> 00:00:50,533
整个大模型的参数配比

26
00:00:50,566 --> 00:00:52,600
那关于整个系列的课程里面

27
00:00:52,600 --> 00:00:54,633
ZOMI除了这期的原理之外

28
00:00:54,633 --> 00:00:57,800
后面还会推出相关的一些实践的课程

29
00:00:57,800 --> 00:00:59,166
或者实践的内容了

30
00:00:59,166 --> 00:01:00,066
那不管怎么样

31
00:01:00,100 --> 00:01:01,733
整个Transformer的系列里面

32
00:01:01,733 --> 00:01:04,800
ZOMI觉得最有意思的就是第一个视频

33
00:01:04,866 --> 00:01:06,433
了解什么是Transformer

34
00:01:06,433 --> 00:01:07,033
那这个

35
00:01:07,033 --> 00:01:08,966
ZOMI觉得还是讲得非常不错

36
00:01:08,966 --> 00:01:09,500
那第2个

37
00:01:09,533 --> 00:01:12,566
就是看一下整个核心的机制的attention

38
00:01:12,566 --> 00:01:15,666
那因为整个从attention的来临出现

39
00:01:15,700 --> 00:01:16,633
到self attention

40
00:01:16,633 --> 00:01:17,900
再到Multi attention

41
00:01:17,933 --> 00:01:18,933
再到mask

42
00:01:18,966 --> 00:01:20,566
全都讲的明明白白了

43
00:01:20,566 --> 00:01:21,433
那第 3个

44
00:01:21,433 --> 00:01:22,600
ZOMI觉得特别重要

45
00:01:22,600 --> 00:01:23,600
就是去了解一下

46
00:01:23,600 --> 00:01:26,300
整个大模型的参数的配比

47
00:01:26,333 --> 00:01:28,800
它的配置应该怎么去设置

48
00:01:29,333 --> 00:01:31,733
同样我们看一下这一期视频

49
00:01:31,733 --> 00:01:32,633
或者今天的内容

50
00:01:32,633 --> 00:01:33,766
有哪些首先

51
00:01:33,766 --> 00:01:34,800
我们会看一下

52
00:01:34,800 --> 00:01:39,533
整个QKV的维度和头数的一个配比的参数的设置

53
00:01:39,600 --> 00:01:42,066
然后打开Qwen跟Llama

54
00:01:42,100 --> 00:01:43,900
两个比较重要的模型

55
00:01:43,900 --> 00:01:46,200
一个不同的参数的情况

56
00:01:46,200 --> 00:01:47,366
还有DeepSeek

57
00:01:47,366 --> 00:01:47,766
那最后

58
00:01:47,766 --> 00:01:48,500
我们看一下

59
00:01:48,533 --> 00:01:51,733
Qwen跟DeepSeek的一个模型的差异

60
00:01:51,733 --> 00:01:54,033
那大家都是MoE的架构

61
00:01:54,300 --> 00:01:56,300
MoE架构之间有什么差异

62
00:01:56,300 --> 00:01:59,300
大家往后的模型应该怎么去演进

63
00:01:59,300 --> 00:02:02,300
我们今天就主要是去学习

64
00:02:02,300 --> 00:02:04,700
去了解这 3个内容

65
00:02:05,800 --> 00:02:06,433
马上

66
00:02:06,433 --> 00:02:08,700
我们跟大家打开一个QKV的维度

67
00:02:08,733 --> 00:02:10,700
跟head数的配置

68
00:02:11,300 --> 00:02:12,200
哎呦慢着

69
00:02:12,200 --> 00:02:13,833
我们重播一下

70
00:02:14,000 --> 00:02:15,400
我们应该

71
00:02:15,400 --> 00:02:18,400
首先不是说去看一个QKV的维度

72
00:02:18,400 --> 00:02:21,100
而是打开HuggingFace的这个网站

73
00:02:21,133 --> 00:02:23,533
然后点击里面的file system

74
00:02:23,566 --> 00:02:26,900
那这个模型是Qwen3-8B的模型

75
00:02:26,933 --> 00:02:28,500
那在整个模型里面

76
00:02:28,500 --> 00:02:30,400
我们之前讲vocab的时候

77
00:02:30,400 --> 00:02:31,900
其实跟大家去看

78
00:02:32,000 --> 00:02:34,400
Tokenizer . json里面的作用

79
00:02:34,400 --> 00:02:37,366
它主要是加一些上下文的语义

80
00:02:37,366 --> 00:02:38,433
句子think

81
00:02:38,433 --> 00:02:39,366
unthink

82
00:02:39,366 --> 00:02:41,966
还有对应的句子开头的标志位

83
00:02:41,966 --> 00:02:43,300
Tokenizer的configure

84
00:02:43,333 --> 00:02:44,833
其实也是类似

85
00:02:44,833 --> 00:02:46,700
那vocab. json

86
00:02:46,733 --> 00:02:48,400
就是我们对应的词表了

87
00:02:48,400 --> 00:02:49,266
那Qwen3

88
00:02:49,300 --> 00:02:52,900
一共是15万的一个词表的大小

89
00:02:52,900 --> 00:02:56,233
每个词表都对应一个Token的一个ID

90
00:02:56,366 --> 00:02:57,600
那了解完这些之后

91
00:02:57,600 --> 00:03:00,033
其实ZOMI觉得整个file and version里面

92
00:03:00,033 --> 00:03:01,766
最重要的一个内容

93
00:03:01,766 --> 00:03:03,366
就是configur. json

94
00:03:03,366 --> 00:03:06,666
这里面就讲了很多的模型的参数

95
00:03:06,733 --> 00:03:08,033
应该怎么去配比

96
00:03:08,033 --> 00:03:10,866
然后它的模型参数到底是怎么做

97
00:03:10,900 --> 00:03:11,900
有没有drop out

98
00:03:11,900 --> 00:03:12,800
hidden Dim

99
00:03:12,800 --> 00:03:14,266
有多少还有

100
00:03:14,300 --> 00:03:15,766
一个hidden size有多少

101
00:03:15,766 --> 00:03:18,200
Max position inventing有多少了

102
00:03:18,200 --> 00:03:18,800
今天

103
00:03:18,800 --> 00:03:21,200
我们重点的去看一下这些参数

104
00:03:21,200 --> 00:03:23,266
应该怎么去设置

105
00:03:23,433 --> 00:03:24,833
回到PPT里面

106
00:03:24,833 --> 00:03:25,700
我们看一下

107
00:03:25,733 --> 00:03:26,733
首先蛮有意思

108
00:03:26,733 --> 00:03:28,233
就是注意力的头数

109
00:03:28,233 --> 00:03:30,100
我们看一下head number

110
00:03:30,166 --> 00:03:30,866
那head number

111
00:03:30,900 --> 00:03:33,233
实际上会跟隐藏层

112
00:03:33,433 --> 00:03:34,300
会成比例

113
00:03:34,333 --> 00:03:36,533
也就是hidden size成比例

114
00:03:36,533 --> 00:03:37,633
那单头的维度

115
00:03:37,633 --> 00:03:40,000
实际上是等于hidden size除以一个number

116
00:03:40,000 --> 00:03:42,600
head我们以两个模型作为例子

117
00:03:42,600 --> 00:03:46,100
一个是Qwen2-7B它的隐藏层的维度

118
00:03:46,100 --> 00:03:48,133
就是3,584

119
00:03:48,133 --> 00:03:49,166
那head size

120
00:03:49,166 --> 00:03:52,100
是28 所以说单head的维度

121
00:03:52,133 --> 00:03:53,433
就是head Dim

122
00:03:53,433 --> 00:03:56,433
就是3584除以那个28

123
00:03:56,433 --> 00:03:58,633
等于1289了

124
00:03:58,766 --> 00:04:00,200
那Llama3-8B

125
00:04:00,200 --> 00:04:00,833
蛮有意思

126
00:04:00,833 --> 00:04:01,600
也是一样

127
00:04:01,600 --> 00:04:04,066
隐藏层的维度是4096

128
00:04:04,166 --> 00:04:05,800
head size是32

129
00:04:05,800 --> 00:04:06,800
那头的维度

130
00:04:06,800 --> 00:04:08,066
就是4096除以32

131
00:04:08,100 --> 00:04:09,700
等于10821

132
00:04:09,966 --> 00:04:11,366
同样的我们看一下这个表

133
00:04:11,366 --> 00:04:14,500
里面其实关心的就是这个hidden size

134
00:04:14,533 --> 00:04:17,600
hidden size就是一个隐藏层的维度

135
00:04:17,600 --> 00:04:20,200
那head-Dim是128

136
00:04:20,200 --> 00:04:22,633
跟一个number attention head

137
00:04:22,633 --> 00:04:23,233
是一样

138
00:04:23,233 --> 00:04:24,766
我们有32个头

139
00:04:24,766 --> 00:04:26,800
每个头的维度是128所以

140
00:04:26,800 --> 00:04:31,066
32*128就等于hidden size 4096了

141
00:04:31,200 --> 00:04:32,100
那一般来说

142
00:04:32,133 --> 00:04:33,833
我们会保持一个原则

143
00:04:33,833 --> 00:04:35,500
就是单头的维度

144
00:04:35,533 --> 00:04:37,766
会在64到128

145
00:04:37,766 --> 00:04:39,500
可以看到不管是Qwen还是Llama了

146
00:04:39,533 --> 00:04:41,233
都是在128

147
00:04:41,233 --> 00:04:43,866
那为什么会保持在64跟128之间

148
00:04:43,900 --> 00:04:46,766
主要是整个要确保Transformer的架构

149
00:04:46,766 --> 00:04:48,366
特征的捕捉能力

150
00:04:48,366 --> 00:04:50,766
和计算效率的平衡

151
00:04:50,800 --> 00:04:52,033
因为如果我们单独

152
00:04:52,033 --> 00:04:52,900
维度越高

153
00:04:52,933 --> 00:04:54,700
特征的捕捉能力

154
00:04:54,700 --> 00:04:55,833
可能会变弱

155
00:04:55,833 --> 00:04:56,933
会变泛化

156
00:04:56,966 --> 00:04:57,966
计算的效率

157
00:04:57,966 --> 00:04:58,833
就计算量

158
00:04:58,833 --> 00:04:59,600
就比较大

159
00:04:59,600 --> 00:05:02,033
但是又不能取得很好的效果

160
00:05:02,033 --> 00:05:04,300
那虽然我们现在这么去讲hidden size

161
00:05:04,333 --> 00:05:04,966
hidden Dim

162
00:05:04,966 --> 00:05:07,966
还有那个hidden head的一个维度Dimension

163
00:05:07,966 --> 00:05:10,900
有点虚我们现在看一下这个图

164
00:05:10,933 --> 00:05:11,600
这个图

165
00:05:11,600 --> 00:05:12,466
首先乍一看

166
00:05:12,500 --> 00:05:13,700
好像有点复杂

167
00:05:13,700 --> 00:05:15,433
我们简单的串一串

168
00:05:15,433 --> 00:05:17,433
首先这个就是hidden stage

169
00:05:17,433 --> 00:05:20,000
上一层传过来的特征

170
00:05:20,000 --> 00:05:21,966
我们假设它是一个x

171
00:05:21,966 --> 00:05:23,266
就输入x嘛

172
00:05:23,300 --> 00:05:26,100
s要乘以一个矩阵w k

173
00:05:26,100 --> 00:05:28,166
得到一个key

174
00:05:28,266 --> 00:05:29,500
那x乘以wq

175
00:05:29,500 --> 00:05:31,833
就得到我们另外一个特征

176
00:05:31,833 --> 00:05:32,966
叫做quer

177
00:05:33,033 --> 00:05:34,233
x乘以w u

178
00:05:34,233 --> 00:05:36,600
得到第 3个特征value

179
00:05:36,600 --> 00:05:40,700
那现在我们有了q k v 3个之后

180
00:05:40,733 --> 00:05:42,433
那K1乘以q相乘

181
00:05:42,433 --> 00:05:43,966
然后再得到的score

182
00:05:43,966 --> 00:05:45,666
乘以v这就是整个Transformer

183
00:05:45,700 --> 00:05:47,000
一个最基本原理嘛

184
00:05:47,000 --> 00:05:49,000
那这个为什么这么去乘

185
00:05:49,000 --> 00:05:51,166
大家可以看回Transformer

186
00:05:51,166 --> 00:05:54,100
ZOMI之前分享的Transformer的来源

187
00:05:54,133 --> 00:05:55,966
那不管怎么样

188
00:05:56,166 --> 00:05:56,833
我们现在

189
00:05:56,833 --> 00:05:58,800
更关心的是它的参数

190
00:05:58,800 --> 00:06:01,600
首先我们刚才讲到的隐藏层的维度

191
00:06:01,600 --> 00:06:03,300
特别是attention的隐藏层维度

192
00:06:03,333 --> 00:06:04,600
主要是这个

193
00:06:04,600 --> 00:06:07,400
参数一般会设置成为hidden size

194
00:06:07,500 --> 00:06:08,966
这个隐藏层的维度

195
00:06:08,966 --> 00:06:13,066
主要是指WK WQ WV的这个hidden size

196
00:06:13,100 --> 00:06:14,433
它的一个大小

197
00:06:14,433 --> 00:06:16,266
这个维度的大小

198
00:06:16,766 --> 00:06:18,300
那关于lumber_head

199
00:06:18,333 --> 00:06:19,500
注意力的头数

200
00:06:19,500 --> 00:06:20,633
就是指这里面

201
00:06:20,633 --> 00:06:23,233
 我们有多少个attention

202
00:06:23,233 --> 00:06:24,166
那这个attention

203
00:06:24,166 --> 00:06:25,500
主要是指self attention

204
00:06:25,533 --> 00:06:26,000
当然了

205
00:06:26,000 --> 00:06:27,800
self attention也可以叫做

206
00:06:27,800 --> 00:06:30,100
Scaled dot-product self-attention

207
00:06:30,133 --> 00:06:32,500
也就是矩阵的点乘的一个attention

208
00:06:32,500 --> 00:06:33,700
因为之前讲到了attention 

209
00:06:33,700 --> 00:06:35,333
其实它有很多种计算方式

210
00:06:35,333 --> 00:06:36,533
你可以用矩阵点乘

211
00:06:36,533 --> 00:06:38,600
你可以用sin cos tan

212
00:06:38,600 --> 00:06:39,200
各种各样

213
00:06:39,200 --> 00:06:41,266
只是点乘的效果和效率

214
00:06:41,300 --> 00:06:43,233
会更好那不管怎么样

215
00:06:43,233 --> 00:06:45,266
有兴趣的同学或有兴趣的小伙伴

216
00:06:45,300 --> 00:06:46,566
可以看回之前的视频

217
00:06:46,566 --> 00:06:47,200
我们现在

218
00:06:47,200 --> 00:06:49,666
还是关注于number head的大小

219
00:06:49,700 --> 00:06:52,100
head主要是指头数的数量

220
00:06:52,100 --> 00:06:55,433
有多少个可并行的一个self attention

221
00:06:55,433 --> 00:06:57,833
那回到我们上面的这个图

222
00:06:57,833 --> 00:07:01,066
那这个我们刚才讲到的一个head Dim

223
00:07:01,100 --> 00:07:02,500
就单头的维度

224
00:07:02,500 --> 00:07:03,966
主要就是指这里面

225
00:07:03,966 --> 00:07:07,966
hidden sized除以一个头数

226
00:07:07,966 --> 00:07:11,633
就是我们这里面的每一个的宽

227
00:07:11,633 --> 00:07:15,200
每一个WK的一个宽度就是指这个了

228
00:07:15,200 --> 00:07:17,233
因为在真正计算的时候

229
00:07:17,233 --> 00:07:19,900
我不是把所有的这个单独的head啦

230
00:07:19,933 --> 00:07:21,766
或者self attention都是单独算

231
00:07:21,766 --> 00:07:24,766
我是合在一个大矩阵里面去算

232
00:07:24,766 --> 00:07:27,966
所以就变成了一个单头的维度

233
00:07:27,966 --> 00:07:29,566
就是指hidden size

234
00:07:29,566 --> 00:07:31,300
除以head数量

235
00:07:31,333 --> 00:07:32,500
那这么去来

236
00:07:32,500 --> 00:07:34,933
所以大家感性的去认识一下

237
00:07:34,933 --> 00:07:37,133
我们现在已经了解完单头的维度

238
00:07:37,133 --> 00:07:39,700
hidden size跟number head之间的关系

239
00:07:39,700 --> 00:07:42,366
那我们隐层的维度

240
00:07:42,366 --> 00:07:43,166
除以number head

241
00:07:43,166 --> 00:07:44,900
就等于单头的维度了

242
00:07:44,933 --> 00:07:46,000
那现在来看

243
00:07:46,000 --> 00:07:50,266
大部分大模型的单头维度都是在128

244
00:07:51,433 --> 00:07:52,366
那第2个内容

245
00:07:52,366 --> 00:07:54,833
就是KV的value值

246
00:07:54,833 --> 00:07:55,866
KV的head

247
00:07:55,900 --> 00:07:57,333
到底有多少了

248
00:07:57,333 --> 00:08:00,000
到底KV head应该是怎么去设置

249
00:08:00,300 --> 00:08:03,900
同样我们还是打开一个Qwen3-8B

250
00:08:03,900 --> 00:08:05,366
看一下KV的head

251
00:08:05,366 --> 00:08:06,166
这边蛮有意思

252
00:08:06,166 --> 00:08:09,200
就是number KV head有个单独的数量

253
00:08:09,200 --> 00:08:10,600
是等于8

254
00:08:10,700 --> 00:08:12,766
那我们回到PPT里面

255
00:08:12,766 --> 00:08:14,200
那KV head

256
00:08:14,200 --> 00:08:15,700
一共有8个

257
00:08:15,733 --> 00:08:16,400
蛮有意思

258
00:08:16,400 --> 00:08:17,466
这里面一共有8个

259
00:08:17,500 --> 00:08:19,700
就证明了整个Qwen

260
00:08:19,700 --> 00:08:21,766
实际上它不是用MQA

261
00:08:21,766 --> 00:08:23,033
也不是用MHA

262
00:08:23,033 --> 00:08:24,300
也不是用MLA

263
00:08:24,333 --> 00:08:27,166
而是真正使用的是GQA

264
00:08:27,166 --> 00:08:29,400
所以我们之前说到了GQA

265
00:08:29,400 --> 00:08:30,833
现在用的是最多

266
00:08:30,833 --> 00:08:32,500
一共有8个k跟v

267
00:08:32,566 --> 00:08:33,633
那这KV值

268
00:08:33,633 --> 00:08:35,233
应该怎么去设置

269
00:08:35,233 --> 00:08:37,300
那首先我们看一下之前我们讲到

270
00:08:37,333 --> 00:08:39,633
分组查询注意力

271
00:08:39,633 --> 00:08:40,866
GQA

272
00:08:41,000 --> 00:08:44,033
最重要的就是去设置KV head

273
00:08:44,033 --> 00:08:45,966
减少显存的消耗

274
00:08:45,966 --> 00:08:48,900
让更多的这些Multi head attention

275
00:08:48,933 --> 00:08:52,133
attention之间共享k值跟v值

276
00:08:52,266 --> 00:08:53,866
那在Qwen2-7B里面

277
00:08:53,900 --> 00:08:55,766
Query是28个

278
00:08:55,766 --> 00:08:57,800
然后KV值是4个

279
00:08:57,800 --> 00:09:00,800
那形成了7:1的一个分组比例

280
00:09:00,833 --> 00:09:03,033
那在Llama3-8B里面

281
00:09:03,033 --> 00:09:04,266
Query是32

282
00:09:04,300 --> 00:09:06,100
KV head是8个

283
00:09:06,100 --> 00:09:07,300
那Qwen的3

284
00:09:07,300 --> 00:09:08,566
其实也变成8个了

285
00:09:08,566 --> 00:09:10,666
形成4:1的比例

286
00:09:10,766 --> 00:09:11,700
那总体来说

287
00:09:11,700 --> 00:09:12,800
原则

288
00:09:12,800 --> 00:09:14,900
就是KV的head数

289
00:09:14,933 --> 00:09:17,400
通常为1/4或者1/8

290
00:09:17,400 --> 00:09:20,866
那整体1/4跟1/8的显存的需求

291
00:09:20,900 --> 00:09:25,500
可以对比没有用GQA要减少30%-50%

292
00:09:25,500 --> 00:09:28,400
但是也能保证模型的效果很好

293
00:09:28,400 --> 00:09:30,100
那这里面就回到我们这个图

294
00:09:30,133 --> 00:09:32,833
真正的现在用的大模型GQA

295
00:09:32,833 --> 00:09:34,366
GQA就变成这样

296
00:09:34,366 --> 00:09:35,466
这里面是32

297
00:09:35,500 --> 00:09:37,033
上面去4*8

298
00:09:37,033 --> 00:09:37,266
那就

299
00:09:37,300 --> 00:09:42,333
意味着四个quiver共享一个k值跟v值

300
00:09:42,333 --> 00:09:44,933
这种方式去节省内存

301
00:09:45,633 --> 00:09:47,366
那接下来我们看一下QKV

302
00:09:47,366 --> 00:09:50,166
相关的参数的和hidden size的配置

303
00:09:50,166 --> 00:09:51,233
是怎么比例

304
00:09:51,233 --> 00:09:53,966
首先就是head数跟维度

305
00:09:53,966 --> 00:09:55,100
现在我们可以看到

306
00:09:55,133 --> 00:09:56,200
必须要保持平衡

307
00:09:56,200 --> 00:09:58,166
也就我们刚才讲到

308
00:09:58,166 --> 00:10:01,500
大概是64-128左右

309
00:10:01,833 --> 00:10:03,400
对于单头的维度

310
00:10:03,400 --> 00:10:04,966
那我们增加head

311
00:10:04,966 --> 00:10:06,100
增加头数

312
00:10:06,133 --> 00:10:06,500
实际上

313
00:10:06,500 --> 00:10:09,300
是可以提升我们模型的并行能力

314
00:10:09,566 --> 00:10:11,266
但是提升并行能力的时候

315
00:10:11,300 --> 00:10:13,033
我们还是要降低hidden Dim

316
00:10:13,033 --> 00:10:15,266
以控制显存的增加

317
00:10:15,333 --> 00:10:17,333
那如果hidden Dim过小了

318
00:10:17,333 --> 00:10:20,033
就可能导致模型的表达能力不足了

319
00:10:20,033 --> 00:10:23,066
那现在一般来说设置的都是128

320
00:10:23,400 --> 00:10:24,033
那第2个

321
00:10:24,033 --> 00:10:25,433
就是层数的扩展

322
00:10:25,433 --> 00:10:26,700
我们可以知道

323
00:10:26,733 --> 00:10:28,933
这个有多少层网络模型

324
00:10:28,933 --> 00:10:31,433
模型的规模就会越来越大

325
00:10:31,433 --> 00:10:32,500
那层数堆叠了

326
00:10:32,533 --> 00:10:34,133
每增加一倍左右

327
00:10:34,133 --> 00:10:35,533
模型的参数量

328
00:10:35,533 --> 00:10:37,833
会显著的提升一倍左右

329
00:10:37,833 --> 00:10:38,233
但是

330
00:10:38,233 --> 00:10:40,900
模型的参数量或者模型的层数

331
00:10:40,933 --> 00:10:41,800
增加一倍

332
00:10:41,800 --> 00:10:45,033
我们就需要更大的一个数据配比

333
00:10:45,033 --> 00:10:46,833
去进行一个计算

334
00:10:46,833 --> 00:10:48,233
所以说每增加一倍

335
00:10:48,233 --> 00:10:50,500
我们大概需要增加1.4T Token

336
00:10:50,533 --> 00:10:53,033
去增加模型的配比

337
00:10:56,433 --> 00:10:58,200
那我们现在看第2个内容

338
00:10:58,200 --> 00:11:00,666
就是模型的一个参数

339
00:11:00,700 --> 00:11:03,000
就网络模型的参数量是怎么去设置

340
00:11:03,000 --> 00:11:05,166
那这里面蛮有意思的就是ZOMI

341
00:11:05,166 --> 00:11:07,500
就简单的分析了Qwen3

342
00:11:07,533 --> 00:11:09,166
因为Qwen3这个稠密

343
00:11:09,166 --> 00:11:10,400
模型的参数量

344
00:11:10,433 --> 00:11:11,433
是非常的多

345
00:11:11,433 --> 00:11:12,766
都是小钢炮

346
00:11:12,766 --> 00:11:15,033
所谓的小钢炮就是小规模的大

347
00:11:15,033 --> 00:11:20,500
模型我们叫做SLLM small large language model

348
00:11:20,566 --> 00:11:21,566
那千文3

349
00:11:21,566 --> 00:11:24,600
一共有 123456 6组

350
00:11:24,600 --> 00:11:26,966
从0.6B 1.7B

351
00:11:26,966 --> 00:11:28,200
4B 8B

352
00:11:28,200 --> 00:11:32,400
14B到32B那一共有这么几组参数

353
00:11:32,400 --> 00:11:33,300
来重点看一下

354
00:11:33,333 --> 00:11:35,233
刚才讲到的hidden Dim

355
00:11:35,233 --> 00:11:37,100
基本上是固定在128

356
00:11:37,133 --> 00:11:39,633
因为模型的泛化表达能力比较好

357
00:11:39,633 --> 00:11:41,166
那hidden activation

358
00:11:41,166 --> 00:11:42,500
就是一个激活

359
00:11:42,533 --> 00:11:45,400
基本上现在都用比较统一的SiLU

360
00:11:45,500 --> 00:11:48,000
那比较大的区别就是hidden size了

361
00:11:48,000 --> 00:11:50,866
我们刚才讲到的QKV的矩阵

362
00:11:50,900 --> 00:11:54,366
W WQ WV WK

363
00:11:54,400 --> 00:11:55,266
那 3个矩阵

364
00:11:55,300 --> 00:11:55,933
可以看到

365
00:11:55,933 --> 00:11:56,966
基本上

366
00:11:56,966 --> 00:11:59,633
如果网络模型越小

367
00:11:59,733 --> 00:12:01,166
参数量越小

368
00:12:01,166 --> 00:12:02,700
hidden size

369
00:12:02,733 --> 00:12:06,100
是越少也就是对应权重

370
00:12:06,100 --> 00:12:07,500
这个hidden size

371
00:12:07,500 --> 00:12:08,766
是越小

372
00:12:08,766 --> 00:12:11,666
因为这个能够极大地去减少

373
00:12:11,700 --> 00:12:13,500
模型的参数量

374
00:12:14,100 --> 00:12:14,900
那第 3个

375
00:12:14,900 --> 00:12:16,733
就是intermediate size了

376
00:12:16,733 --> 00:12:17,300
那这个

377
00:12:17,300 --> 00:12:19,133
对应的是FFN层

378
00:12:19,133 --> 00:12:20,966
也就是Transformer架构

379 
00:12:21,166 --> 00:12:21,833
这里面

380
00:12:21,833 --> 00:12:22,600
整个Transformer

381
00:12:22,600 --> 00:12:24,233
会有一个feed forward

382
00:12:24,233 --> 00:12:27,400
就是FFN层里面的hidden size

383
00:12:27,400 --> 00:12:28,966
就是这个参数量了

384
00:12:28,966 --> 00:12:30,266
我们刚才所设置

385
00:12:30,300 --> 00:12:32,633
都是attention层的一个hidden size

386
00:12:32,633 --> 00:12:33,266
那现在

387
00:12:33,300 --> 00:12:37,333
intermediate size就是这个FFN层的参数量

388
00:12:37,333 --> 00:12:38,466
它的维度

389
00:12:38,466 --> 00:12:40,466
基本上改变最多

390
00:12:40,500 --> 00:12:44,533
就是hidden size跟intermediate size

391
00:12:44,533 --> 00:12:44,933
这两个

392
00:12:44,933 --> 00:12:47,566
会极大地影响模型的参数量

393
00:12:47,566 --> 00:12:49,266
同样的模型参数量的配比

394
00:12:49,300 --> 00:12:50,566
也会很高

395
00:12:50,566 --> 00:12:52,233
那Max position embedding

396
00:12:52,233 --> 00:12:53,566
position embedding

397
00:12:53,566 --> 00:12:55,233
基本上都会比较固定

398
00:12:55,233 --> 00:12:57,233
然后Max Windows layer

399
00:12:57,233 --> 00:12:58,866
会有一定的差异

400
00:12:58,900 --> 00:13:01,200
那hidden ，attention head

401
00:13:01,200 --> 00:13:03,900
随着模型的规模head数

402
00:13:03,933 --> 00:13:05,600
变化也比较大

403
00:13:05,700 --> 00:13:07,333
那对应的number hidden layer

404
00:13:07,333 --> 00:13:08,800
其实这个是算出来

405
00:13:08,800 --> 00:13:11,833
也就hidden size除以那个head数头数

406
00:13:11,833 --> 00:13:14,300
就等于number hidden layer了

407
00:13:14,366 --> 00:13:17,100
所以number一的内容就不单独圈出来了

408
00:13:17,133 --> 00:13:19,766
那刚才讲到的都会用GQA

409
00:13:19,766 --> 00:13:21,833
所以number kVhead是8个

410
00:13:21,833 --> 00:13:23,033
而vocab size

411
00:13:23,033 --> 00:13:24,833
就是我们词汇表的大小

412
00:13:24,833 --> 00:13:27,366
不管小模型还是大模型

413
00:13:27,366 --> 00:13:30,366
之前有一个客户去问ZOMI了

414
00:13:30,366 --> 00:13:31,400
是不是小模型

415
00:13:31,400 --> 00:13:32,766
我的词汇表就小一点

416
00:13:32,766 --> 00:13:34,433
我的大模型词汇表就大一点

417
00:13:34,433 --> 00:13:36,300
其实没有这个必然的意义

418
00:13:36,333 --> 00:13:38,500
反正模型小跟模型大

419
00:13:38,500 --> 00:13:41,733
我们都希望用更能够学习更多的知识

420
00:13:41,733 --> 00:13:44,000
所以说词汇表都可以做相同

421
00:13:44,000 --> 00:13:46,600
而且真正的可能0.6B

422
00:13:46,600 --> 00:13:49,266
1.7B跟4B这些模型不是训出来

423
00:13:49,300 --> 00:13:52,833
而是由32B这些模型蒸馏出来

424
00:13:52,833 --> 00:13:54,566
蒸馏给这些小模型

425
00:13:54,566 --> 00:13:56,200
所以说越小的模型

426
00:13:56,200 --> 00:13:57,866
它词汇表大小是一样

427
00:13:57,900 --> 00:14:00,166
是因为从上面蒸馏出来

428
00:14:00,166 --> 00:14:01,466
如果词汇表不一样了

429
00:14:01,500 --> 00:14:03,700
有可能蒸馏的效果就不好了

430
00:14:03,700 --> 00:14:05,166
那现在来看到了

431
00:14:05,166 --> 00:14:06,566
小规模的模型

432
00:14:06,566 --> 00:14:09,066
自己训完之后再进行蒸馏

433
00:14:09,100 --> 00:14:11,033
效果会比完全

434
00:14:11,033 --> 00:14:14,033
原生的去训练一个0.6B的模型

435
00:14:14,033 --> 00:14:15,166
效果越好

436
00:14:15,166 --> 00:14:16,366
所以说现在

437
00:14:16,366 --> 00:14:19,033
大部分的小规模的大模型

438
00:14:19,033 --> 00:14:20,166
都是蒸馏出来

439
00:14:20,166 --> 00:14:23,100
而在整个大模型的参数里面

440
00:14:23,133 --> 00:14:24,566
现在变化比较大

441
00:14:24,566 --> 00:14:26,866
或者极大的影响模型参数量

442
00:14:26,900 --> 00:14:27,966
就是hidden size

443
00:14:28,133 --> 00:14:29,300
intermediate size

444
00:14:29,300 --> 00:14:30,933
还有一个attention head啦

445
00:14:30,933 --> 00:14:32,366
那这个是被动响应

446
00:14:32,366 --> 00:14:33,000
那其他

447
00:14:33,000 --> 00:14:34,600
其实差别不大

448
00:14:34,766 --> 00:14:36,000
那Qwen3

449
00:14:36,000 --> 00:14:38,200
其实它有两个系列的模型

450
00:14:38,200 --> 00:14:39,433
一个30B

451
00:14:39,433 --> 00:14:40,966
激活的时候是3B

452
00:14:40,966 --> 00:14:41,500
另外一个

453
00:14:41,533 --> 00:14:42,900
是235B

454
00:14:42,900 --> 00:14:44,700
激活时候是22B

455
00:14:44,700 --> 00:14:45,166
另外的话

456
00:14:45,166 --> 00:14:45,800
我们这里面

457
00:14:45,800 --> 00:14:47,000
做了一个对比

458
00:14:47,000 --> 00:14:49,866
也就是DeepSeek V2的236B

459
00:14:50,000 --> 00:14:52,300
还有DeepSeek V3的671B

460
00:14:52,300 --> 00:14:54,533
因为现在幻方也是非常的火

461
00:14:54,533 --> 00:14:55,766
同样我们观察一下

462
00:14:55,766 --> 00:14:57,666
它的一个参数的配比

463
00:14:57,700 --> 00:14:59,333
那这些参数的配比

464
00:14:59,333 --> 00:15:00,500
ZOMI再啰嗦一下

465
00:15:00,500 --> 00:15:03,033
就是它可以来源于HuggingFace里面

466
00:15:03,033 --> 00:15:04,400
找到对应的模型

467
00:15:04,400 --> 00:15:08,500
看一下file system里面的这些参数了

468
00:15:08,533 --> 00:15:09,133
那同样

469
00:15:09,133 --> 00:15:10,566
我们回到PPT里面

470
00:15:10,566 --> 00:15:11,100
hidden Dim

471
00:15:11,133 --> 00:15:12,933
刚才讲到的都非常固定

472
00:15:12,933 --> 00:15:14,933
都是128 所以hidden Dim

473
00:15:14,933 --> 00:15:15,833
已经固定了

474
00:15:15,833 --> 00:15:17,100
那activation

475
00:15:17,133 --> 00:15:18,200
也是激活

476
00:15:18,200 --> 00:15:19,066
都用silu

477
00:15:19,233 --> 00:15:20,166
那hidden size

478
00:15:20,166 --> 00:15:21,866
可能会有所区别

479
00:15:21,900 --> 00:15:23,600
DeepSeek幻方的hidden size

480
00:15:23,600 --> 00:15:26,033
会比Qwen的hidden size要大

481
00:15:26,166 --> 00:15:27,600
那intermediate size

482
00:15:27,600 --> 00:15:28,966
就对应的FFN层

483
00:15:28,966 --> 00:15:30,466
也会比它要大

484
00:15:30,500 --> 00:15:31,166
那说白了

485
00:15:31,166 --> 00:15:32,100
这么去看

486
00:15:32,133 --> 00:15:33,733
整个DeepSeek的模型

487
00:15:33,733 --> 00:15:36,966
好像要比Qwen的模型要胖一点

488
00:15:37,000 --> 00:15:37,666
那同样

489
00:15:37,700 --> 00:15:39,933
这个max position embendding

490
00:15:39,933 --> 00:15:41,833
也会非常的宽

491
00:15:42,000 --> 00:15:43,233
同样的有一些差别

492
00:15:43,233 --> 00:15:44,466
就是MoE

493
00:15:44,500 --> 00:15:44,966
这里面

494
00:15:44,966 --> 00:15:45,866
这 3个参数

495
00:15:45,900 --> 00:15:47,500
标的不同颜色

496
00:15:47,500 --> 00:15:50,000
都是MoE架构独有

497
00:15:50,000 --> 00:15:52,100
那FFM层的MoE可以看到

498
00:15:52,133 --> 00:15:54,033
基本上Qwen3的最新一代

499
00:15:54,033 --> 00:15:56,766
跟Qwen2的是保持一致

500
00:15:56,966 --> 00:15:58,100
DeepSeek V3

501
00:15:58,133 --> 00:15:59,333
可能会更大

502
00:15:59,333 --> 00:16:00,966
那后面的一个number expert

503
00:16:00,966 --> 00:16:01,866
蛮有意思

504
00:16:01,900 --> 00:16:04,633
Qwen的number expert就是他的专家数量

505
00:16:04,633 --> 00:16:05,433
不会太多

506
00:16:05,433 --> 00:16:07,233
在128 但是

507
00:16:07,233 --> 00:16:08,666
DeepSeek的专家数量

508
00:16:08,700 --> 00:16:13,833
到了256 当专家数量越多模型越不容易训练

509
00:16:13,833 --> 00:16:15,000
越容易发散

510
00:16:15,000 --> 00:16:16,166
越不容易收敛

511
00:16:16,166 --> 00:16:17,233
因为专家多了

512
00:16:17,233 --> 00:16:18,700
模型

513
00:16:18,733 --> 00:16:20,633
或者专家的均衡

514
00:16:20,833 --> 00:16:21,966
变得很重要了

515
00:16:21,966 --> 00:16:22,366
所以

516
00:16:22,366 --> 00:16:24,833
我们可以看到Qwen还是相对保守

517
00:16:24,833 --> 00:16:26,233
因为这个是Qwen

518
00:16:26,233 --> 00:16:27,966
第一代的MoE的架构

519
00:16:27,966 --> 00:16:28,833
那DeepSeek

520
00:16:28,833 --> 00:16:29,700
这个V3

521
00:16:29,733 --> 00:16:31,800
已经是它第 3代的MoE了

522
00:16:31,800 --> 00:16:35,500
它可以扩充到更大的一个MoE的模型

523
00:16:35,533 --> 00:16:37,400
MoE的专家的数量

524
00:16:37,433 --> 00:16:38,766
当然了这里面有个区别

525
00:16:38,766 --> 00:16:40,400
就是整个DeepSeek

526
00:16:40,400 --> 00:16:42,066
它有共享专家

527
00:16:42,100 --> 00:16:43,300
而整个Qwen

528
00:16:43,300 --> 00:16:44,966
它是没有共享专家

529
00:16:44,966 --> 00:16:45,600
我们后面

530
00:16:45,600 --> 00:16:46,066
会打开

531
00:16:46,100 --> 00:16:48,433
Qwen跟DeepSeek的一个模型的差异

532
00:16:48,766 --> 00:16:49,200
那下面

533
00:16:49,200 --> 00:16:50,200
就number hidden layer了

534
00:16:50,200 --> 00:16:53,666
就是模型的层数了

535
00:16:53,700 --> 00:16:54,300
那Qwen

536
00:16:54,300 --> 00:16:54,933
可以看到

537
00:16:54,933 --> 00:16:56,600
它比较瘦

538
00:16:56,700 --> 00:16:58,700
比较长hidden layer

539
00:16:58,700 --> 00:16:59,433
也比较多

540
00:16:59,433 --> 00:17:01,033
就是模型的层很多

541
00:17:01,033 --> 00:17:03,800
那DeepSeek它的一个参数

542
00:17:03,800 --> 00:17:04,833
会比较多

543
00:17:04,833 --> 00:17:06,500
但是模型层没那么高

544
00:17:06,533 --> 00:17:09,133
证明这个模型会比较胖一点

545
00:17:09,300 --> 00:17:11,633
那number KV head蛮有意思

546
00:17:11,633 --> 00:17:13,800
就是DeepSeek的KV head

547
00:17:13,800 --> 00:17:15,566
共享的非常的多

548
00:17:15,633 --> 00:17:17,166
就KV head很多

549
00:17:17,166 --> 00:17:19,300
通过很多个KV head去学习

550
00:17:19,333 --> 00:17:20,600
但是Qwen的KV head 

551
00:17:20,600 --> 00:17:23,033
很少大部分都是做共享

552
00:17:23,033 --> 00:17:25,566
那最后一个差别就是词汇表了

553
00:17:25,600 --> 00:17:27,833
Qwen的词汇表是15万

554
00:17:27,833 --> 00:17:30,500
但是DeepSeek的词汇表是十万

555
00:17:30,533 --> 00:17:31,400
那可以看到

556
00:17:31,400 --> 00:17:33,866
DeepSeek主要是处理中文跟英文

557
00:17:33,900 --> 00:17:34,800
而Qwen

558
00:17:34,800 --> 00:17:36,900
可以做各种各样的语言

559
00:17:36,933 --> 00:17:38,200
包括阿拉伯语

560
00:17:38,200 --> 00:17:38,866
法语英语

561
00:17:38,900 --> 00:17:40,500
德语都能够处理

562
00:17:40,500 --> 00:17:41,500
这个就是词汇表

563
00:17:41,500 --> 00:17:43,800
带来最大的差别和意义

564
00:17:44,166 --> 00:17:45,200
讲完这些参数之后

565
00:17:45,200 --> 00:17:46,500
我们看一下这些参数

566
00:17:46,533 --> 00:17:47,200
对模型

567
00:17:47,200 --> 00:17:48,200
一些影响

568
00:17:48,200 --> 00:17:50,266
当然这些只是总结性的内容了

569
00:17:50,300 --> 00:17:52,700
大家也可以自己去训练模型的时候

570
00:17:52,700 --> 00:17:54,533
去注意一下相关的内容

571
00:17:54,600 --> 00:17:55,033
那第一个

572
00:17:55,033 --> 00:17:56,300
就是MHA

573
00:17:56,333 --> 00:17:58,500
很多头注意力机制相关的参数

574
00:17:58,500 --> 00:17:59,966
首先就是头数

575
00:18:00,000 --> 00:18:02,766
头数是真正决定我们模型并行

576
00:18:02,766 --> 00:18:04,266
关注不同的子空间

577
00:18:04,300 --> 00:18:05,900
就并行度越高

578
00:18:05,900 --> 00:18:07,700
每个并行的head

579
00:18:07,700 --> 00:18:10,766
是针对不同的序列提取不同的特征

580
00:18:10,933 --> 00:18:11,800
头数越多

581
00:18:11,800 --> 00:18:13,300
可以对我们输入的特征

582
00:18:13,333 --> 00:18:15,766
提取出更高维度的信息

583
00:18:15,766 --> 00:18:18,000
但是会增加计算量

584
00:18:18,566 --> 00:18:19,866
所以number head

585
00:18:19,900 --> 00:18:23,400
一般是从12到那个128之间

586
00:18:23,400 --> 00:18:25,633
反正number head可以换算出来

587
00:18:25,633 --> 00:18:27,600
我们之前也讲过怎么换算

588
00:18:27,700 --> 00:18:28,133
第2个

589
00:18:28,133 --> 00:18:30,800
就是QKV的线性层

590
00:18:30,800 --> 00:18:31,966
那QKV

591
00:18:31,966 --> 00:18:34,700
说实话是向量投影的一个具体参数

592
00:18:34,733 --> 00:18:37,733
会直接影响到attention的稳定性

593
00:18:37,733 --> 00:18:40,300
跟效率需要跟hidden size

594
00:18:40,300 --> 00:18:42,700
隐藏层相互配合

595
00:18:42,700 --> 00:18:44,500
不能太大也不能太小

596
00:18:44,500 --> 00:18:45,400
大家可以参考

597
00:18:45,400 --> 00:18:47,566
Qwen跟DeepSeek相关的模型

598
00:18:48,300 --> 00:18:49,766
第2个就是FFN

599
00:18:49,766 --> 00:18:52,200
中间层的维度FFN

600
00:18:52,200 --> 00:18:53,600
intermediate size

601
00:18:53,600 --> 00:18:55,700
它叫做主要是作为隐藏层

602
00:18:55,733 --> 00:18:57,400
一个hidden size的4倍

603
00:18:57,400 --> 00:18:58,666
一般都是4倍

604
00:18:58,700 --> 00:19:01,566
来控制模型的非线性表达

605
00:19:01,566 --> 00:19:03,166
那我们可以看到

606
00:19:03,166 --> 00:19:05,200
我们attention的hidden size

607
00:19:05,200 --> 00:19:09,800
绝对要小于FFN层的一个intermediate sized

608
00:19:09,800 --> 00:19:12,066
是希望一个FFN层

609
00:19:12,100 --> 00:19:14,166
能够更好地表达我们

610
00:19:14,166 --> 00:19:16,433
或者提取我们隐藏层之后

611
00:19:16,433 --> 00:19:17,900
抽象出来的特征

612
00:19:17,933 --> 00:19:19,400
那例如举个具体例子了

613
00:19:19,400 --> 00:19:21,633
Llama2里面的intermediate size

614
00:19:21,633 --> 00:19:23,000
是1万多

615
00:19:23,000 --> 00:19:24,066
但是hidden size

616
00:19:24,100 --> 00:19:25,166
大概是4,000

617
00:19:25,166 --> 00:19:27,600
多那基本上是2到 3倍

618
00:19:27,633 --> 00:19:29,800
那过大了intermediate size

619
00:19:29,800 --> 00:19:31,833
就会造成我们计算成本增加

620
00:19:31,833 --> 00:19:34,566
过小了有可能就限制模型的容量

621
00:19:34,566 --> 00:19:36,300
和模型的表征能力

622
00:19:36,966 --> 00:19:38,666
第 3个就是模型的深度了

623
00:19:38,700 --> 00:19:40,200
是模型层数

624
00:19:40,300 --> 00:19:41,033
模型层数

625
00:19:41,033 --> 00:19:43,666
是决定Transformer堆叠的一个数量

626
00:19:43,700 --> 00:19:45,333
模型层数越高

627
00:19:45,333 --> 00:19:47,400
可以表达的维度就越多

628
00:19:47,433 --> 00:19:48,300
因为每一层

629
00:19:48,333 --> 00:19:50,600
都会对我们数的序列来进行

630
00:19:50,600 --> 00:19:52,266
提取高维的特征

631
00:19:52,333 --> 00:19:52,633
但是

632
00:19:52,633 --> 00:19:55,033
也有可能导致我们模型训练越来越难

633
00:19:55,033 --> 00:19:56,166
或者过拟合

634
00:19:56,166 --> 00:19:57,633
这个模型的深度

635
00:19:57,633 --> 00:19:59,366
需要结合具体的任务

636
00:19:59,366 --> 00:20:02,233
和硬件的资源进行权衡

637
00:20:03,266 --> 00:20:04,433
来到了第 3个内容

638
00:20:04,433 --> 00:20:06,666
就是看一下模型结构的差异

639
00:20:06,700 --> 00:20:08,000
那这里面的模型结构

640
00:20:08,000 --> 00:20:09,433
更多的是用两个

641
00:20:09,433 --> 00:20:09,800
第一个

642
00:20:09,800 --> 00:20:12,366
就用Qwen的3的Dense模型

643
00:20:12,366 --> 00:20:14,366
做一个简单的对比

644
00:20:14,400 --> 00:20:15,833
特别是跟Transformer

645
00:20:15,833 --> 00:20:18,700
就GPT的跟Llama的进行一个对比

646
00:20:18,733 --> 00:20:19,733
那我们可以看到

647
00:20:19,733 --> 00:20:22,766
这里面很重要的就是感谢

648
00:20:22,833 --> 00:20:25,166
知乎我也不知道怎么读了

649
00:20:25,166 --> 00:20:27,400
根吗叫根根AI

650
00:20:27,966 --> 00:20:28,866
那可以看到

651
00:20:28,900 --> 00:20:31,333
Qwen 3的一个稠密的模型的参数量

652
00:20:31,333 --> 00:20:32,500
还是蛮有意思

653
00:20:32,500 --> 00:20:34,300
首先我们可以看到两个内容

654
00:20:34,300 --> 00:20:36,200
第一个就是attention这一块

655
00:20:36,200 --> 00:20:38,500
大家都比较关心attention里面

656
00:20:38,533 --> 00:20:39,566
他蛮有意思

657
00:20:39,566 --> 00:20:41,033
就是ZOMI觉得首先

658
00:20:41,033 --> 00:20:42,433
在QKV计算完之后

659
00:20:42,433 --> 00:20:43,566
加了一个RMSNoRM

660
00:20:43,566 --> 00:20:46,900
RMSNorm是放在每个QKV之后

661
00:20:47,000 --> 00:20:48,500
然后attention的一个推理

662
00:20:48,500 --> 00:20:50,000
就基本上是类似了

663
00:20:50,000 --> 00:20:51,066
没有太多区别

664
00:20:51,100 --> 00:20:52,633
第2个就是MLP

665
00:20:52,633 --> 00:20:53,900
也就是FFN层了

666
00:20:53,966 --> 00:20:57,200
FFN层这里面就来了两个

667
00:20:57,200 --> 00:20:57,933
两个残差

668
00:20:57,933 --> 00:21:00,600
第一个就是经过了FFN层之后

669
00:21:00,600 --> 00:21:01,833
加一个Silu用了

670
00:21:01,833 --> 00:21:03,566
经过了FFN层之后

671
00:21:03,566 --> 00:21:06,100
就直接把两个concat到一起进行输出

672
00:21:06,133 --> 00:21:08,100
给的l给linear层了

673
00:21:08,100 --> 00:21:10,133
所以它先up再down

674
00:21:10,133 --> 00:21:11,333
先变大再变小

675
00:21:11,333 --> 00:21:13,166
这种方式进行一个输出

676
00:21:13,166 --> 00:21:16,366
而不是只有一个FFN层

677
00:21:16,400 --> 00:21:18,866
那这个就是Qwen 3Dense的一个模型

678
00:21:18,900 --> 00:21:19,933
最大的区别

679
00:21:20,000 --> 00:21:22,200
那接着我们看一下MoE架构

680
00:21:22,200 --> 00:21:23,633
那幻方的DeepSeek

681
00:21:23,633 --> 00:21:24,433
它的MoE

682
00:21:24,433 --> 00:21:27,600
我们其实之前已经讲解了非常的多

683
00:21:27,633 --> 00:21:29,100
最重要的就是第一个

684
00:21:29,133 --> 00:21:31,766
在一个attention里面

685
00:21:31,766 --> 00:21:33,633
引入了MLA

686
00:21:33,633 --> 00:21:36,500
先把维度就很多的维度hidden Dim

687
00:21:36,533 --> 00:21:38,200
变小变成很小

688
00:21:38,200 --> 00:21:40,366
然后再进行一个压缩

689
00:21:40,366 --> 00:21:42,266
然后把位置编码加进去

690
00:21:42,300 --> 00:21:44,833
把位置编码加进去进行压缩

691
00:21:44,833 --> 00:21:46,566
然后把位置编码加进去

692
00:21:46,566 --> 00:21:48,900
然后再给到MHA

693
00:21:48,933 --> 00:21:50,333
进行一个计算

694
00:21:50,333 --> 00:21:51,166
那这个l

695
00:21:51,166 --> 00:21:53,766
主要是发生在中间的这一段

696
00:21:53,766 --> 00:21:55,033
就把

697
00:21:55,133 --> 00:21:57,533
就把模型输入的特征的维度

698
00:21:57,533 --> 00:21:59,933
压缩成为一个比较小的latent

699
00:22:00,433 --> 00:22:02,466
第2个就DeepSeek的MoE

700
00:22:02,533 --> 00:22:04,600
引入了一个共享专家

701
00:22:04,600 --> 00:22:06,966
那这里面就有了一个share expert

702
00:22:07,166 --> 00:22:08,800
在整个幻方DeepSeek里面

703
00:22:08,800 --> 00:22:10,400
V2的模型的版本

704
00:22:10,400 --> 00:22:12,333
用的是两个共享专家

705
00:22:12,366 --> 00:22:15,033
在MoE的大家很火的一个R1

706
00:22:15,033 --> 00:22:16,500
跟那个V3的模型

707
00:22:16,533 --> 00:22:19,400
用的就是一个共享专家

708
00:22:19,400 --> 00:22:21,433
后面有很多个专家路由

709
00:22:21,433 --> 00:22:24,033
基本上都扩充到256个专家

710
00:22:24,033 --> 00:22:25,966
再加一个共享专家了

711
00:22:25,966 --> 00:22:28,033
那这个是DeepSeek的一个内容

712
00:22:28,033 --> 00:22:29,500
和模型的结构

713
00:22:29,600 --> 00:22:30,633
那反观

714
00:22:30,633 --> 00:22:33,433
Qwen 3的自己的MoE

715
00:22:33,500 --> 00:22:34,200
模型层数

716
00:22:34,200 --> 00:22:37,466
会比刚才的DeepSeek更长

717
00:22:37,500 --> 00:22:41,000
就模型更多乘以n模型更高

718
00:22:41,000 --> 00:22:42,800
但是模型会比较瘦

719
00:22:42,800 --> 00:22:43,866
取决于 3个内容

720
00:22:43,900 --> 00:22:45,633
第一个我们看一下attention

721
00:22:45,733 --> 00:22:46,733
同样的attention

722
00:22:46,733 --> 00:22:49,766
跟刚才的一个MoE的Dense是一样

723
00:22:49,766 --> 00:22:52,500
在Linear后面加了一个RMSNorm

724
00:22:52,800 --> 00:22:53,966
那重点我们看一下

725
00:22:53,966 --> 00:22:56,166
第2个就是MLP了

726
00:22:56,166 --> 00:22:59,066
MLP实际上是MoE里面MoE

727
00:22:59,100 --> 00:23:01,700
这个MoE里面会有每一个专家

728
00:23:01,700 --> 00:23:04,033
每一个专家就跟Dense一样

729
00:23:04,033 --> 00:23:05,600
Dense里面有很多个Linear

730
00:23:05,600 --> 00:23:06,600
再加一个silu

731
00:23:06,600 --> 00:23:08,466
然后做一个残差输入

732
00:23:08,500 --> 00:23:10,133
把两个concat到一起

733
00:23:10,200 --> 00:23:11,933
这是它的一个MLP

734
00:23:11,933 --> 00:23:14,633
那多个MLP作为一个专家

735
00:23:14,633 --> 00:23:15,633
那专家里面

736
00:23:15,633 --> 00:23:18,300
就有很多个这种的线性层

737
00:23:18,333 --> 00:23:19,766
然后这个线性gate

738
00:23:19,766 --> 00:23:21,900
就是门控网络或路由网络

739
00:23:21,933 --> 00:23:25,233
去选择其中几个的MLP

740
00:23:25,233 --> 00:23:27,166
几个专家作为输出

741
00:23:27,166 --> 00:23:30,700
那整个Qwen 3的MoE会相对长

742
00:23:30,733 --> 00:23:32,000
比较瘦一点

743
00:23:32,200 --> 00:23:34,633
因为在整个Qwen 3在为什么会瘦

744
00:23:34,633 --> 00:23:37,866
瘦就瘦在这里面这个MLP

745
00:23:37,900 --> 00:23:40,966
每个专家还有每个attention里面

746
00:23:41,633 --> 00:23:43,066
当然有兴趣的小伙伴

747
00:23:43,100 --> 00:23:44,800
也可以重点的去看一下这个表

748
00:23:44,800 --> 00:23:46,033
ZOMI做了一个对比

749
00:23:46,033 --> 00:23:47,766
Qwen 3的一个MoE

750
00:23:47,766 --> 00:23:51,200
跟Dipsick的一个MoE之间的一个区别

751
00:23:51,200 --> 00:23:54,300
那区别刚才ZOMI已经讲到过了

752
00:23:54,333 --> 00:23:56,700
就不再重复的赘述了

753
00:23:57,000 --> 00:23:59,566
同样的我们每一期视频后面

754
00:23:59,566 --> 00:24:02,233
都会有一个总结跟思考

755
00:24:02,233 --> 00:24:03,466
那总结跟思考了

756
00:24:03,500 --> 00:24:05,300
就是调优的顺序

757
00:24:05,300 --> 00:24:06,433
因为这期视频

758
00:24:06,433 --> 00:24:09,633
更关心的是一个网络模型的一个参数量

759
00:24:09,633 --> 00:24:10,866
首先网络模型的层数

760
00:24:10,900 --> 00:24:12,033
我们先调层数

761
00:24:12,033 --> 00:24:13,666
再调那个并行的头数

762
00:24:13,700 --> 00:24:15,200
再调隐藏层

763
00:24:15,200 --> 00:24:18,833
再来改初始化和再调正则化

764
00:24:18,833 --> 00:24:19,566
正则话

765
00:24:19,566 --> 00:24:20,466
是放在最后

766
00:24:20,500 --> 00:24:22,966
现在已经形成一个正常

767
00:24:22,966 --> 00:24:25,633
或者比较约定成熟的内容了

768
00:24:26,000 --> 00:24:27,666
那所谓的避坑指南

769
00:24:27,700 --> 00:24:29,166
就是避免头数

770
00:24:29,166 --> 00:24:30,666
就层数跟头数了

771
00:24:30,700 --> 00:24:31,966
盲目的堆砌

772
00:24:32,066 --> 00:24:34,033
更多的需要提供验证

773
00:24:34,033 --> 00:24:37,000
及去动态的监控下游任务

774
00:24:37,000 --> 00:24:38,366
所以现在训练大模型

775
00:24:38,366 --> 00:24:40,200
不是一把训了两个月之后

776
00:24:40,200 --> 00:24:41,066
再看下游任务

777
00:24:41,100 --> 00:24:42,166
而是经常

778
00:24:42,166 --> 00:24:43,900
两三天跑一个check point

779
00:24:44,000 --> 00:24:45,066
把一个check point出来

780
00:24:45,100 --> 00:24:47,333
拿出来看一下模型的参数

781
00:24:47,333 --> 00:24:49,700
在下游任务的一个情况

782
00:24:49,700 --> 00:24:51,800
所以我们会有非常大量的数据集

783
00:24:51,800 --> 00:24:55,066
做验证不是盲目的去堆砌和扩充

784
00:24:55,200 --> 00:24:56,033
今天的内容

785
00:24:56,033 --> 00:24:56,800
就到这里为止

786
00:24:56,800 --> 00:24:57,400
谢谢各位

787
00:24:57,400 --> 00:24:58,566
拜了个拜

